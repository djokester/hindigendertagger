Training new model, loss:categorical_crossentropy, optimizer=nadam, cnn_len=128, dropoff=0.4
Epoch 1/20
 - 2s - loss: 0.8777 - acc: 0.5941
Epoch 2/20
 - 1s - loss: 0.7131 - acc: 0.7037
Epoch 3/20
 - 1s - loss: 0.6679 - acc: 0.7241
Epoch 4/20
 - 1s - loss: 0.6410 - acc: 0.7388
Epoch 5/20
 - 1s - loss: 0.6254 - acc: 0.7480
Epoch 6/20
 - 1s - loss: 0.6097 - acc: 0.7538
Epoch 7/20
 - 1s - loss: 0.6016 - acc: 0.7581
Epoch 8/20
 - 1s - loss: 0.5886 - acc: 0.7649
Epoch 9/20
 - 1s - loss: 0.5797 - acc: 0.7681
Epoch 10/20
 - 1s - loss: 0.5706 - acc: 0.7722
Epoch 11/20
 - 1s - loss: 0.5605 - acc: 0.7778
Epoch 12/20
 - 1s - loss: 0.5537 - acc: 0.7810
Epoch 13/20
 - 1s - loss: 0.5441 - acc: 0.7836
Epoch 14/20
 - 1s - loss: 0.5393 - acc: 0.7870
Epoch 15/20
 - 1s - loss: 0.5287 - acc: 0.7896
Epoch 16/20
 - 1s - loss: 0.5243 - acc: 0.7924
Epoch 17/20
 - 1s - loss: 0.5195 - acc: 0.7959
Epoch 18/20
 - 1s - loss: 0.5135 - acc: 0.7974
Epoch 19/20
 - 1s - loss: 0.5048 - acc: 0.8004
Epoch 20/20
 - 1s - loss: 0.4946 - acc: 0.8027

  32/9391 [..............................] - ETA: 13s
1024/9391 [==>...........................] - ETA: 0s 
1792/9391 [====>.........................] - ETA: 0s
2560/9391 [=======>......................] - ETA: 0s
3360/9391 [=========>....................] - ETA: 0s
4192/9391 [============>.................] - ETA: 0s
4992/9391 [==============>...............] - ETA: 0s
5760/9391 [=================>............] - ETA: 0s
6528/9391 [===================>..........] - ETA: 0s
7328/9391 [======================>.......] - ETA: 0s
8160/9391 [=========================>....] - ETA: 0s
8960/9391 [===========================>..] - ETA: 0s
9391/9391 [==============================] - 1s 68us/step
[0.631498766236127, 0.7543392610009163]
test score: [0.6314987652904245, 0.754339261464247]
recall: array([0.75145805, 0.53498099, 0.88305384])
precision: array([0.80220307, 0.7709589 , 0.7305586 ])
true count:array([2229, 2630, 4532])
predict count:array([2088, 1825, 5478])

Training new model, loss:categorical_crossentropy, optimizer=adam, cnn_len=128, dropoff=0.4
Epoch 1/20
 - 1s - loss: 0.9690 - acc: 0.5262
Epoch 2/20
 - 1s - loss: 0.7827 - acc: 0.6661
Epoch 3/20
 - 1s - loss: 0.7228 - acc: 0.6971
Epoch 4/20
 - 1s - loss: 0.6894 - acc: 0.7114
Epoch 5/20
 - 1s - loss: 0.6642 - acc: 0.7262
Epoch 6/20
 - 1s - loss: 0.6484 - acc: 0.7339
Epoch 7/20
 - 1s - loss: 0.6376 - acc: 0.7409
Epoch 8/20
 - 1s - loss: 0.6263 - acc: 0.7465
Epoch 9/20
 - 1s - loss: 0.6151 - acc: 0.7529
Epoch 10/20
 - 1s - loss: 0.6084 - acc: 0.7547
Epoch 11/20
 - 1s - loss: 0.5987 - acc: 0.7605
Epoch 12/20
 - 1s - loss: 0.5943 - acc: 0.7630
Epoch 13/20
 - 1s - loss: 0.5891 - acc: 0.7635
Epoch 14/20
 - 1s - loss: 0.5814 - acc: 0.7692
Epoch 15/20
 - 1s - loss: 0.5730 - acc: 0.7738
Epoch 16/20
 - 1s - loss: 0.5681 - acc: 0.7745
Epoch 17/20
 - 1s - loss: 0.5634 - acc: 0.7773
Epoch 18/20
 - 1s - loss: 0.5606 - acc: 0.7773
Epoch 19/20
 - 1s - loss: 0.5540 - acc: 0.7793
Epoch 20/20
 - 1s - loss: 0.5483 - acc: 0.7821

  32/9391 [..............................] - ETA: 16s
1024/9391 [==>...........................] - ETA: 0s 
1824/9391 [====>.........................] - ETA: 0s
2656/9391 [=======>......................] - ETA: 0s
3424/9391 [=========>....................] - ETA: 0s
4192/9391 [============>.................] - ETA: 0s
4992/9391 [==============>...............] - ETA: 0s
5792/9391 [=================>............] - ETA: 0s
6592/9391 [====================>.........] - ETA: 0s
7424/9391 [======================>.......] - ETA: 0s
8544/9391 [==========================>...] - ETA: 0s
9391/9391 [==============================] - 1s 64us/step
[0.6092580774063729, 0.7516771376977116]
test score: [0.6092580785234444, 0.7516771375073017]
recall: array([0.77972185, 0.59201521, 0.83053839])
precision: array([0.77382012, 0.70772727, 0.7611729 ])
true count:array([2229, 2630, 4532])
predict count:array([2246, 2200, 4945])

Training new model, loss:categorical_crossentropy, optimizer=nadam, cnn_len=128, dropoff=0.55
Epoch 1/20
 - 1s - loss: 0.8909 - acc: 0.5882
Epoch 2/20
 - 1s - loss: 0.7278 - acc: 0.6960
Epoch 3/20
 - 1s - loss: 0.6810 - acc: 0.7186
Epoch 4/20
 - 1s - loss: 0.6573 - acc: 0.7329
Epoch 5/20
 - 1s - loss: 0.6393 - acc: 0.7405
Epoch 6/20
 - 1s - loss: 0.6244 - acc: 0.7479
Epoch 7/20
 - 1s - loss: 0.6165 - acc: 0.7528
Epoch 8/20
 - 1s - loss: 0.6019 - acc: 0.7592
Epoch 9/20
 - 1s - loss: 0.5959 - acc: 0.7626
Epoch 10/20
 - 1s - loss: 0.5834 - acc: 0.7684
Epoch 11/20
 - 1s - loss: 0.5756 - acc: 0.7718
Epoch 12/20
 - 1s - loss: 0.5699 - acc: 0.7742
Epoch 13/20
 - 1s - loss: 0.5600 - acc: 0.7791
Epoch 14/20
 - 1s - loss: 0.5522 - acc: 0.7804
Epoch 15/20
 - 1s - loss: 0.5459 - acc: 0.7830
Epoch 16/20
 - 1s - loss: 0.5403 - acc: 0.7867
Epoch 17/20
 - 1s - loss: 0.5327 - acc: 0.7906
Epoch 18/20
 - 1s - loss: 0.5265 - acc: 0.7916
Epoch 19/20
 - 1s - loss: 0.5210 - acc: 0.7936
Epoch 20/20
 - 1s - loss: 0.5145 - acc: 0.7977

  32/9391 [..............................] - ETA: 19s
 928/9391 [=>............................] - ETA: 1s 
1696/9391 [====>.........................] - ETA: 0s
2432/9391 [======>.......................] - ETA: 0s
3232/9391 [=========>....................] - ETA: 0s
3968/9391 [===========>..................] - ETA: 0s
4704/9391 [==============>...............] - ETA: 0s
5440/9391 [================>.............] - ETA: 0s
6240/9391 [==================>...........] - ETA: 0s
6976/9391 [=====================>........] - ETA: 0s
7968/9391 [========================>.....] - ETA: 0s
9248/9391 [============================>.] - ETA: 0s
9391/9391 [==============================] - 1s 68us/step
[0.6049978141490929, 0.7577467788434894]
test score: [0.6049978053109002, 0.7577467784372817]
recall: array([0.78375953, 0.61140684, 0.82987643])
precision: array([0.77748109, 0.71498444, 0.76833504])
true count:array([2229, 2630, 4532])
predict count:array([2247, 2249, 4895])

Training new model, loss:categorical_crossentropy, optimizer=nadam, cnn_len=128, dropoff=0.5
Epoch 1/20
 - 1s - loss: 0.8975 - acc: 0.5863
Epoch 2/20
 - 1s - loss: 0.7282 - acc: 0.6949
Epoch 3/20
 - 1s - loss: 0.6791 - acc: 0.7183
Epoch 4/20
 - 1s - loss: 0.6559 - acc: 0.7315
Epoch 5/20
 - 1s - loss: 0.6363 - acc: 0.7426
Epoch 6/20
 - 1s - loss: 0.6252 - acc: 0.7491
Epoch 7/20
 - 1s - loss: 0.6118 - acc: 0.7566
Epoch 8/20
 - 1s - loss: 0.6003 - acc: 0.7603
Epoch 9/20
 - 1s - loss: 0.5915 - acc: 0.7656
Epoch 10/20
 - 1s - loss: 0.5834 - acc: 0.7682
Epoch 11/20
 - 1s - loss: 0.5712 - acc: 0.7739
Epoch 12/20
 - 1s - loss: 0.5648 - acc: 0.7765
Epoch 13/20
 - 1s - loss: 0.5572 - acc: 0.7787
Epoch 14/20
 - 1s - loss: 0.5517 - acc: 0.7834
Epoch 15/20
 - 1s - loss: 0.5406 - acc: 0.7868
Epoch 16/20
 - 1s - loss: 0.5349 - acc: 0.7911
Epoch 17/20
 - 1s - loss: 0.5297 - acc: 0.7914
Epoch 18/20
 - 1s - loss: 0.5266 - acc: 0.7931
Epoch 19/20
 - 1s - loss: 0.5163 - acc: 0.7964
Epoch 20/20
 - 1s - loss: 0.5120 - acc: 0.7969

  32/9391 [..............................] - ETA: 23s
 896/9391 [=>............................] - ETA: 1s 
1632/9391 [====>.........................] - ETA: 0s
2368/9391 [======>.......................] - ETA: 0s
3136/9391 [=========>....................] - ETA: 0s
3904/9391 [===========>..................] - ETA: 0s
4640/9391 [=============>................] - ETA: 0s
5408/9391 [================>.............] - ETA: 0s
6176/9391 [==================>...........] - ETA: 0s
6944/9391 [=====================>........] - ETA: 0s
8000/9391 [========================>.....] - ETA: 0s
9344/9391 [============================>.] - ETA: 0s
9391/9391 [==============================] - 1s 68us/step
[0.6137394767677856, 0.7583856884377819]
test score: [0.6137394803474917, 0.758385688247372]
recall: array([0.76716016, 0.62015209, 0.8342895 ])
precision: array([0.79683131, 0.71253823, 0.76291364])
true count:array([2229, 2630, 4532])
predict count:array([2146, 2289, 4956])

Training new model, loss:categorical_crossentropy, optimizer=rmsprop, cnn_len=128, dropoff=0.4
Epoch 1/20
 - 1s - loss: 0.9234 - acc: 0.5644
Epoch 2/20
 - 1s - loss: 0.7753 - acc: 0.6695
Epoch 3/20
 - 1s - loss: 0.7266 - acc: 0.6943
Epoch 4/20
 - 1s - loss: 0.6937 - acc: 0.7125
Epoch 5/20
 - 1s - loss: 0.6736 - acc: 0.7223
Epoch 6/20
 - 1s - loss: 0.6580 - acc: 0.7287
Epoch 7/20
 - 1s - loss: 0.6446 - acc: 0.7377
Epoch 8/20
 - 1s - loss: 0.6345 - acc: 0.7404
Epoch 9/20
 - 1s - loss: 0.6254 - acc: 0.7455
Epoch 10/20
 - 1s - loss: 0.6189 - acc: 0.7489
Epoch 11/20
 - 1s - loss: 0.6104 - acc: 0.7542
Epoch 12/20
 - 1s - loss: 0.6039 - acc: 0.7570
Epoch 13/20
 - 1s - loss: 0.5992 - acc: 0.7584
Epoch 14/20
 - 1s - loss: 0.5927 - acc: 0.7639
Epoch 15/20
 - 1s - loss: 0.5846 - acc: 0.7679
Epoch 16/20
 - 1s - loss: 0.5806 - acc: 0.7678
Epoch 17/20
 - 1s - loss: 0.5765 - acc: 0.7690
Epoch 18/20
 - 1s - loss: 0.5730 - acc: 0.7713
Epoch 19/20
 - 1s - loss: 0.5656 - acc: 0.7757
Epoch 20/20
 - 1s - loss: 0.5615 - acc: 0.7759

  32/9391 [..............................] - ETA: 25s
 928/9391 [=>............................] - ETA: 1s 
1664/9391 [====>.........................] - ETA: 0s
2432/9391 [======>.......................] - ETA: 0s
3168/9391 [=========>....................] - ETA: 0s
4192/9391 [============>.................] - ETA: 0s
5536/9391 [================>.............] - ETA: 0s
6816/9391 [====================>.........] - ETA: 0s
7968/9391 [========================>.....] - ETA: 0s
9120/9391 [============================>.] - ETA: 0s
9391/9391 [==============================] - 1s 60us/step
[0.6103032413140588, 0.7513576828942183]
test score: [0.6103032387911277, 0.7513576829259534]
recall: array([0.79676985, 0.57794677, 0.82965578])
precision: array([0.75865015, 0.72209026, 0.760364  ])
true count:array([2229, 2630, 4532])
predict count:array([2341, 2105, 4945])

