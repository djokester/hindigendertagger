{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of valid Hindi literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(set(list(\"ऀँंःऄअआइईउऊऋऌऍऎएऐऑऒओऔकखगघङचछजझञटठडढणतथदधनऩपफबभमयरऱलळऴवशषसहऺऻ़ऽािीुूृॄॅॆेैॉॊोौ्ॎॏॐ॒॑॓॔ॕॖॗक़ख़ग़ज़ड़ढ़फ़य़ॠॡॢॣ।॥॰ॱॲॳॴॵॶॷॸॹॺॻॼॽॾॿ-\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290872, 284250)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(genderListCleared),len(set(genderListCleared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderListCleared = list(set(genderListCleared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "mCount = 0\n",
    "fCount = 0\n",
    "nCount = 0\n",
    "for item in genderListCleared:\n",
    "    if item[1] == 'm':\n",
    "        mCount+=1\n",
    "    elif item[1] == 'f':\n",
    "        fCount+=1\n",
    "    elif item[1] == 'none':\n",
    "        nCount+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17803, 9695, 243852, 12900)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mCount,fCount,nCount,len(genderListCleared)-mCount-fCount-nCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genderListCleared', 'wb') as fp:\n",
    "    pickle.dump(genderListCleared, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genderListCleared', 'rb') as fp:\n",
    "    genderListCleared = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderListNoNone= []\n",
    "for item in genderListCleared:\n",
    "    if item[1] == 'm':\n",
    "        genderListNoNone.append(item)\n",
    "    elif item[1] == 'f':\n",
    "        genderListNoNone.append(item)\n",
    "    elif item[1] == 'any':\n",
    "        genderListNoNone.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genderListNoNone', 'wb') as fp:\n",
    "    pickle.dump(genderListNoNone, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genderListNoNone', 'rb') as fp:\n",
    "    genderListNoNone = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneWords = list(set(genderListCleared)-set(genderListNoNone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "noneWords = set([x[0] for x in noneWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lingatagger.genderlist as gndrlist\n",
    "import lingatagger.tokenizer as tok\n",
    "from lingatagger.tagger import *\n",
    "\n",
    "genders2 = gndrlist.drawlist()\n",
    "genderList2 = []\n",
    "for i in genders2:\n",
    "    x = i.split(\"\\t\")\n",
    "    if type(numericTagger(x[0])[0]) != tuple:\n",
    "        count = 0\n",
    "        for ch in list(x[0]):\n",
    "            if ch not in a:\n",
    "                count+=1\n",
    "        if count == 0:\n",
    "            if len(x)>=3:\n",
    "                genderList2.append((x[0],'any'))\n",
    "            else:\n",
    "                genderList2.append((x[0],x[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderList2.sort()\n",
    "genderList2Cleared = genderList2\n",
    "for ind in range(0, len(genderList2Cleared)-1):\n",
    "    if genderList2Cleared[ind][0] == genderList2Cleared[ind+1][0]:\n",
    "        genderList2Cleared[ind] = genderList2Cleared[ind][0], 'any'\n",
    "        genderList2Cleared[ind+1] = genderList2Cleared[ind][0], 'any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderList2Cleared = list(set(genderList2Cleared))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mCount2 = 0\n",
    "fCount2 = 0\n",
    "for item in genderList2Cleared:\n",
    "    if item[1] == 'm':\n",
    "        mCount2+=1\n",
    "    elif item[1] == 'f':\n",
    "        fCount2+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10636, 3892, 5604)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mCount2,fCount2,len(genderList2Cleared)-mCount2-fCount2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genderList2Cleared', 'wb') as fp:\n",
    "    pickle.dump(genderList2Cleared, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genderList2Cleared', 'rb') as fp:\n",
    "    genderList2Cleared = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderList2Matched = []\n",
    "for item in genderList2Cleared:\n",
    "    if item[0] in noneWords:\n",
    "        continue\n",
    "    genderList2Matched.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(genderList2Cleared)-len(genderList2Matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('genderList2Matched', 'wb') as fp:\n",
    "    pickle.dump(genderList2Matched, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedList = []\n",
    "for item in genderList2Cleared:\n",
    "    mergedList.append((item[0], item[1]))\n",
    "for item in genderListNoNone:\n",
    "    mergedList.append((item[0], item[1]))\n",
    "mergedList.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in range(0, len(mergedList)-1):\n",
    "    if mergedList[ind][0] == mergedList[ind+1][0]:\n",
    "        fgend = 'any'\n",
    "        if  mergedList[ind][1] == 'm' or mergedList[ind+1][1] == 'm':\n",
    "            fgend = 'm'\n",
    "        elif  mergedList[ind][1] == 'f' or mergedList[ind+1][1] == 'f':\n",
    "            if fgend == 'm':\n",
    "                fgend = 'any'\n",
    "            else:\n",
    "                fgend = 'f'\n",
    "        else:\n",
    "            fgend = 'any'\n",
    "        mergedList[ind] = mergedList[ind][0], fgend\n",
    "        mergedList[ind+1] = mergedList[ind][0], fgend\n",
    "\n",
    "mergedList = list(set(mergedList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mCount3 = 0\n",
    "fCount3 = 0\n",
    "for item in mergedList:\n",
    "    if item[1] == 'm':\n",
    "        mCount3+=1\n",
    "    elif item[1] == 'f':\n",
    "        fCount3+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25701, 11897, 12879)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mCount3,fCount3,len(mergedList)-mCount3-fCount3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mergedList', 'wb') as fp:\n",
    "    pickle.dump(mergedList, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mergedList', 'rb') as fp:\n",
    "    mergedList = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(18, dtype=\"int\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import lingatagger.genderlist as gndrlist\n",
    "import lingatagger.tokenizer as tok\n",
    "from lingatagger.tagger import *\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "def encodex(text):\n",
    "    s = list(text)\n",
    "    \n",
    "    indices = []\n",
    "    for i in s:\n",
    "        indices.append(a.index(i))\n",
    "    encoded = np.zeros(18, dtype=\"int\")\n",
    "    #print(len(a)+1)\n",
    "    k = 0\n",
    "    for i in indices:\n",
    "        encoded[k] = i\n",
    "        k = k + 1\n",
    "    for i in range(18-len(list(s))):\n",
    "        encoded[k+i] = len(a)\n",
    "    return encoded\n",
    "\n",
    "def encodey(text):\n",
    "    if text == \"f\":\n",
    "        return [1,0,0]\n",
    "    elif text == \"m\":\n",
    "        return [0,0,1]\n",
    "\n",
    "    else:\n",
    "        return [0,1,0] \n",
    "\n",
    "def genderdecode(genderTag):\n",
    "    \"\"\"\n",
    "    one-hot decoding for the gender tag predicted by the classfier\n",
    "    Dimension = 2.\n",
    "    \"\"\"\n",
    "    genderTag = list(genderTag[0])\n",
    "    index = genderTag.index(heapq.nlargest(1, genderTag)[0])\n",
    "    if index == 0:\n",
    "        return 'f'\n",
    "    if index == 2:\n",
    "        return 'm'\n",
    "    if index == 1:\n",
    "        return 'any'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in genderListNoNone:\n",
    "    if len(i[0]) > 18:\n",
    "        continue\n",
    "    x_train.append(encodex(i[0]))\n",
    "    y_train.append(encodey(i[1]))\n",
    "    \n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "for i in genderList2Matched:\n",
    "    if len(i[0]) > 18:\n",
    "        continue\n",
    "    x_test.append(encodex(i[0]))\n",
    "    y_test.append(encodey(i[1]))\n",
    "    \n",
    "    \n",
    "\n",
    "x_merged = []\n",
    "y_merged = []\n",
    "for i in mergedList:\n",
    "    if len(i[0]) > 18:\n",
    "        continue\n",
    "    x_merged.append(encodex(i[0]))\n",
    "    y_merged.append(encodey(i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = np.array(x_train)\n",
    "Y_train = np.array(y_train)\n",
    "\n",
    "X_test = np.array(x_test)\n",
    "Y_test = np.array(y_test)\n",
    "\n",
    "X_merged = np.array(x_merged)\n",
    "Y_merged = np.array(y_merged)\n",
    "\n",
    "with open('X_train', 'wb') as fp:\n",
    "    pickle.dump(X_train, fp)\n",
    "\n",
    "with open('Y_train', 'wb') as fp:\n",
    "    pickle.dump(Y_train, fp)\n",
    "\n",
    "with open('X_test', 'wb') as fp:\n",
    "    pickle.dump(X_test, fp)\n",
    "\n",
    "with open('Y_test', 'wb') as fp:\n",
    "    pickle.dump(Y_test, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.4\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 30s 943us/step - loss: 1.0692 - acc: 0.4402 - val_loss: 1.0691 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 31s 946us/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0690 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 31s 944us/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0687 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 28s 880us/step - loss: 1.0680 - acc: 0.4407 - val_loss: 1.0685 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 28s 880us/step - loss: 1.0679 - acc: 0.4407 - val_loss: 1.0676 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 30s 933us/step - loss: 1.0671 - acc: 0.4407 - val_loss: 1.0666 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 30s 935us/step - loss: 1.0648 - acc: 0.4407 - val_loss: 1.0608 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 30s 929us/step - loss: 1.0438 - acc: 0.4623 - val_loss: 1.0237 - val_acc: 0.4759\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 30s 930us/step - loss: 0.9995 - acc: 0.4833 - val_loss: 0.9702 - val_acc: 0.5137\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 30s 924us/step - loss: 0.9556 - acc: 0.5278 - val_loss: 0.9907 - val_acc: 0.4884\n",
      "20122/20122 [==============================] - 5s 251us/step\n",
      "\n",
      "test score: [1.0663544713781388, 0.4062220455341625]\n",
      "\n",
      "\n",
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.45\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 35s 1ms/step - loss: 1.0692 - acc: 0.4406 - val_loss: 1.0685 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 32s 983us/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0684 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 30s 934us/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0684 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 32s 987us/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0683 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 31s 947us/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0685 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 31s 944us/step - loss: 1.0678 - acc: 0.4407 - val_loss: 1.0683 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 31s 953us/step - loss: 1.0675 - acc: 0.4407 - val_loss: 1.0679 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 32s 982us/step - loss: 1.0667 - acc: 0.4407 - val_loss: 1.0663 - val_acc: 0.4406\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 31s 949us/step - loss: 1.0625 - acc: 0.4411 - val_loss: 1.0564 - val_acc: 0.4406\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 31s 963us/step - loss: 1.0407 - acc: 0.4733 - val_loss: 1.0268 - val_acc: 0.4813\n",
      "20122/20122 [==============================] - 5s 262us/step\n",
      "\n",
      "test score: [1.02362715051018, 0.49110426399262525]\n",
      "\n",
      "\n",
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.5\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 34s 1ms/step - loss: 1.0695 - acc: 0.4399 - val_loss: 1.0694 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 31s 969us/step - loss: 1.0688 - acc: 0.4407 - val_loss: 1.0690 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 31s 957us/step - loss: 1.0685 - acc: 0.4407 - val_loss: 1.0686 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 32s 986us/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0684 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 32s 987us/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0684 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 32s 991us/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0683 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 31s 963us/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0683 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 31s 962us/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0682 - val_acc: 0.4406\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 32s 991us/step - loss: 1.0680 - acc: 0.4407 - val_loss: 1.0678 - val_acc: 0.4406\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 1.0675 - acc: 0.4407 - val_loss: 1.0673 - val_acc: 0.4406\n",
      "20122/20122 [==============================] - 6s 274us/step\n",
      "\n",
      "test score: [1.0238210319844738, 0.5285756883043239]\n",
      "\n",
      "\n",
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.55\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 35s 1ms/step - loss: 1.0692 - acc: 0.4406 - val_loss: 1.0684 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 1.0687 - acc: 0.4407 - val_loss: 1.0687 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0682 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 32s 991us/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0682 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 32s 978us/step - loss: 1.0682 - acc: 0.4407 - val_loss: 1.0678 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 32s 999us/step - loss: 1.0676 - acc: 0.4407 - val_loss: 1.0689 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 32s 999us/step - loss: 1.0672 - acc: 0.4407 - val_loss: 1.0665 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 32s 999us/step - loss: 1.0652 - acc: 0.4408 - val_loss: 1.0623 - val_acc: 0.4406\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 32s 1ms/step - loss: 1.0509 - acc: 0.4624 - val_loss: 1.0352 - val_acc: 0.4847\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 1.0279 - acc: 0.4883 - val_loss: 1.0159 - val_acc: 0.4948\n",
      "20122/20122 [==============================] - 6s 300us/step\n",
      "\n",
      "test score: [1.0234103390857934, 0.49726667329587537]\n",
      "\n",
      "\n",
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.6\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 38s 1ms/step - loss: 1.0694 - acc: 0.4406 - val_loss: 1.0685 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 1.0684 - acc: 0.4407 - val_loss: 1.0686 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 34s 1ms/step - loss: 1.0685 - acc: 0.4407 - val_loss: 1.0696 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 35s 1ms/step - loss: 1.0680 - acc: 0.4407 - val_loss: 1.0685 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 34s 1ms/step - loss: 1.0672 - acc: 0.4407 - val_loss: 1.0664 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 34s 1ms/step - loss: 1.0639 - acc: 0.4407 - val_loss: 1.0578 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 1.0414 - acc: 0.4698 - val_loss: 1.0244 - val_acc: 0.4806\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 1.0036 - acc: 0.4833 - val_loss: 0.9859 - val_acc: 0.5181\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 0.9609 - acc: 0.5228 - val_loss: 0.9430 - val_acc: 0.5547\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 33s 1ms/step - loss: 0.9401 - acc: 0.5384 - val_loss: 0.9377 - val_acc: 0.5335\n",
      "20122/20122 [==============================] - 6s 285us/step\n",
      "\n",
      "test score: [1.0087274505276647, 0.5294205347499462]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.4\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 112s 3ms/step - loss: 1.0694 - acc: 0.4407 - val_loss: 1.0689 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 93s 3ms/step - loss: 1.0685 - acc: 0.4407 - val_loss: 1.0688 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 89s 3ms/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0684 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 90s 3ms/step - loss: 1.0682 - acc: 0.4407 - val_loss: 1.0681 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 96s 3ms/step - loss: 1.0679 - acc: 0.4407 - val_loss: 1.0679 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 97s 3ms/step - loss: 1.0674 - acc: 0.4407 - val_loss: 1.0671 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 96s 3ms/step - loss: 1.0659 - acc: 0.4407 - val_loss: 1.0651 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 93s 3ms/step - loss: 1.0555 - acc: 0.4479 - val_loss: 1.0466 - val_acc: 0.4615\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 115s 4ms/step - loss: 1.0183 - acc: 0.4749 - val_loss: 1.0555 - val_acc: 0.4270\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 114s 4ms/step - loss: 0.9688 - acc: 0.5141 - val_loss: 0.9553 - val_acc: 0.5410\n",
      "20122/20122 [==============================] - 18s 873us/step\n",
      "\n",
      "test score: [1.024113481187664, 0.47773581154362343]\n",
      "\n",
      "\n",
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.45\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 109s 3ms/step - loss: 1.0693 - acc: 0.4406 - val_loss: 1.0685 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 105s 3ms/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0692 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 107s 3ms/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0683 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 134s 4ms/step - loss: 1.0681 - acc: 0.4407 - val_loss: 1.0687 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 124s 4ms/step - loss: 1.0680 - acc: 0.4407 - val_loss: 1.0678 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 99s 3ms/step - loss: 1.0675 - acc: 0.4407 - val_loss: 1.0669 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 93s 3ms/step - loss: 1.0656 - acc: 0.4407 - val_loss: 1.0642 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 111s 3ms/step - loss: 1.0536 - acc: 0.4529 - val_loss: 1.0367 - val_acc: 0.4736\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 114s 4ms/step - loss: 1.0184 - acc: 0.4789 - val_loss: 0.9898 - val_acc: 0.4871\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 93s 3ms/step - loss: 0.9710 - acc: 0.5140 - val_loss: 0.9583 - val_acc: 0.5349\n",
      "20122/20122 [==============================] - 15s 721us/step\n",
      "\n",
      "test score: [1.0418092684579434, 0.5000993937043638]\n",
      "\n",
      "\n",
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.5\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 108s 3ms/step - loss: 1.0691 - acc: 0.4404 - val_loss: 1.0686 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 124s 4ms/step - loss: 1.0685 - acc: 0.4407 - val_loss: 1.0685 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 127s 4ms/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0688 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 103s 3ms/step - loss: 1.0682 - acc: 0.4407 - val_loss: 1.0683 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 112s 3ms/step - loss: 1.0680 - acc: 0.4407 - val_loss: 1.0681 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 123s 4ms/step - loss: 1.0680 - acc: 0.4407 - val_loss: 1.0690 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 134s 4ms/step - loss: 1.0675 - acc: 0.4407 - val_loss: 1.0673 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 134s 4ms/step - loss: 1.0664 - acc: 0.4407 - val_loss: 1.0664 - val_acc: 0.4406\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 100s 3ms/step - loss: 1.0576 - acc: 0.4474 - val_loss: 1.0466 - val_acc: 0.4658\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 87s 3ms/step - loss: 1.0245 - acc: 0.4779 - val_loss: 1.0021 - val_acc: 0.4785\n",
      "20122/20122 [==============================] - 16s 808us/step\n",
      "\n",
      "test score: [1.029496891196899, 0.47579763442997713]\n",
      "\n",
      "\n",
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.55\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 99s 3ms/step - loss: 1.0693 - acc: 0.4407 - val_loss: 1.0685 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 98s 3ms/step - loss: 1.0687 - acc: 0.4407 - val_loss: 1.0686 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 94s 3ms/step - loss: 1.0685 - acc: 0.4407 - val_loss: 1.0682 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 94s 3ms/step - loss: 1.0682 - acc: 0.4407 - val_loss: 1.0679 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 97s 3ms/step - loss: 1.0680 - acc: 0.4407 - val_loss: 1.0675 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 96s 3ms/step - loss: 1.0671 - acc: 0.4407 - val_loss: 1.0660 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 97s 3ms/step - loss: 1.0630 - acc: 0.4410 - val_loss: 1.0558 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 98s 3ms/step - loss: 1.0352 - acc: 0.4677 - val_loss: 1.0364 - val_acc: 0.4515\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 100s 3ms/step - loss: 0.9846 - acc: 0.4973 - val_loss: 0.9597 - val_acc: 0.5343\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 102s 3ms/step - loss: 0.9503 - acc: 0.5330 - val_loss: 0.9398 - val_acc: 0.5452\n",
      "20122/20122 [==============================] - 17s 824us/step\n",
      "\n",
      "test score: [1.0178065125220173, 0.5139648146307524]\n",
      "\n",
      "\n",
      "Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.6\n",
      "Train on 32318 samples, validate on 8080 samples\n",
      "Epoch 1/10\n",
      "32318/32318 [==============================] - 107s 3ms/step - loss: 1.0698 - acc: 0.4405 - val_loss: 1.0686 - val_acc: 0.4406\n",
      "Epoch 2/10\n",
      "32318/32318 [==============================] - 97s 3ms/step - loss: 1.0686 - acc: 0.4407 - val_loss: 1.0686 - val_acc: 0.4406\n",
      "Epoch 3/10\n",
      "32318/32318 [==============================] - 105s 3ms/step - loss: 1.0683 - acc: 0.4407 - val_loss: 1.0684 - val_acc: 0.4406\n",
      "Epoch 4/10\n",
      "32318/32318 [==============================] - 100s 3ms/step - loss: 1.0686 - acc: 0.4407 - val_loss: 1.0684 - val_acc: 0.4406\n",
      "Epoch 5/10\n",
      "32318/32318 [==============================] - 105s 3ms/step - loss: 1.0681 - acc: 0.4407 - val_loss: 1.0683 - val_acc: 0.4406\n",
      "Epoch 6/10\n",
      "32318/32318 [==============================] - 120s 4ms/step - loss: 1.0680 - acc: 0.4407 - val_loss: 1.0681 - val_acc: 0.4406\n",
      "Epoch 7/10\n",
      "32318/32318 [==============================] - 118s 4ms/step - loss: 1.0675 - acc: 0.4407 - val_loss: 1.0681 - val_acc: 0.4406\n",
      "Epoch 8/10\n",
      "32318/32318 [==============================] - 116s 4ms/step - loss: 1.0665 - acc: 0.4407 - val_loss: 1.0654 - val_acc: 0.4406\n",
      "Epoch 9/10\n",
      "32318/32318 [==============================] - 117s 4ms/step - loss: 1.0615 - acc: 0.4435 - val_loss: 1.0516 - val_acc: 0.4478\n",
      "Epoch 10/10\n",
      "32318/32318 [==============================] - 116s 4ms/step - loss: 1.0341 - acc: 0.4758 - val_loss: 1.0198 - val_acc: 0.4787\n",
      "20122/20122 [==============================] - 19s 966us/step\n",
      "\n",
      "test score: [1.019344429623333, 0.5065599840999704]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "max_features = len(a)+1\n",
    "\n",
    "for loss_f in ['categorical_crossentropy']:\n",
    "    for opt in ['rmsprop','adam','nadam','sgd']:\n",
    "        for lstm_len in [32,64,128,256]:\n",
    "            for dropout in [0.4,0.45,0.5,0.55,0.6]:\n",
    "                model = Sequential()\n",
    "                model.add(Embedding(max_features, output_dim=18))\n",
    "                model.add(LSTM(lstm_len))\n",
    "                model.add(Dropout(dropout))\n",
    "                model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "                model.compile(loss=loss_f,\n",
    "                              optimizer=opt,\n",
    "                              metrics=['accuracy'])\n",
    "                print(\"Training new model, loss:\"+loss_f+\", optimizer=\"+opt+\", lstm_len=\"+str(lstm_len)+\", dropoff=\"+str(dropout))\n",
    "                model.fit(X_train, Y_train, batch_size=16, validation_split = 0.2, epochs=10)\n",
    "                score = model.evaluate(X_test, Y_test, batch_size=16)\n",
    "                print(\"\")\n",
    "                print(\"test score: \" + str(score))\n",
    "                print(\"\")\n",
    "                print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
