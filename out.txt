
Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=32, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 10s - loss: 0.9095 - acc: 0.5870 - val_loss: 0.8274 - val_acc: 0.6361
Epoch 2/10
 - 9s - loss: 0.8088 - acc: 0.6487 - val_loss: 0.8080 - val_acc: 0.6583
Epoch 3/10
 - 9s - loss: 0.7742 - acc: 0.6726 - val_loss: 0.7654 - val_acc: 0.6866
Epoch 4/10
 - 9s - loss: 0.7467 - acc: 0.6912 - val_loss: 0.7472 - val_acc: 0.6911
Epoch 5/10
 - 9s - loss: 0.7199 - acc: 0.7052 - val_loss: 0.7323 - val_acc: 0.6988
Epoch 6/10
 - 9s - loss: 0.6995 - acc: 0.7110 - val_loss: 0.7411 - val_acc: 0.6792
Epoch 7/10
 - 8s - loss: 0.6843 - acc: 0.7216 - val_loss: 0.7166 - val_acc: 0.7047
Epoch 8/10
 - 8s - loss: 0.6758 - acc: 0.7247 - val_loss: 0.7131 - val_acc: 0.7140
Epoch 9/10
 - 8s - loss: 0.6644 - acc: 0.7305 - val_loss: 0.6943 - val_acc: 0.7154
Epoch 10/10
 - 8s - loss: 0.6533 - acc: 0.7346 - val_loss: 0.6771 - val_acc: 0.7262

  16/8086 [..............................] - ETA: 0s
 784/8086 [=>............................] - ETA: 0s
1552/8086 [====>.........................] - ETA: 0s
2336/8086 [=======>......................] - ETA: 0s
3104/8086 [==========>...................] - ETA: 0s
3888/8086 [=============>................] - ETA: 0s
4656/8086 [================>.............] - ETA: 0s
5440/8086 [===================>..........] - ETA: 0s
6224/8086 [======================>.......] - ETA: 0s
7024/8086 [=========================>....] - ETA: 0s
7776/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 65us/step

test score: [0.7090177043163036, 0.696759831822806]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=32, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 9s - loss: 0.9189 - acc: 0.5792 - val_loss: 0.8506 - val_acc: 0.6339
Epoch 2/10
 - 8s - loss: 0.8393 - acc: 0.6339 - val_loss: 0.8449 - val_acc: 0.6277
Epoch 3/10
 - 8s - loss: 0.7891 - acc: 0.6674 - val_loss: 0.7747 - val_acc: 0.6758
Epoch 4/10
 - 8s - loss: 0.7552 - acc: 0.6873 - val_loss: 0.7625 - val_acc: 0.6933
Epoch 5/10
 - 8s - loss: 0.7346 - acc: 0.6982 - val_loss: 0.7478 - val_acc: 0.6919
Epoch 6/10
 - 8s - loss: 0.7146 - acc: 0.7081 - val_loss: 0.7252 - val_acc: 0.7030
Epoch 7/10
 - 8s - loss: 0.6973 - acc: 0.7172 - val_loss: 0.7204 - val_acc: 0.7106
Epoch 8/10
 - 8s - loss: 0.6748 - acc: 0.7259 - val_loss: 0.6976 - val_acc: 0.7095
Epoch 9/10
 - 8s - loss: 0.6620 - acc: 0.7294 - val_loss: 0.6913 - val_acc: 0.7169
Epoch 10/10
 - 8s - loss: 0.6522 - acc: 0.7357 - val_loss: 0.6965 - val_acc: 0.7151

  16/8086 [..............................] - ETA: 0s
 800/8086 [=>............................] - ETA: 0s
1584/8086 [====>.........................] - ETA: 0s
2384/8086 [=======>......................] - ETA: 0s
3184/8086 [==========>...................] - ETA: 0s
3936/8086 [=============>................] - ETA: 0s
4720/8086 [================>.............] - ETA: 0s
5488/8086 [===================>..........] - ETA: 0s
6224/8086 [======================>.......] - ETA: 0s
6992/8086 [========================>.....] - ETA: 0s
7760/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 66us/step

test score: [0.730657833027362, 0.6944100915309435]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=32, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 9s - loss: 0.9130 - acc: 0.5866 - val_loss: 0.8848 - val_acc: 0.6217
Epoch 2/10
 - 8s - loss: 0.8227 - acc: 0.6446 - val_loss: 0.8048 - val_acc: 0.6503
Epoch 3/10
 - 8s - loss: 0.7793 - acc: 0.6677 - val_loss: 0.7686 - val_acc: 0.6792
Epoch 4/10
 - 8s - loss: 0.7482 - acc: 0.6896 - val_loss: 0.7414 - val_acc: 0.6857
Epoch 5/10
 - 8s - loss: 0.7222 - acc: 0.7003 - val_loss: 0.7176 - val_acc: 0.6991
Epoch 6/10
 - 8s - loss: 0.7028 - acc: 0.7103 - val_loss: 0.7129 - val_acc: 0.7045
Epoch 7/10
 - 8s - loss: 0.6876 - acc: 0.7195 - val_loss: 0.6975 - val_acc: 0.7075
Epoch 8/10
 - 8s - loss: 0.6782 - acc: 0.7257 - val_loss: 0.6943 - val_acc: 0.7098
Epoch 9/10
 - 8s - loss: 0.6689 - acc: 0.7289 - val_loss: 0.6831 - val_acc: 0.7194
Epoch 10/10
 - 8s - loss: 0.6585 - acc: 0.7343 - val_loss: 0.7004 - val_acc: 0.7141

  16/8086 [..............................] - ETA: 0s
 736/8086 [=>............................] - ETA: 0s
1472/8086 [====>.........................] - ETA: 0s
2224/8086 [=======>......................] - ETA: 0s
2976/8086 [==========>...................] - ETA: 0s
3728/8086 [============>.................] - ETA: 0s
4464/8086 [===============>..............] - ETA: 0s
5216/8086 [==================>...........] - ETA: 0s
5952/8086 [=====================>........] - ETA: 0s
6688/8086 [=======================>......] - ETA: 0s
7440/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 68us/step

test score: [0.7413797569457712, 0.6884739055157062]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=32, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 9s - loss: 0.9152 - acc: 0.5804 - val_loss: 0.8457 - val_acc: 0.6336
Epoch 2/10
 - 8s - loss: 0.8159 - acc: 0.6349 - val_loss: 0.8188 - val_acc: 0.6602
Epoch 3/10
 - 8s - loss: 0.7758 - acc: 0.6662 - val_loss: 0.7682 - val_acc: 0.6738
Epoch 4/10
 - 8s - loss: 0.7559 - acc: 0.6838 - val_loss: 0.7676 - val_acc: 0.6892
Epoch 5/10
 - 8s - loss: 0.7291 - acc: 0.7019 - val_loss: 0.7450 - val_acc: 0.6970
Epoch 6/10
 - 8s - loss: 0.7102 - acc: 0.7092 - val_loss: 0.7198 - val_acc: 0.7030
Epoch 7/10
 - 8s - loss: 0.6957 - acc: 0.7167 - val_loss: 0.7126 - val_acc: 0.7084
Epoch 8/10
 - 8s - loss: 0.6857 - acc: 0.7233 - val_loss: 0.7029 - val_acc: 0.7089
Epoch 9/10
 - 8s - loss: 0.6755 - acc: 0.7265 - val_loss: 0.7019 - val_acc: 0.7178
Epoch 10/10
 - 8s - loss: 0.6707 - acc: 0.7313 - val_loss: 0.6864 - val_acc: 0.7225

  16/8086 [..............................] - ETA: 0s
 704/8086 [=>............................] - ETA: 0s
1408/8086 [====>.........................] - ETA: 0s
2096/8086 [======>.......................] - ETA: 0s
2784/8086 [=========>....................] - ETA: 0s
3504/8086 [============>.................] - ETA: 0s
4192/8086 [==============>...............] - ETA: 0s
4944/8086 [=================>............] - ETA: 0s
5632/8086 [===================>..........] - ETA: 0s
6320/8086 [======================>.......] - ETA: 0s
7056/8086 [=========================>....] - ETA: 0s
7744/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 72us/step

test score: [0.7145150441094964, 0.6934207271975278]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=32, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 9s - loss: 0.9270 - acc: 0.5768 - val_loss: 0.8588 - val_acc: 0.6282
Epoch 2/10
 - 8s - loss: 0.8462 - acc: 0.6288 - val_loss: 0.8064 - val_acc: 0.6597
Epoch 3/10
 - 8s - loss: 0.7973 - acc: 0.6587 - val_loss: 0.7949 - val_acc: 0.6684
Epoch 4/10
 - 8s - loss: 0.7759 - acc: 0.6752 - val_loss: 0.7682 - val_acc: 0.6688
Epoch 5/10
 - 8s - loss: 0.7503 - acc: 0.6907 - val_loss: 0.7579 - val_acc: 0.6766
Epoch 6/10
 - 8s - loss: 0.7226 - acc: 0.7005 - val_loss: 0.7309 - val_acc: 0.6905
Epoch 7/10
 - 8s - loss: 0.7074 - acc: 0.7126 - val_loss: 0.7170 - val_acc: 0.7013
Epoch 8/10
 - 9s - loss: 0.6886 - acc: 0.7194 - val_loss: 0.7086 - val_acc: 0.7127
Epoch 9/10
 - 9s - loss: 0.6845 - acc: 0.7241 - val_loss: 0.7115 - val_acc: 0.7010
Epoch 10/10
 - 8s - loss: 0.6723 - acc: 0.7284 - val_loss: 0.6902 - val_acc: 0.7225

  16/8086 [..............................] - ETA: 0s
 752/8086 [=>............................] - ETA: 0s
1488/8086 [====>.........................] - ETA: 0s
2224/8086 [=======>......................] - ETA: 0s
2944/8086 [=========>....................] - ETA: 0s
3664/8086 [============>.................] - ETA: 0s
4384/8086 [===============>..............] - ETA: 0s
5104/8086 [=================>............] - ETA: 0s
5824/8086 [====================>.........] - ETA: 0s
6544/8086 [=======================>......] - ETA: 0s
7264/8086 [=========================>....] - ETA: 0s
7984/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 70us/step

test score: [0.7145840713153992, 0.6973781845311908]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=64, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 10s - loss: 0.9074 - acc: 0.5893 - val_loss: 0.8542 - val_acc: 0.6316
Epoch 2/10
 - 9s - loss: 0.8172 - acc: 0.6439 - val_loss: 0.8166 - val_acc: 0.6538
Epoch 3/10
 - 9s - loss: 0.7730 - acc: 0.6714 - val_loss: 0.7778 - val_acc: 0.6747
Epoch 4/10
 - 9s - loss: 0.7400 - acc: 0.6939 - val_loss: 0.7457 - val_acc: 0.7011
Epoch 5/10
 - 9s - loss: 0.7046 - acc: 0.7113 - val_loss: 0.7402 - val_acc: 0.7045
Epoch 6/10
 - 9s - loss: 0.6823 - acc: 0.7224 - val_loss: 0.7302 - val_acc: 0.7045
Epoch 7/10
 - 9s - loss: 0.6606 - acc: 0.7307 - val_loss: 0.6943 - val_acc: 0.7160
Epoch 8/10
 - 9s - loss: 0.6439 - acc: 0.7382 - val_loss: 0.6839 - val_acc: 0.7276
Epoch 9/10
 - 9s - loss: 0.6329 - acc: 0.7423 - val_loss: 0.6727 - val_acc: 0.7270
Epoch 10/10
 - 9s - loss: 0.6250 - acc: 0.7442 - val_loss: 0.6820 - val_acc: 0.7299

  16/8086 [..............................] - ETA: 0s
 656/8086 [=>............................] - ETA: 0s
1328/8086 [===>..........................] - ETA: 0s
1984/8086 [======>.......................] - ETA: 0s
2640/8086 [========>.....................] - ETA: 0s
3280/8086 [===========>..................] - ETA: 0s
3920/8086 [=============>................] - ETA: 0s
4560/8086 [===============>..............] - ETA: 0s
5216/8086 [==================>...........] - ETA: 0s
5856/8086 [====================>.........] - ETA: 0s
6512/8086 [=======================>......] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7824/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 77us/step

test score: [0.7083168053889434, 0.7065298046152868]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=64, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 10s - loss: 0.9045 - acc: 0.5894 - val_loss: 0.8362 - val_acc: 0.6416
Epoch 2/10
 - 9s - loss: 0.8101 - acc: 0.6503 - val_loss: 0.8016 - val_acc: 0.6574
Epoch 3/10
 - 9s - loss: 0.7729 - acc: 0.6769 - val_loss: 0.7698 - val_acc: 0.6806
Epoch 4/10
 - 9s - loss: 0.7372 - acc: 0.6934 - val_loss: 0.7455 - val_acc: 0.6818
Epoch 5/10
 - 9s - loss: 0.7114 - acc: 0.7069 - val_loss: 0.7293 - val_acc: 0.7032
Epoch 6/10
 - 9s - loss: 0.6907 - acc: 0.7151 - val_loss: 0.7077 - val_acc: 0.7103
Epoch 7/10
 - 9s - loss: 0.6678 - acc: 0.7281 - val_loss: 0.7113 - val_acc: 0.7066
Epoch 8/10
 - 9s - loss: 0.6533 - acc: 0.7364 - val_loss: 0.6798 - val_acc: 0.7214
Epoch 9/10
 - 9s - loss: 0.6412 - acc: 0.7413 - val_loss: 0.6807 - val_acc: 0.7308
Epoch 10/10
 - 9s - loss: 0.6318 - acc: 0.7443 - val_loss: 0.6633 - val_acc: 0.7322

  16/8086 [..............................] - ETA: 0s
 640/8086 [=>............................] - ETA: 0s
1264/8086 [===>..........................] - ETA: 0s
1952/8086 [======>.......................] - ETA: 0s
2640/8086 [========>.....................] - ETA: 0s
3344/8086 [===========>..................] - ETA: 0s
4032/8086 [=============>................] - ETA: 0s
4704/8086 [================>.............] - ETA: 0s
5392/8086 [===================>..........] - ETA: 0s
6096/8086 [=====================>........] - ETA: 0s
6800/8086 [========================>.....] - ETA: 0s
7488/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 75us/step

test score: [0.6910646462717653, 0.7107346030323038]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=64, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 10s - loss: 0.9070 - acc: 0.5888 - val_loss: 0.8468 - val_acc: 0.6354
Epoch 2/10
 - 9s - loss: 0.8120 - acc: 0.6437 - val_loss: 0.7906 - val_acc: 0.6619
Epoch 3/10
 - 9s - loss: 0.7736 - acc: 0.6729 - val_loss: 0.7611 - val_acc: 0.6925
Epoch 4/10
 - 9s - loss: 0.7379 - acc: 0.6930 - val_loss: 0.7581 - val_acc: 0.6814
Epoch 5/10
 - 9s - loss: 0.7115 - acc: 0.7021 - val_loss: 0.7235 - val_acc: 0.6985
Epoch 6/10
 - 9s - loss: 0.6916 - acc: 0.7132 - val_loss: 0.7087 - val_acc: 0.7047
Epoch 7/10
 - 9s - loss: 0.6758 - acc: 0.7229 - val_loss: 0.6856 - val_acc: 0.7188
Epoch 8/10
 - 9s - loss: 0.6626 - acc: 0.7320 - val_loss: 0.6659 - val_acc: 0.7260
Epoch 9/10
 - 9s - loss: 0.6480 - acc: 0.7348 - val_loss: 0.6654 - val_acc: 0.7308
Epoch 10/10
 - 9s - loss: 0.6335 - acc: 0.7422 - val_loss: 0.6655 - val_acc: 0.7240

  16/8086 [..............................] - ETA: 0s
 704/8086 [=>............................] - ETA: 0s
1392/8086 [====>.........................] - ETA: 0s
2064/8086 [======>.......................] - ETA: 0s
2752/8086 [=========>....................] - ETA: 0s
3440/8086 [===========>..................] - ETA: 0s
4128/8086 [==============>...............] - ETA: 0s
4816/8086 [================>.............] - ETA: 0s
5504/8086 [===================>..........] - ETA: 0s
6208/8086 [======================>.......] - ETA: 0s
6896/8086 [========================>.....] - ETA: 0s
7584/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 74us/step

test score: [0.6934516360389399, 0.7071481573236716]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=64, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 10s - loss: 0.9143 - acc: 0.5870 - val_loss: 0.8991 - val_acc: 0.5966
Epoch 2/10
 - 9s - loss: 0.8272 - acc: 0.6418 - val_loss: 0.7896 - val_acc: 0.6735
Epoch 3/10
 - 9s - loss: 0.7814 - acc: 0.6689 - val_loss: 0.7638 - val_acc: 0.6783
Epoch 4/10
 - 9s - loss: 0.7433 - acc: 0.6890 - val_loss: 0.7417 - val_acc: 0.6987
Epoch 5/10
 - 9s - loss: 0.7144 - acc: 0.7041 - val_loss: 0.7273 - val_acc: 0.6920
Epoch 6/10
 - 9s - loss: 0.6970 - acc: 0.7124 - val_loss: 0.7047 - val_acc: 0.7064
Epoch 7/10
 - 9s - loss: 0.6807 - acc: 0.7188 - val_loss: 0.7063 - val_acc: 0.7073
Epoch 8/10
 - 9s - loss: 0.6627 - acc: 0.7262 - val_loss: 0.6877 - val_acc: 0.7206
Epoch 9/10
 - 9s - loss: 0.6469 - acc: 0.7372 - val_loss: 0.6760 - val_acc: 0.7260
Epoch 10/10
 - 9s - loss: 0.6375 - acc: 0.7400 - val_loss: 0.6673 - val_acc: 0.7305

  16/8086 [..............................] - ETA: 0s
 704/8086 [=>............................] - ETA: 0s
1376/8086 [====>.........................] - ETA: 0s
2064/8086 [======>.......................] - ETA: 0s
2688/8086 [========>.....................] - ETA: 0s
3328/8086 [===========>..................] - ETA: 0s
4016/8086 [=============>................] - ETA: 0s
4688/8086 [================>.............] - ETA: 0s
5376/8086 [==================>...........] - ETA: 0s
6064/8086 [=====================>........] - ETA: 0s
6752/8086 [========================>.....] - ETA: 0s
7440/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 75us/step

test score: [0.6972628307024088, 0.7066534751569638]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=64, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 11s - loss: 0.9152 - acc: 0.5858 - val_loss: 0.8492 - val_acc: 0.6339
Epoch 2/10
 - 9s - loss: 0.8213 - acc: 0.6402 - val_loss: 0.8197 - val_acc: 0.6447
Epoch 3/10
 - 9s - loss: 0.7827 - acc: 0.6672 - val_loss: 0.7796 - val_acc: 0.6702
Epoch 4/10
 - 9s - loss: 0.7494 - acc: 0.6884 - val_loss: 0.7516 - val_acc: 0.6934
Epoch 5/10
 - 9s - loss: 0.7202 - acc: 0.7010 - val_loss: 0.7216 - val_acc: 0.7015
Epoch 6/10
 - 9s - loss: 0.6979 - acc: 0.7160 - val_loss: 0.7253 - val_acc: 0.6982
Epoch 7/10
 - 9s - loss: 0.6773 - acc: 0.7246 - val_loss: 0.6931 - val_acc: 0.7161
Epoch 8/10
 - 9s - loss: 0.6618 - acc: 0.7311 - val_loss: 0.6924 - val_acc: 0.7199
Epoch 9/10
 - 9s - loss: 0.6464 - acc: 0.7379 - val_loss: 0.6764 - val_acc: 0.7271
Epoch 10/10
 - 9s - loss: 0.6375 - acc: 0.7426 - val_loss: 0.6663 - val_acc: 0.7325

  16/8086 [..............................] - ETA: 0s
 688/8086 [=>............................] - ETA: 0s
1360/8086 [====>.........................] - ETA: 0s
2048/8086 [======>.......................] - ETA: 0s
2720/8086 [=========>....................] - ETA: 0s
3392/8086 [===========>..................] - ETA: 0s
4080/8086 [==============>...............] - ETA: 0s
4752/8086 [================>.............] - ETA: 0s
5408/8086 [===================>..........] - ETA: 0s
6080/8086 [=====================>........] - ETA: 0s
6752/8086 [========================>.....] - ETA: 0s
7424/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 75us/step

test score: [0.6910896170207129, 0.705787781365225]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=128, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 15s - loss: 0.9073 - acc: 0.5912 - val_loss: 0.8272 - val_acc: 0.6543
Epoch 2/10
 - 13s - loss: 0.8117 - acc: 0.6513 - val_loss: 0.7848 - val_acc: 0.6647
Epoch 3/10
 - 13s - loss: 0.7583 - acc: 0.6840 - val_loss: 0.7701 - val_acc: 0.6953
Epoch 4/10
 - 13s - loss: 0.7158 - acc: 0.7036 - val_loss: 0.7422 - val_acc: 0.7036
Epoch 5/10
 - 13s - loss: 0.6816 - acc: 0.7200 - val_loss: 0.7083 - val_acc: 0.7106
Epoch 6/10
 - 13s - loss: 0.6574 - acc: 0.7340 - val_loss: 0.6796 - val_acc: 0.7265
Epoch 7/10
 - 13s - loss: 0.6387 - acc: 0.7418 - val_loss: 0.6630 - val_acc: 0.7344
Epoch 8/10
 - 13s - loss: 0.6268 - acc: 0.7464 - val_loss: 0.6731 - val_acc: 0.7325
Epoch 9/10
 - 13s - loss: 0.6186 - acc: 0.7512 - val_loss: 0.6624 - val_acc: 0.7358
Epoch 10/10
 - 13s - loss: 0.6076 - acc: 0.7541 - val_loss: 0.6496 - val_acc: 0.7358

  16/8086 [..............................] - ETA: 0s
 448/8086 [>.............................] - ETA: 0s
 864/8086 [==>...........................] - ETA: 0s
1280/8086 [===>..........................] - ETA: 0s
1712/8086 [=====>........................] - ETA: 0s
2128/8086 [======>.......................] - ETA: 0s
2512/8086 [========>.....................] - ETA: 0s
2896/8086 [=========>....................] - ETA: 0s
3296/8086 [===========>..................] - ETA: 0s
3712/8086 [============>.................] - ETA: 0s
4128/8086 [==============>...............] - ETA: 0s
4560/8086 [===============>..............] - ETA: 0s
4976/8086 [=================>............] - ETA: 0s
5392/8086 [===================>..........] - ETA: 0s
5808/8086 [====================>.........] - ETA: 0s
6240/8086 [======================>.......] - ETA: 0s
6656/8086 [=======================>......] - ETA: 0s
7072/8086 [=========================>....] - ETA: 0s
7488/8086 [==========================>...] - ETA: 0s
7920/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 123us/step

test score: [0.6694994709322484, 0.7200098936580769]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=128, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 15s - loss: 0.9070 - acc: 0.5904 - val_loss: 0.8768 - val_acc: 0.6317
Epoch 2/10
 - 13s - loss: 0.8219 - acc: 0.6443 - val_loss: 0.8158 - val_acc: 0.6676
Epoch 3/10
 - 13s - loss: 0.7681 - acc: 0.6762 - val_loss: 0.7542 - val_acc: 0.6735
Epoch 4/10
 - 13s - loss: 0.7263 - acc: 0.6956 - val_loss: 0.7224 - val_acc: 0.7007
Epoch 5/10
 - 13s - loss: 0.6962 - acc: 0.7122 - val_loss: 0.7067 - val_acc: 0.7135
Epoch 6/10
 - 14s - loss: 0.6686 - acc: 0.7276 - val_loss: 0.6805 - val_acc: 0.7294
Epoch 7/10
 - 14s - loss: 0.6491 - acc: 0.7342 - val_loss: 0.7294 - val_acc: 0.7022
Epoch 8/10
 - 14s - loss: 0.6335 - acc: 0.7421 - val_loss: 0.6576 - val_acc: 0.7367
Epoch 9/10
 - 14s - loss: 0.6205 - acc: 0.7491 - val_loss: 0.6846 - val_acc: 0.7250
Epoch 10/10
 - 14s - loss: 0.6131 - acc: 0.7505 - val_loss: 0.6646 - val_acc: 0.7274

  16/8086 [..............................] - ETA: 1s
 400/8086 [>.............................] - ETA: 1s
 800/8086 [=>............................] - ETA: 0s
1184/8086 [===>..........................] - ETA: 0s
1568/8086 [====>.........................] - ETA: 0s
1952/8086 [======>.......................] - ETA: 0s
2352/8086 [=======>......................] - ETA: 0s
2736/8086 [=========>....................] - ETA: 0s
3120/8086 [==========>...................] - ETA: 0s
3504/8086 [============>.................] - ETA: 0s
3888/8086 [=============>................] - ETA: 0s
4272/8086 [==============>...............] - ETA: 0s
4656/8086 [================>.............] - ETA: 0s
5040/8086 [=================>............] - ETA: 0s
5424/8086 [===================>..........] - ETA: 0s
5808/8086 [====================>.........] - ETA: 0s
6192/8086 [=====================>........] - ETA: 0s
6576/8086 [=======================>......] - ETA: 0s
6960/8086 [========================>.....] - ETA: 0s
7344/8086 [==========================>...] - ETA: 0s
7728/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 132us/step

test score: [0.6832018039057286, 0.7107346030175612]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=128, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 15s - loss: 0.9054 - acc: 0.5915 - val_loss: 0.8391 - val_acc: 0.6529
Epoch 2/10
 - 14s - loss: 0.8140 - acc: 0.6522 - val_loss: 0.7936 - val_acc: 0.6704
Epoch 3/10
 - 14s - loss: 0.7618 - acc: 0.6799 - val_loss: 0.7673 - val_acc: 0.6843
Epoch 4/10
 - 14s - loss: 0.7251 - acc: 0.6990 - val_loss: 0.7175 - val_acc: 0.7062
Epoch 5/10
 - 14s - loss: 0.6930 - acc: 0.7188 - val_loss: 0.7121 - val_acc: 0.7174
Epoch 6/10
 - 14s - loss: 0.6660 - acc: 0.7313 - val_loss: 0.6948 - val_acc: 0.7202
Epoch 7/10
 - 14s - loss: 0.6493 - acc: 0.7380 - val_loss: 0.6863 - val_acc: 0.7353
Epoch 8/10
 - 14s - loss: 0.6344 - acc: 0.7452 - val_loss: 0.6602 - val_acc: 0.7322
Epoch 9/10
 - 14s - loss: 0.6244 - acc: 0.7461 - val_loss: 0.6559 - val_acc: 0.7373
Epoch 10/10
 - 14s - loss: 0.6137 - acc: 0.7526 - val_loss: 0.6697 - val_acc: 0.7344

  16/8086 [..............................] - ETA: 0s
 416/8086 [>.............................] - ETA: 0s
 784/8086 [=>............................] - ETA: 0s
1152/8086 [===>..........................] - ETA: 0s
1520/8086 [====>.........................] - ETA: 0s
1904/8086 [======>.......................] - ETA: 0s
2288/8086 [=======>......................] - ETA: 0s
2672/8086 [========>.....................] - ETA: 0s
3056/8086 [==========>...................] - ETA: 0s
3456/8086 [===========>..................] - ETA: 0s
3856/8086 [=============>................] - ETA: 0s
4256/8086 [==============>...............] - ETA: 0s
4640/8086 [================>.............] - ETA: 0s
5040/8086 [=================>............] - ETA: 0s
5440/8086 [===================>..........] - ETA: 0s
5840/8086 [====================>.........] - ETA: 0s
6224/8086 [======================>.......] - ETA: 0s
6624/8086 [=======================>......] - ETA: 0s
7024/8086 [=========================>....] - ETA: 0s
7408/8086 [==========================>...] - ETA: 0s
7808/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 132us/step

test score: [0.6925709586375838, 0.7171654711995065]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=128, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 15s - loss: 0.9089 - acc: 0.5922 - val_loss: 0.8381 - val_acc: 0.6401
Epoch 2/10
 - 13s - loss: 0.8173 - acc: 0.6444 - val_loss: 0.8188 - val_acc: 0.6459
Epoch 3/10
 - 14s - loss: 0.7672 - acc: 0.6742 - val_loss: 0.7631 - val_acc: 0.6863
Epoch 4/10
 - 13s - loss: 0.7202 - acc: 0.7016 - val_loss: 0.7252 - val_acc: 0.7010
Epoch 5/10
 - 13s - loss: 0.6894 - acc: 0.7144 - val_loss: 0.7132 - val_acc: 0.6979
Epoch 6/10
 - 13s - loss: 0.6647 - acc: 0.7292 - val_loss: 0.7160 - val_acc: 0.7222
Epoch 7/10
 - 13s - loss: 0.6475 - acc: 0.7382 - val_loss: 0.6821 - val_acc: 0.7234
Epoch 8/10
 - 14s - loss: 0.6340 - acc: 0.7431 - val_loss: 0.6704 - val_acc: 0.7338
Epoch 9/10
 - 13s - loss: 0.6201 - acc: 0.7495 - val_loss: 0.6607 - val_acc: 0.7279
Epoch 10/10
 - 13s - loss: 0.6144 - acc: 0.7509 - val_loss: 0.6527 - val_acc: 0.7328

  16/8086 [..............................] - ETA: 1s
 416/8086 [>.............................] - ETA: 0s
 816/8086 [==>...........................] - ETA: 0s
1232/8086 [===>..........................] - ETA: 0s
1616/8086 [====>.........................] - ETA: 0s
2000/8086 [======>.......................] - ETA: 0s
2368/8086 [=======>......................] - ETA: 0s
2752/8086 [=========>....................] - ETA: 0s
3136/8086 [==========>...................] - ETA: 0s
3504/8086 [============>.................] - ETA: 0s
3888/8086 [=============>................] - ETA: 0s
4272/8086 [==============>...............] - ETA: 0s
4640/8086 [================>.............] - ETA: 0s
5024/8086 [=================>............] - ETA: 0s
5392/8086 [===================>..........] - ETA: 0s
5760/8086 [====================>.........] - ETA: 0s
6112/8086 [=====================>........] - ETA: 0s
6496/8086 [=======================>......] - ETA: 0s
6896/8086 [========================>.....] - ETA: 0s
7264/8086 [=========================>....] - ETA: 0s
7648/8086 [===========================>..] - ETA: 0s
8016/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 134us/step

test score: [0.6726887009092525, 0.7184021766162761]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=128, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 16s - loss: 0.9069 - acc: 0.5932 - val_loss: 0.8367 - val_acc: 0.6429
Epoch 2/10
 - 14s - loss: 0.8147 - acc: 0.6496 - val_loss: 0.8044 - val_acc: 0.6528
Epoch 3/10
 - 14s - loss: 0.7679 - acc: 0.6756 - val_loss: 0.7596 - val_acc: 0.6761
Epoch 4/10
 - 14s - loss: 0.7283 - acc: 0.6956 - val_loss: 0.7227 - val_acc: 0.7066
Epoch 5/10
 - 14s - loss: 0.6959 - acc: 0.7158 - val_loss: 0.7023 - val_acc: 0.7110
Epoch 6/10
 - 14s - loss: 0.6744 - acc: 0.7244 - val_loss: 0.6812 - val_acc: 0.7214
Epoch 7/10
 - 14s - loss: 0.6603 - acc: 0.7321 - val_loss: 0.6927 - val_acc: 0.7147
Epoch 8/10
 - 14s - loss: 0.6449 - acc: 0.7359 - val_loss: 0.6770 - val_acc: 0.7166
Epoch 9/10
 - 14s - loss: 0.6313 - acc: 0.7439 - val_loss: 0.6771 - val_acc: 0.7212
Epoch 10/10
 - 14s - loss: 0.6256 - acc: 0.7477 - val_loss: 0.6610 - val_acc: 0.7369

  16/8086 [..............................] - ETA: 0s
 368/8086 [>.............................] - ETA: 1s
 752/8086 [=>............................] - ETA: 1s
1136/8086 [===>..........................] - ETA: 0s
1520/8086 [====>.........................] - ETA: 0s
1920/8086 [======>.......................] - ETA: 0s
2288/8086 [=======>......................] - ETA: 0s
2656/8086 [========>.....................] - ETA: 0s
3040/8086 [==========>...................] - ETA: 0s
3440/8086 [===========>..................] - ETA: 0s
3824/8086 [=============>................] - ETA: 0s
4224/8086 [==============>...............] - ETA: 0s
4624/8086 [================>.............] - ETA: 0s
5008/8086 [=================>............] - ETA: 0s
5392/8086 [===================>..........] - ETA: 0s
5776/8086 [====================>.........] - ETA: 0s
6160/8086 [=====================>........] - ETA: 0s
6528/8086 [=======================>......] - ETA: 0s
6912/8086 [========================>.....] - ETA: 0s
7280/8086 [==========================>...] - ETA: 0s
7664/8086 [===========================>..] - ETA: 0s
8048/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 134us/step

test score: [0.6855408477954218, 0.7134553549491973]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=256, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 31s - loss: 0.9063 - acc: 0.5904 - val_loss: 0.8462 - val_acc: 0.6390
Epoch 2/10
 - 29s - loss: 0.7972 - acc: 0.6568 - val_loss: 0.7670 - val_acc: 0.6831
Epoch 3/10
 - 29s - loss: 0.7277 - acc: 0.6963 - val_loss: 0.7505 - val_acc: 0.6874
Epoch 4/10
 - 30s - loss: 0.6899 - acc: 0.7156 - val_loss: 0.7367 - val_acc: 0.7008
Epoch 5/10
 - 29s - loss: 0.6633 - acc: 0.7296 - val_loss: 0.6924 - val_acc: 0.7200
Epoch 6/10
 - 30s - loss: 0.6423 - acc: 0.7393 - val_loss: 0.6690 - val_acc: 0.7290
Epoch 7/10
 - 29s - loss: 0.6260 - acc: 0.7460 - val_loss: 0.6661 - val_acc: 0.7318
Epoch 8/10
 - 30s - loss: 0.6142 - acc: 0.7514 - val_loss: 0.6941 - val_acc: 0.7180
Epoch 9/10
 - 30s - loss: 0.6054 - acc: 0.7559 - val_loss: 0.6963 - val_acc: 0.7089
Epoch 10/10
 - 30s - loss: 0.5910 - acc: 0.7626 - val_loss: 0.6614 - val_acc: 0.7330

  16/8086 [..............................] - ETA: 1s
 240/8086 [..............................] - ETA: 1s
 464/8086 [>.............................] - ETA: 1s
 688/8086 [=>............................] - ETA: 1s
 912/8086 [==>...........................] - ETA: 1s
1120/8086 [===>..........................] - ETA: 1s
1344/8086 [===>..........................] - ETA: 1s
1552/8086 [====>.........................] - ETA: 1s
1760/8086 [=====>........................] - ETA: 1s
1968/8086 [======>.......................] - ETA: 1s
2176/8086 [=======>......................] - ETA: 1s
2384/8086 [=======>......................] - ETA: 1s
2608/8086 [========>.....................] - ETA: 1s
2832/8086 [=========>....................] - ETA: 1s
3040/8086 [==========>...................] - ETA: 1s
3248/8086 [===========>..................] - ETA: 1s
3472/8086 [===========>..................] - ETA: 1s
3680/8086 [============>.................] - ETA: 1s
3888/8086 [=============>................] - ETA: 1s
4112/8086 [==============>...............] - ETA: 0s
4336/8086 [===============>..............] - ETA: 0s
4560/8086 [===============>..............] - ETA: 0s
4784/8086 [================>.............] - ETA: 0s
5008/8086 [=================>............] - ETA: 0s
5232/8086 [==================>...........] - ETA: 0s
5456/8086 [===================>..........] - ETA: 0s
5680/8086 [====================>.........] - ETA: 0s
5904/8086 [====================>.........] - ETA: 0s
6128/8086 [=====================>........] - ETA: 0s
6336/8086 [======================>.......] - ETA: 0s
6544/8086 [=======================>......] - ETA: 0s
6768/8086 [========================>.....] - ETA: 0s
6992/8086 [========================>.....] - ETA: 0s
7216/8086 [=========================>....] - ETA: 0s
7424/8086 [==========================>...] - ETA: 0s
7648/8086 [===========================>..] - ETA: 0s
7856/8086 [============================>.] - ETA: 0s
8080/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 239us/step

test score: [0.6883206510201794, 0.7226069750332933]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=256, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 32s - loss: 0.8996 - acc: 0.5940 - val_loss: 0.8256 - val_acc: 0.6572
Epoch 2/10
 - 30s - loss: 0.7928 - acc: 0.6643 - val_loss: 0.7854 - val_acc: 0.6634
Epoch 3/10
 - 30s - loss: 0.7349 - acc: 0.6922 - val_loss: 0.7298 - val_acc: 0.7075
Epoch 4/10
 - 30s - loss: 0.6905 - acc: 0.7145 - val_loss: 0.6944 - val_acc: 0.7169
Epoch 5/10
 - 30s - loss: 0.6665 - acc: 0.7265 - val_loss: 0.6925 - val_acc: 0.7246
Epoch 6/10
 - 30s - loss: 0.6474 - acc: 0.7381 - val_loss: 0.6788 - val_acc: 0.7339
Epoch 7/10
 - 30s - loss: 0.6322 - acc: 0.7435 - val_loss: 0.6819 - val_acc: 0.7324
Epoch 8/10
 - 30s - loss: 0.6224 - acc: 0.7479 - val_loss: 0.6527 - val_acc: 0.7350
Epoch 9/10
 - 30s - loss: 0.6111 - acc: 0.7544 - val_loss: 0.6770 - val_acc: 0.7273
Epoch 10/10
 - 30s - loss: 0.6005 - acc: 0.7589 - val_loss: 0.6529 - val_acc: 0.7324

  16/8086 [..............................] - ETA: 2s
 224/8086 [..............................] - ETA: 1s
 432/8086 [>.............................] - ETA: 1s
 640/8086 [=>............................] - ETA: 1s
 848/8086 [==>...........................] - ETA: 1s
1056/8086 [==>...........................] - ETA: 1s
1264/8086 [===>..........................] - ETA: 1s
1472/8086 [====>.........................] - ETA: 1s
1680/8086 [=====>........................] - ETA: 1s
1888/8086 [======>.......................] - ETA: 1s
2096/8086 [======>.......................] - ETA: 1s
2304/8086 [=======>......................] - ETA: 1s
2512/8086 [========>.....................] - ETA: 1s
2720/8086 [=========>....................] - ETA: 1s
2928/8086 [=========>....................] - ETA: 1s
3136/8086 [==========>...................] - ETA: 1s
3344/8086 [===========>..................] - ETA: 1s
3552/8086 [============>.................] - ETA: 1s
3760/8086 [============>.................] - ETA: 1s
3968/8086 [=============>................] - ETA: 1s
4176/8086 [==============>...............] - ETA: 0s
4384/8086 [===============>..............] - ETA: 0s
4592/8086 [================>.............] - ETA: 0s
4800/8086 [================>.............] - ETA: 0s
5008/8086 [=================>............] - ETA: 0s
5216/8086 [==================>...........] - ETA: 0s
5424/8086 [===================>..........] - ETA: 0s
5632/8086 [===================>..........] - ETA: 0s
5840/8086 [====================>.........] - ETA: 0s
6048/8086 [=====================>........] - ETA: 0s
6256/8086 [======================>.......] - ETA: 0s
6464/8086 [======================>.......] - ETA: 0s
6672/8086 [=======================>......] - ETA: 0s
6880/8086 [========================>.....] - ETA: 0s
7088/8086 [=========================>....] - ETA: 0s
7296/8086 [==========================>...] - ETA: 0s
7504/8086 [==========================>...] - ETA: 0s
7712/8086 [===========================>..] - ETA: 0s
7920/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 249us/step

test score: [0.6792248588975868, 0.7148157309076439]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=256, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 33s - loss: 0.9110 - acc: 0.5887 - val_loss: 0.8522 - val_acc: 0.6379
Epoch 2/10
 - 30s - loss: 0.8029 - acc: 0.6584 - val_loss: 0.7842 - val_acc: 0.6724
Epoch 3/10
 - 30s - loss: 0.7408 - acc: 0.6882 - val_loss: 0.7214 - val_acc: 0.6987
Epoch 4/10
 - 31s - loss: 0.6995 - acc: 0.7122 - val_loss: 0.6962 - val_acc: 0.7160
Epoch 5/10
 - 30s - loss: 0.6724 - acc: 0.7231 - val_loss: 0.6863 - val_acc: 0.7228
Epoch 6/10
 - 30s - loss: 0.6490 - acc: 0.7340 - val_loss: 0.6800 - val_acc: 0.7239
Epoch 7/10
 - 30s - loss: 0.6326 - acc: 0.7434 - val_loss: 0.6954 - val_acc: 0.7231
Epoch 8/10
 - 30s - loss: 0.6179 - acc: 0.7485 - val_loss: 0.6791 - val_acc: 0.7280
Epoch 9/10
 - 30s - loss: 0.6085 - acc: 0.7531 - val_loss: 0.6509 - val_acc: 0.7370
Epoch 10/10
 - 30s - loss: 0.5970 - acc: 0.7594 - val_loss: 0.6622 - val_acc: 0.7288

  16/8086 [..............................] - ETA: 2s
 224/8086 [..............................] - ETA: 1s
 432/8086 [>.............................] - ETA: 1s
 640/8086 [=>............................] - ETA: 1s
 848/8086 [==>...........................] - ETA: 1s
1056/8086 [==>...........................] - ETA: 1s
1264/8086 [===>..........................] - ETA: 1s
1472/8086 [====>.........................] - ETA: 1s
1680/8086 [=====>........................] - ETA: 1s
1888/8086 [======>.......................] - ETA: 1s
2096/8086 [======>.......................] - ETA: 1s
2304/8086 [=======>......................] - ETA: 1s
2512/8086 [========>.....................] - ETA: 1s
2720/8086 [=========>....................] - ETA: 1s
2912/8086 [=========>....................] - ETA: 1s
3120/8086 [==========>...................] - ETA: 1s
3328/8086 [===========>..................] - ETA: 1s
3536/8086 [============>.................] - ETA: 1s
3744/8086 [============>.................] - ETA: 1s
3952/8086 [=============>................] - ETA: 1s
4160/8086 [==============>...............] - ETA: 0s
4368/8086 [===============>..............] - ETA: 0s
4576/8086 [===============>..............] - ETA: 0s
4784/8086 [================>.............] - ETA: 0s
4976/8086 [=================>............] - ETA: 0s
5184/8086 [==================>...........] - ETA: 0s
5392/8086 [===================>..........] - ETA: 0s
5600/8086 [===================>..........] - ETA: 0s
5808/8086 [====================>.........] - ETA: 0s
6016/8086 [=====================>........] - ETA: 0s
6224/8086 [======================>.......] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
6640/8086 [=======================>......] - ETA: 0s
6848/8086 [========================>.....] - ETA: 0s
7056/8086 [=========================>....] - ETA: 0s
7264/8086 [=========================>....] - ETA: 0s
7472/8086 [==========================>...] - ETA: 0s
7680/8086 [===========================>..] - ETA: 0s
7888/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 250us/step

test score: [0.6931439353456716, 0.7109819441156578]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=256, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 33s - loss: 0.9056 - acc: 0.5914 - val_loss: 0.8478 - val_acc: 0.6317
Epoch 2/10
 - 32s - loss: 0.7994 - acc: 0.6613 - val_loss: 0.7582 - val_acc: 0.6818
Epoch 3/10
 - 32s - loss: 0.7307 - acc: 0.6956 - val_loss: 0.7213 - val_acc: 0.7061
Epoch 4/10
 - 32s - loss: 0.6972 - acc: 0.7111 - val_loss: 0.7262 - val_acc: 0.7087
Epoch 5/10
 - 32s - loss: 0.6689 - acc: 0.7270 - val_loss: 0.7103 - val_acc: 0.7182
Epoch 6/10
 - 32s - loss: 0.6500 - acc: 0.7363 - val_loss: 0.6771 - val_acc: 0.7203
Epoch 7/10
 - 32s - loss: 0.6347 - acc: 0.7450 - val_loss: 0.6761 - val_acc: 0.7313
Epoch 8/10
 - 32s - loss: 0.6207 - acc: 0.7501 - val_loss: 0.6906 - val_acc: 0.7310
Epoch 9/10
 - 32s - loss: 0.6102 - acc: 0.7547 - val_loss: 0.6624 - val_acc: 0.7350
Epoch 10/10
 - 32s - loss: 0.6003 - acc: 0.7604 - val_loss: 0.6720 - val_acc: 0.7348

  16/8086 [..............................] - ETA: 2s
 224/8086 [..............................] - ETA: 2s
 432/8086 [>.............................] - ETA: 1s
 640/8086 [=>............................] - ETA: 1s
 848/8086 [==>...........................] - ETA: 1s
1056/8086 [==>...........................] - ETA: 1s
1264/8086 [===>..........................] - ETA: 1s
1472/8086 [====>.........................] - ETA: 1s
1680/8086 [=====>........................] - ETA: 1s
1888/8086 [======>.......................] - ETA: 1s
2096/8086 [======>.......................] - ETA: 1s
2304/8086 [=======>......................] - ETA: 1s
2512/8086 [========>.....................] - ETA: 1s
2720/8086 [=========>....................] - ETA: 1s
2912/8086 [=========>....................] - ETA: 1s
3120/8086 [==========>...................] - ETA: 1s
3328/8086 [===========>..................] - ETA: 1s
3536/8086 [============>.................] - ETA: 1s
3744/8086 [============>.................] - ETA: 1s
3952/8086 [=============>................] - ETA: 1s
4160/8086 [==============>...............] - ETA: 0s
4368/8086 [===============>..............] - ETA: 0s
4576/8086 [===============>..............] - ETA: 0s
4784/8086 [================>.............] - ETA: 0s
4992/8086 [=================>............] - ETA: 0s
5200/8086 [==================>...........] - ETA: 0s
5408/8086 [===================>..........] - ETA: 0s
5616/8086 [===================>..........] - ETA: 0s
5808/8086 [====================>.........] - ETA: 0s
6016/8086 [=====================>........] - ETA: 0s
6224/8086 [======================>.......] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
6640/8086 [=======================>......] - ETA: 0s
6832/8086 [========================>.....] - ETA: 0s
7024/8086 [=========================>....] - ETA: 0s
7216/8086 [=========================>....] - ETA: 0s
7408/8086 [==========================>...] - ETA: 0s
7616/8086 [===========================>..] - ETA: 0s
7824/8086 [============================>.] - ETA: 0s
8016/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 256us/step

test score: [0.6883789950665229, 0.7171654711995065]


Training new model, loss:categorical_crossentropy, optimizer=rmsprop, lstm_len=256, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 34s - loss: 0.9113 - acc: 0.5937 - val_loss: 0.8779 - val_acc: 0.6073
Epoch 2/10
 - 31s - loss: 0.8089 - acc: 0.6520 - val_loss: 0.7845 - val_acc: 0.6622
Epoch 3/10
 - 31s - loss: 0.7443 - acc: 0.6884 - val_loss: 0.7375 - val_acc: 0.6926
Epoch 4/10
 - 31s - loss: 0.7050 - acc: 0.7076 - val_loss: 0.7356 - val_acc: 0.7067
Epoch 5/10
 - 31s - loss: 0.6755 - acc: 0.7248 - val_loss: 0.7134 - val_acc: 0.7089
Epoch 6/10
 - 31s - loss: 0.6537 - acc: 0.7355 - val_loss: 0.6940 - val_acc: 0.7240
Epoch 7/10
 - 31s - loss: 0.6403 - acc: 0.7400 - val_loss: 0.6683 - val_acc: 0.7352
Epoch 8/10
 - 31s - loss: 0.6262 - acc: 0.7472 - val_loss: 0.6768 - val_acc: 0.7297
Epoch 9/10
 - 31s - loss: 0.6153 - acc: 0.7525 - val_loss: 0.6704 - val_acc: 0.7318
Epoch 10/10
 - 30s - loss: 0.6038 - acc: 0.7570 - val_loss: 0.6640 - val_acc: 0.7370

  16/8086 [..............................] - ETA: 2s
 224/8086 [..............................] - ETA: 2s
 416/8086 [>.............................] - ETA: 1s
 624/8086 [=>............................] - ETA: 1s
 832/8086 [==>...........................] - ETA: 1s
1040/8086 [==>...........................] - ETA: 1s
1248/8086 [===>..........................] - ETA: 1s
1456/8086 [====>.........................] - ETA: 1s
1664/8086 [=====>........................] - ETA: 1s
1872/8086 [=====>........................] - ETA: 1s
2080/8086 [======>.......................] - ETA: 1s
2288/8086 [=======>......................] - ETA: 1s
2496/8086 [========>.....................] - ETA: 1s
2704/8086 [=========>....................] - ETA: 1s
2912/8086 [=========>....................] - ETA: 1s
3120/8086 [==========>...................] - ETA: 1s
3312/8086 [===========>..................] - ETA: 1s
3520/8086 [============>.................] - ETA: 1s
3728/8086 [============>.................] - ETA: 1s
3936/8086 [=============>................] - ETA: 1s
4144/8086 [==============>...............] - ETA: 1s
4352/8086 [===============>..............] - ETA: 0s
4560/8086 [===============>..............] - ETA: 0s
4768/8086 [================>.............] - ETA: 0s
4976/8086 [=================>............] - ETA: 0s
5184/8086 [==================>...........] - ETA: 0s
5392/8086 [===================>..........] - ETA: 0s
5600/8086 [===================>..........] - ETA: 0s
5808/8086 [====================>.........] - ETA: 0s
6000/8086 [=====================>........] - ETA: 0s
6208/8086 [======================>.......] - ETA: 0s
6416/8086 [======================>.......] - ETA: 0s
6624/8086 [=======================>......] - ETA: 0s
6832/8086 [========================>.....] - ETA: 0s
7040/8086 [=========================>....] - ETA: 0s
7232/8086 [=========================>....] - ETA: 0s
7424/8086 [==========================>...] - ETA: 0s
7616/8086 [===========================>..] - ETA: 0s
7824/8086 [============================>.] - ETA: 0s
8032/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 257us/step

test score: [0.6820231196006394, 0.7187731882413071]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=32, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 11s - loss: 0.8830 - acc: 0.6035 - val_loss: 0.8072 - val_acc: 0.6579
Epoch 2/10
 - 8s - loss: 0.7779 - acc: 0.6750 - val_loss: 0.7677 - val_acc: 0.6792
Epoch 3/10
 - 8s - loss: 0.7445 - acc: 0.6983 - val_loss: 0.7531 - val_acc: 0.6920
Epoch 4/10
 - 8s - loss: 0.7210 - acc: 0.7096 - val_loss: 0.7341 - val_acc: 0.7084
Epoch 5/10
 - 8s - loss: 0.7048 - acc: 0.7139 - val_loss: 0.7262 - val_acc: 0.7076
Epoch 6/10
 - 8s - loss: 0.6909 - acc: 0.7232 - val_loss: 0.6998 - val_acc: 0.7202
Epoch 7/10
 - 8s - loss: 0.6752 - acc: 0.7291 - val_loss: 0.6935 - val_acc: 0.7279
Epoch 8/10
 - 8s - loss: 0.6582 - acc: 0.7331 - val_loss: 0.6760 - val_acc: 0.7280
Epoch 9/10
 - 8s - loss: 0.6512 - acc: 0.7385 - val_loss: 0.6679 - val_acc: 0.7327
Epoch 10/10
 - 8s - loss: 0.6381 - acc: 0.7417 - val_loss: 0.6634 - val_acc: 0.7322

  16/8086 [..............................] - ETA: 0s
 752/8086 [=>............................] - ETA: 0s
1488/8086 [====>.........................] - ETA: 0s
2208/8086 [=======>......................] - ETA: 0s
2912/8086 [=========>....................] - ETA: 0s
3648/8086 [============>.................] - ETA: 0s
4368/8086 [===============>..............] - ETA: 0s
5104/8086 [=================>............] - ETA: 0s
5840/8086 [====================>.........] - ETA: 0s
6576/8086 [=======================>......] - ETA: 0s
7312/8086 [==========================>...] - ETA: 0s
8048/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 70us/step

test score: [0.6979685361627715, 0.702819688350235]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=32, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 11s - loss: 0.8849 - acc: 0.6076 - val_loss: 0.8238 - val_acc: 0.6436
Epoch 2/10
 - 8s - loss: 0.7831 - acc: 0.6727 - val_loss: 0.7610 - val_acc: 0.6852
Epoch 3/10
 - 8s - loss: 0.7485 - acc: 0.6918 - val_loss: 0.7486 - val_acc: 0.6925
Epoch 4/10
 - 8s - loss: 0.7183 - acc: 0.7073 - val_loss: 0.7270 - val_acc: 0.6965
Epoch 5/10
 - 8s - loss: 0.7002 - acc: 0.7159 - val_loss: 0.7081 - val_acc: 0.7047
Epoch 6/10
 - 8s - loss: 0.6793 - acc: 0.7258 - val_loss: 0.7004 - val_acc: 0.7064
Epoch 7/10
 - 8s - loss: 0.6682 - acc: 0.7321 - val_loss: 0.6914 - val_acc: 0.7165
Epoch 8/10
 - 8s - loss: 0.6525 - acc: 0.7372 - val_loss: 0.6829 - val_acc: 0.7237
Epoch 9/10
 - 8s - loss: 0.6445 - acc: 0.7406 - val_loss: 0.6879 - val_acc: 0.7186
Epoch 10/10
 - 8s - loss: 0.6368 - acc: 0.7399 - val_loss: 0.6634 - val_acc: 0.7301

  16/8086 [..............................] - ETA: 0s
 640/8086 [=>............................] - ETA: 0s
1360/8086 [====>.........................] - ETA: 0s
2080/8086 [======>.......................] - ETA: 0s
2800/8086 [=========>....................] - ETA: 0s
3504/8086 [============>.................] - ETA: 0s
4176/8086 [==============>...............] - ETA: 0s
4864/8086 [=================>............] - ETA: 0s
5568/8086 [===================>..........] - ETA: 0s
6288/8086 [======================>.......] - ETA: 0s
7008/8086 [=========================>....] - ETA: 0s
7728/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 72us/step

test score: [0.6894098655321855, 0.7065298046152868]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=32, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 11s - loss: 0.8996 - acc: 0.5939 - val_loss: 0.8355 - val_acc: 0.6381
Epoch 2/10
 - 8s - loss: 0.8269 - acc: 0.6360 - val_loss: 0.7902 - val_acc: 0.6643
Epoch 3/10
 - 8s - loss: 0.7698 - acc: 0.6746 - val_loss: 0.7819 - val_acc: 0.6586
Epoch 4/10
 - 8s - loss: 0.7326 - acc: 0.6954 - val_loss: 0.7301 - val_acc: 0.6984
Epoch 5/10
 - 8s - loss: 0.7128 - acc: 0.7081 - val_loss: 0.7247 - val_acc: 0.7007
Epoch 6/10
 - 8s - loss: 0.6987 - acc: 0.7143 - val_loss: 0.7266 - val_acc: 0.6906
Epoch 7/10
 - 9s - loss: 0.6803 - acc: 0.7236 - val_loss: 0.6972 - val_acc: 0.7225
Epoch 8/10
 - 8s - loss: 0.6675 - acc: 0.7311 - val_loss: 0.6896 - val_acc: 0.7219
Epoch 9/10
 - 8s - loss: 0.6526 - acc: 0.7371 - val_loss: 0.6706 - val_acc: 0.7335
Epoch 10/10
 - 8s - loss: 0.6457 - acc: 0.7423 - val_loss: 0.6699 - val_acc: 0.7305

  16/8086 [..............................] - ETA: 0s
 704/8086 [=>............................] - ETA: 0s
1424/8086 [====>.........................] - ETA: 0s
2128/8086 [======>.......................] - ETA: 0s
2848/8086 [=========>....................] - ETA: 0s
3568/8086 [============>.................] - ETA: 0s
4288/8086 [==============>...............] - ETA: 0s
5008/8086 [=================>............] - ETA: 0s
5728/8086 [====================>.........] - ETA: 0s
6448/8086 [======================>.......] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7888/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 71us/step

test score: [0.70821382578959, 0.705540440281871]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=32, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 11s - loss: 0.9017 - acc: 0.5962 - val_loss: 0.8142 - val_acc: 0.6603
Epoch 2/10
 - 8s - loss: 0.7927 - acc: 0.6618 - val_loss: 0.7625 - val_acc: 0.6844
Epoch 3/10
 - 8s - loss: 0.7540 - acc: 0.6914 - val_loss: 0.7460 - val_acc: 0.6951
Epoch 4/10
 - 8s - loss: 0.7250 - acc: 0.7027 - val_loss: 0.7450 - val_acc: 0.6905
Epoch 5/10
 - 8s - loss: 0.7057 - acc: 0.7101 - val_loss: 0.7255 - val_acc: 0.6936
Epoch 6/10
 - 8s - loss: 0.6890 - acc: 0.7185 - val_loss: 0.7105 - val_acc: 0.7123
Epoch 7/10
 - 8s - loss: 0.6775 - acc: 0.7264 - val_loss: 0.7067 - val_acc: 0.7151
Epoch 8/10
 - 8s - loss: 0.6655 - acc: 0.7328 - val_loss: 0.7210 - val_acc: 0.7070
Epoch 9/10
 - 8s - loss: 0.6556 - acc: 0.7358 - val_loss: 0.6755 - val_acc: 0.7294
Epoch 10/10
 - 8s - loss: 0.6440 - acc: 0.7402 - val_loss: 0.6980 - val_acc: 0.7217

  16/8086 [..............................] - ETA: 0s
 736/8086 [=>............................] - ETA: 0s
1440/8086 [====>.........................] - ETA: 0s
2144/8086 [======>.......................] - ETA: 0s
2848/8086 [=========>....................] - ETA: 0s
3552/8086 [============>.................] - ETA: 0s
4272/8086 [==============>...............] - ETA: 0s
4992/8086 [=================>............] - ETA: 0s
5696/8086 [====================>.........] - ETA: 0s
6400/8086 [======================>.......] - ETA: 0s
7104/8086 [=========================>....] - ETA: 0s
7808/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 72us/step

test score: [0.7156256959487269, 0.696388820197775]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=32, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 11s - loss: 0.9042 - acc: 0.5959 - val_loss: 0.8239 - val_acc: 0.6594
Epoch 2/10
 - 8s - loss: 0.8045 - acc: 0.6631 - val_loss: 0.7922 - val_acc: 0.6673
Epoch 3/10
 - 8s - loss: 0.7744 - acc: 0.6781 - val_loss: 0.7596 - val_acc: 0.6874
Epoch 4/10
 - 8s - loss: 0.7417 - acc: 0.6963 - val_loss: 0.7374 - val_acc: 0.6959
Epoch 5/10
 - 8s - loss: 0.7175 - acc: 0.7078 - val_loss: 0.7466 - val_acc: 0.6844
Epoch 6/10
 - 8s - loss: 0.6993 - acc: 0.7146 - val_loss: 0.7167 - val_acc: 0.7104
Epoch 7/10
 - 8s - loss: 0.6875 - acc: 0.7208 - val_loss: 0.6953 - val_acc: 0.7194
Epoch 8/10
 - 8s - loss: 0.6772 - acc: 0.7255 - val_loss: 0.6822 - val_acc: 0.7248
Epoch 9/10
 - 8s - loss: 0.6634 - acc: 0.7339 - val_loss: 0.6885 - val_acc: 0.7127
Epoch 10/10
 - 8s - loss: 0.6543 - acc: 0.7360 - val_loss: 0.6722 - val_acc: 0.7265

  16/8086 [..............................] - ETA: 0s
 720/8086 [=>............................] - ETA: 0s
1424/8086 [====>.........................] - ETA: 0s
2112/8086 [======>.......................] - ETA: 0s
2816/8086 [=========>....................] - ETA: 0s
3504/8086 [============>.................] - ETA: 0s
4208/8086 [==============>...............] - ETA: 0s
4912/8086 [=================>............] - ETA: 0s
5632/8086 [===================>..........] - ETA: 0s
6336/8086 [======================>.......] - ETA: 0s
7040/8086 [=========================>....] - ETA: 0s
7744/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 73us/step

test score: [0.698950072488306, 0.7069008162403178]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=64, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 13s - loss: 0.9023 - acc: 0.5923 - val_loss: 0.8346 - val_acc: 0.6532
Epoch 2/10
 - 9s - loss: 0.7999 - acc: 0.6605 - val_loss: 0.7721 - val_acc: 0.6829
Epoch 3/10
 - 9s - loss: 0.7428 - acc: 0.6905 - val_loss: 0.7657 - val_acc: 0.6852
Epoch 4/10
 - 10s - loss: 0.7090 - acc: 0.7041 - val_loss: 0.7083 - val_acc: 0.7087
Epoch 5/10
 - 10s - loss: 0.6784 - acc: 0.7197 - val_loss: 0.7003 - val_acc: 0.7143
Epoch 6/10
 - 9s - loss: 0.6540 - acc: 0.7333 - val_loss: 0.6890 - val_acc: 0.7287
Epoch 7/10
 - 10s - loss: 0.6436 - acc: 0.7389 - val_loss: 0.6684 - val_acc: 0.7296
Epoch 8/10
 - 9s - loss: 0.6322 - acc: 0.7432 - val_loss: 0.6732 - val_acc: 0.7296
Epoch 9/10
 - 9s - loss: 0.6211 - acc: 0.7474 - val_loss: 0.6576 - val_acc: 0.7324
Epoch 10/10
 - 9s - loss: 0.6109 - acc: 0.7489 - val_loss: 0.6669 - val_acc: 0.7297

  16/8086 [..............................] - ETA: 0s
 640/8086 [=>............................] - ETA: 0s
1216/8086 [===>..........................] - ETA: 0s
1792/8086 [=====>........................] - ETA: 0s
2416/8086 [=======>......................] - ETA: 0s
3024/8086 [==========>...................] - ETA: 0s
3648/8086 [============>.................] - ETA: 0s
4256/8086 [==============>...............] - ETA: 0s
4880/8086 [=================>............] - ETA: 0s
5504/8086 [===================>..........] - ETA: 0s
6128/8086 [=====================>........] - ETA: 0s
6736/8086 [=======================>......] - ETA: 0s
7360/8086 [==========================>...] - ETA: 0s
7984/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 83us/step

test score: [0.6884221152860636, 0.7097452386915167]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=64, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 13s - loss: 0.8831 - acc: 0.6038 - val_loss: 0.8053 - val_acc: 0.6580
Epoch 2/10
 - 10s - loss: 0.7764 - acc: 0.6742 - val_loss: 0.7773 - val_acc: 0.6684
Epoch 3/10
 - 10s - loss: 0.7348 - acc: 0.6954 - val_loss: 0.7545 - val_acc: 0.6746
Epoch 4/10
 - 10s - loss: 0.7008 - acc: 0.7102 - val_loss: 0.7295 - val_acc: 0.6981
Epoch 5/10
 - 10s - loss: 0.6719 - acc: 0.7244 - val_loss: 0.6927 - val_acc: 0.7205
Epoch 6/10
 - 10s - loss: 0.6560 - acc: 0.7331 - val_loss: 0.6743 - val_acc: 0.7328
Epoch 7/10
 - 10s - loss: 0.6384 - acc: 0.7420 - val_loss: 0.6711 - val_acc: 0.7285
Epoch 8/10
 - 10s - loss: 0.6269 - acc: 0.7454 - val_loss: 0.6501 - val_acc: 0.7409
Epoch 9/10
 - 10s - loss: 0.6191 - acc: 0.7476 - val_loss: 0.6604 - val_acc: 0.7342
Epoch 10/10
 - 10s - loss: 0.6116 - acc: 0.7522 - val_loss: 0.6467 - val_acc: 0.7418

  16/8086 [..............................] - ETA: 0s
 624/8086 [=>............................] - ETA: 0s
1232/8086 [===>..........................] - ETA: 0s
1840/8086 [=====>........................] - ETA: 0s
2464/8086 [========>.....................] - ETA: 0s
3072/8086 [==========>...................] - ETA: 0s
3696/8086 [============>.................] - ETA: 0s
4304/8086 [==============>...............] - ETA: 0s
4912/8086 [=================>............] - ETA: 0s
5520/8086 [===================>..........] - ETA: 0s
6144/8086 [=====================>........] - ETA: 0s
6752/8086 [========================>.....] - ETA: 0s
7360/8086 [==========================>...] - ETA: 0s
7968/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 83us/step

test score: [0.669775912365405, 0.7140737076575822]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=64, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 13s - loss: 0.9013 - acc: 0.5923 - val_loss: 0.8407 - val_acc: 0.6387
Epoch 2/10
 - 10s - loss: 0.8026 - acc: 0.6611 - val_loss: 0.7815 - val_acc: 0.6784
Epoch 3/10
 - 10s - loss: 0.7459 - acc: 0.6954 - val_loss: 0.7482 - val_acc: 0.7025
Epoch 4/10
 - 10s - loss: 0.7224 - acc: 0.7046 - val_loss: 0.7217 - val_acc: 0.7089
Epoch 5/10
 - 10s - loss: 0.6954 - acc: 0.7136 - val_loss: 0.7049 - val_acc: 0.7087
Epoch 6/10
 - 10s - loss: 0.6709 - acc: 0.7256 - val_loss: 0.6888 - val_acc: 0.7203
Epoch 7/10
 - 10s - loss: 0.6552 - acc: 0.7293 - val_loss: 0.6949 - val_acc: 0.7152
Epoch 8/10
 - 10s - loss: 0.6442 - acc: 0.7338 - val_loss: 0.6805 - val_acc: 0.7267
Epoch 9/10
 - 10s - loss: 0.6323 - acc: 0.7419 - val_loss: 0.6661 - val_acc: 0.7330
Epoch 10/10
 - 10s - loss: 0.6231 - acc: 0.7461 - val_loss: 0.6605 - val_acc: 0.7314

  16/8086 [..............................] - ETA: 0s
 608/8086 [=>............................] - ETA: 0s
1216/8086 [===>..........................] - ETA: 0s
1824/8086 [=====>........................] - ETA: 0s
2432/8086 [========>.....................] - ETA: 0s
3040/8086 [==========>...................] - ETA: 0s
3648/8086 [============>.................] - ETA: 0s
4256/8086 [==============>...............] - ETA: 0s
4864/8086 [=================>............] - ETA: 0s
5472/8086 [===================>..........] - ETA: 0s
6080/8086 [=====================>........] - ETA: 0s
6688/8086 [=======================>......] - ETA: 0s
7232/8086 [=========================>....] - ETA: 0s
7792/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 85us/step

test score: [0.6787312385206201, 0.7086322038237953]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=64, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 13s - loss: 0.8883 - acc: 0.6032 - val_loss: 0.8403 - val_acc: 0.6432
Epoch 2/10
 - 10s - loss: 0.7900 - acc: 0.6652 - val_loss: 0.7800 - val_acc: 0.6781
Epoch 3/10
 - 10s - loss: 0.7421 - acc: 0.6940 - val_loss: 0.7393 - val_acc: 0.6993
Epoch 4/10
 - 10s - loss: 0.7046 - acc: 0.7072 - val_loss: 0.7082 - val_acc: 0.7021
Epoch 5/10
 - 10s - loss: 0.6770 - acc: 0.7209 - val_loss: 0.7089 - val_acc: 0.7096
Epoch 6/10
 - 10s - loss: 0.6587 - acc: 0.7308 - val_loss: 0.6813 - val_acc: 0.7214
Epoch 7/10
 - 10s - loss: 0.6418 - acc: 0.7400 - val_loss: 0.6583 - val_acc: 0.7301
Epoch 8/10
 - 10s - loss: 0.6278 - acc: 0.7432 - val_loss: 0.6483 - val_acc: 0.7364
Epoch 9/10
 - 10s - loss: 0.6162 - acc: 0.7480 - val_loss: 0.6555 - val_acc: 0.7311
Epoch 10/10
 - 10s - loss: 0.6084 - acc: 0.7517 - val_loss: 0.6403 - val_acc: 0.7382

  16/8086 [..............................] - ETA: 0s
 608/8086 [=>............................] - ETA: 0s
1216/8086 [===>..........................] - ETA: 0s
1808/8086 [=====>........................] - ETA: 0s
2416/8086 [=======>......................] - ETA: 0s
3008/8086 [==========>...................] - ETA: 0s
3616/8086 [============>.................] - ETA: 0s
4208/8086 [==============>...............] - ETA: 0s
4816/8086 [================>.............] - ETA: 0s
5424/8086 [===================>..........] - ETA: 0s
6032/8086 [=====================>........] - ETA: 0s
6624/8086 [=======================>......] - ETA: 0s
7232/8086 [=========================>....] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 84us/step

test score: [0.6649156521665793, 0.7181548355181796]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=64, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 13s - loss: 0.9039 - acc: 0.5924 - val_loss: 0.8533 - val_acc: 0.6351
Epoch 2/10
 - 10s - loss: 0.8134 - acc: 0.6497 - val_loss: 0.7810 - val_acc: 0.6786
Epoch 3/10
 - 10s - loss: 0.7518 - acc: 0.6872 - val_loss: 0.7568 - val_acc: 0.6664
Epoch 4/10
 - 10s - loss: 0.7176 - acc: 0.7040 - val_loss: 0.7320 - val_acc: 0.6987
Epoch 5/10
 - 10s - loss: 0.6885 - acc: 0.7168 - val_loss: 0.7045 - val_acc: 0.7171
Epoch 6/10
 - 10s - loss: 0.6653 - acc: 0.7332 - val_loss: 0.6881 - val_acc: 0.7191
Epoch 7/10
 - 10s - loss: 0.6496 - acc: 0.7371 - val_loss: 0.6667 - val_acc: 0.7325
Epoch 8/10
 - 10s - loss: 0.6366 - acc: 0.7435 - val_loss: 0.6662 - val_acc: 0.7293
Epoch 9/10
 - 10s - loss: 0.6264 - acc: 0.7452 - val_loss: 0.6524 - val_acc: 0.7352
Epoch 10/10
 - 10s - loss: 0.6145 - acc: 0.7492 - val_loss: 0.6477 - val_acc: 0.7375

  16/8086 [..............................] - ETA: 0s
 544/8086 [=>............................] - ETA: 0s
1072/8086 [==>...........................] - ETA: 0s
1600/8086 [====>.........................] - ETA: 0s
2128/8086 [======>.......................] - ETA: 0s
2656/8086 [========>.....................] - ETA: 0s
3184/8086 [==========>...................] - ETA: 0s
3696/8086 [============>.................] - ETA: 0s
4208/8086 [==============>...............] - ETA: 0s
4736/8086 [================>.............] - ETA: 0s
5264/8086 [==================>...........] - ETA: 0s
5792/8086 [====================>.........] - ETA: 0s
6320/8086 [======================>.......] - ETA: 0s
6848/8086 [========================>.....] - ETA: 0s
7344/8086 [==========================>...] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 97us/step

test score: [0.6740178778093108, 0.7141973781992591]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=128, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 27s - loss: 0.8916 - acc: 0.6019 - val_loss: 0.8553 - val_acc: 0.6540
Epoch 2/10
 - 22s - loss: 0.7911 - acc: 0.6724 - val_loss: 0.7705 - val_acc: 0.6878
Epoch 3/10
 - 23s - loss: 0.7329 - acc: 0.6985 - val_loss: 0.7340 - val_acc: 0.7024
Epoch 4/10
 - 23s - loss: 0.6865 - acc: 0.7183 - val_loss: 0.7001 - val_acc: 0.7177
Epoch 5/10
 - 23s - loss: 0.6536 - acc: 0.7318 - val_loss: 0.6746 - val_acc: 0.7256
Epoch 6/10
 - 23s - loss: 0.6335 - acc: 0.7411 - val_loss: 0.6668 - val_acc: 0.7350
Epoch 7/10
 - 23s - loss: 0.6195 - acc: 0.7467 - val_loss: 0.6540 - val_acc: 0.7376
Epoch 8/10
 - 21s - loss: 0.6046 - acc: 0.7522 - val_loss: 0.6456 - val_acc: 0.7426
Epoch 9/10
 - 20s - loss: 0.5937 - acc: 0.7561 - val_loss: 0.6473 - val_acc: 0.7364
Epoch 10/10
 - 20s - loss: 0.5837 - acc: 0.7642 - val_loss: 0.6458 - val_acc: 0.7407

  16/8086 [..............................] - ETA: 1s
 256/8086 [..............................] - ETA: 1s
 496/8086 [>.............................] - ETA: 1s
 736/8086 [=>............................] - ETA: 1s
 976/8086 [==>...........................] - ETA: 1s
1200/8086 [===>..........................] - ETA: 1s
1424/8086 [====>.........................] - ETA: 1s
1648/8086 [=====>........................] - ETA: 1s
1888/8086 [======>.......................] - ETA: 1s
2128/8086 [======>.......................] - ETA: 1s
2368/8086 [=======>......................] - ETA: 1s
2608/8086 [========>.....................] - ETA: 1s
2848/8086 [=========>....................] - ETA: 1s
3088/8086 [==========>...................] - ETA: 1s
3328/8086 [===========>..................] - ETA: 1s
3568/8086 [============>.................] - ETA: 1s
3808/8086 [=============>................] - ETA: 0s
4048/8086 [==============>...............] - ETA: 0s
4288/8086 [==============>...............] - ETA: 0s
4512/8086 [===============>..............] - ETA: 0s
4736/8086 [================>.............] - ETA: 0s
4960/8086 [=================>............] - ETA: 0s
5200/8086 [==================>...........] - ETA: 0s
5440/8086 [===================>..........] - ETA: 0s
5680/8086 [====================>.........] - ETA: 0s
5920/8086 [====================>.........] - ETA: 0s
6128/8086 [=====================>........] - ETA: 0s
6368/8086 [======================>.......] - ETA: 0s
6608/8086 [=======================>......] - ETA: 0s
6848/8086 [========================>.....] - ETA: 0s
7088/8086 [=========================>....] - ETA: 0s
7328/8086 [==========================>...] - ETA: 0s
7568/8086 [===========================>..] - ETA: 0s
7808/8086 [===========================>..] - ETA: 0s
8048/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 223us/step

test score: [0.7023950227336544, 0.7166707890327986]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=128, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 24s - loss: 0.9094 - acc: 0.5860 - val_loss: 0.8433 - val_acc: 0.6413
Epoch 2/10
 - 21s - loss: 0.7981 - acc: 0.6611 - val_loss: 0.7564 - val_acc: 0.6937
Epoch 3/10
 - 21s - loss: 0.7303 - acc: 0.6989 - val_loss: 0.7317 - val_acc: 0.6984
Epoch 4/10
 - 21s - loss: 0.6909 - acc: 0.7157 - val_loss: 0.7033 - val_acc: 0.7137
Epoch 5/10
 - 21s - loss: 0.6565 - acc: 0.7303 - val_loss: 0.6775 - val_acc: 0.7319
Epoch 6/10
 - 24s - loss: 0.6346 - acc: 0.7415 - val_loss: 0.6590 - val_acc: 0.7330
Epoch 7/10
 - 21s - loss: 0.6194 - acc: 0.7484 - val_loss: 0.6496 - val_acc: 0.7352
Epoch 8/10
 - 21s - loss: 0.6072 - acc: 0.7522 - val_loss: 0.6449 - val_acc: 0.7418
Epoch 9/10
 - 21s - loss: 0.5944 - acc: 0.7589 - val_loss: 0.6475 - val_acc: 0.7379
Epoch 10/10
 - 23s - loss: 0.5876 - acc: 0.7620 - val_loss: 0.6483 - val_acc: 0.7347

  16/8086 [..............................] - ETA: 2s
 208/8086 [..............................] - ETA: 2s
 400/8086 [>.............................] - ETA: 2s
 592/8086 [=>............................] - ETA: 2s
 784/8086 [=>............................] - ETA: 2s
 976/8086 [==>...........................] - ETA: 1s
1168/8086 [===>..........................] - ETA: 1s
1360/8086 [====>.........................] - ETA: 1s
1552/8086 [====>.........................] - ETA: 1s
1744/8086 [=====>........................] - ETA: 1s
1936/8086 [======>.......................] - ETA: 1s
2144/8086 [======>.......................] - ETA: 1s
2352/8086 [=======>......................] - ETA: 1s
2544/8086 [========>.....................] - ETA: 1s
2736/8086 [=========>....................] - ETA: 1s
2928/8086 [=========>....................] - ETA: 1s
3120/8086 [==========>...................] - ETA: 1s
3312/8086 [===========>..................] - ETA: 1s
3504/8086 [============>.................] - ETA: 1s
3696/8086 [============>.................] - ETA: 1s
3888/8086 [=============>................] - ETA: 1s
4080/8086 [==============>...............] - ETA: 1s
4272/8086 [==============>...............] - ETA: 1s
4464/8086 [===============>..............] - ETA: 0s
4656/8086 [================>.............] - ETA: 0s
4848/8086 [================>.............] - ETA: 0s
5040/8086 [=================>............] - ETA: 0s
5232/8086 [==================>...........] - ETA: 0s
5424/8086 [===================>..........] - ETA: 0s
5616/8086 [===================>..........] - ETA: 0s
5808/8086 [====================>.........] - ETA: 0s
6000/8086 [=====================>........] - ETA: 0s
6192/8086 [=====================>........] - ETA: 0s
6384/8086 [======================>.......] - ETA: 0s
6576/8086 [=======================>......] - ETA: 0s
6768/8086 [========================>.....] - ETA: 0s
6960/8086 [========================>.....] - ETA: 0s
7152/8086 [=========================>....] - ETA: 0s
7344/8086 [==========================>...] - ETA: 0s
7536/8086 [==========================>...] - ETA: 0s
7712/8086 [===========================>..] - ETA: 0s
7904/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 273us/step

test score: [0.6878365854830132, 0.7137026960251799]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=128, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 28s - loss: 0.8989 - acc: 0.5951 - val_loss: 0.8355 - val_acc: 0.6529
Epoch 2/10
 - 24s - loss: 0.7905 - acc: 0.6695 - val_loss: 0.8122 - val_acc: 0.6458
Epoch 3/10
 - 24s - loss: 0.7290 - acc: 0.6987 - val_loss: 0.7221 - val_acc: 0.6979
Epoch 4/10
 - 24s - loss: 0.6905 - acc: 0.7141 - val_loss: 0.7053 - val_acc: 0.7137
Epoch 5/10
 - 22s - loss: 0.6587 - acc: 0.7311 - val_loss: 0.7011 - val_acc: 0.7157
Epoch 6/10
 - 23s - loss: 0.6407 - acc: 0.7407 - val_loss: 0.6825 - val_acc: 0.7307
Epoch 7/10
 - 25s - loss: 0.6251 - acc: 0.7485 - val_loss: 0.6586 - val_acc: 0.7372
Epoch 8/10
 - 22s - loss: 0.6139 - acc: 0.7497 - val_loss: 0.6509 - val_acc: 0.7395
Epoch 9/10
 - 22s - loss: 0.6033 - acc: 0.7530 - val_loss: 0.6540 - val_acc: 0.7379
Epoch 10/10
 - 22s - loss: 0.5920 - acc: 0.7600 - val_loss: 0.6456 - val_acc: 0.7404

  16/8086 [..............................] - ETA: 2s
 240/8086 [..............................] - ETA: 1s
 432/8086 [>.............................] - ETA: 1s
 656/8086 [=>............................] - ETA: 1s
 880/8086 [==>...........................] - ETA: 1s
1088/8086 [===>..........................] - ETA: 1s
1312/8086 [===>..........................] - ETA: 1s
1520/8086 [====>.........................] - ETA: 1s
1728/8086 [=====>........................] - ETA: 1s
1952/8086 [======>.......................] - ETA: 1s
2176/8086 [=======>......................] - ETA: 1s
2400/8086 [=======>......................] - ETA: 1s
2624/8086 [========>.....................] - ETA: 1s
2848/8086 [=========>....................] - ETA: 1s
3072/8086 [==========>...................] - ETA: 1s
3296/8086 [===========>..................] - ETA: 1s
3520/8086 [============>.................] - ETA: 1s
3744/8086 [============>.................] - ETA: 1s
3952/8086 [=============>................] - ETA: 1s
4176/8086 [==============>...............] - ETA: 0s
4384/8086 [===============>..............] - ETA: 0s
4608/8086 [================>.............] - ETA: 0s
4832/8086 [================>.............] - ETA: 0s
5040/8086 [=================>............] - ETA: 0s
5232/8086 [==================>...........] - ETA: 0s
5440/8086 [===================>..........] - ETA: 0s
5648/8086 [===================>..........] - ETA: 0s
5872/8086 [====================>.........] - ETA: 0s
6080/8086 [=====================>........] - ETA: 0s
6288/8086 [======================>.......] - ETA: 0s
6512/8086 [=======================>......] - ETA: 0s
6736/8086 [=======================>......] - ETA: 0s
6944/8086 [========================>.....] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7392/8086 [==========================>...] - ETA: 0s
7616/8086 [===========================>..] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8048/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 243us/step

test score: [0.6866184026113454, 0.7156814246993828]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=128, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 26s - loss: 0.9002 - acc: 0.5954 - val_loss: 0.8411 - val_acc: 0.6442
Epoch 2/10
 - 22s - loss: 0.8007 - acc: 0.6554 - val_loss: 0.7670 - val_acc: 0.6739
Epoch 3/10
 - 22s - loss: 0.7364 - acc: 0.6902 - val_loss: 0.7287 - val_acc: 0.6956
Epoch 4/10
 - 22s - loss: 0.6921 - acc: 0.7137 - val_loss: 0.6885 - val_acc: 0.7256
Epoch 5/10
 - 22s - loss: 0.6611 - acc: 0.7304 - val_loss: 0.6985 - val_acc: 0.7206
Epoch 6/10
 - 22s - loss: 0.6416 - acc: 0.7410 - val_loss: 0.6820 - val_acc: 0.7251
Epoch 7/10
 - 22s - loss: 0.6250 - acc: 0.7463 - val_loss: 0.6584 - val_acc: 0.7339
Epoch 8/10
 - 22s - loss: 0.6118 - acc: 0.7530 - val_loss: 0.6544 - val_acc: 0.7370
Epoch 9/10
 - 22s - loss: 0.6025 - acc: 0.7569 - val_loss: 0.6440 - val_acc: 0.7403
Epoch 10/10
 - 22s - loss: 0.5916 - acc: 0.7610 - val_loss: 0.6410 - val_acc: 0.7325

  16/8086 [..............................] - ETA: 2s
 240/8086 [..............................] - ETA: 1s
 464/8086 [>.............................] - ETA: 1s
 688/8086 [=>............................] - ETA: 1s
 912/8086 [==>...........................] - ETA: 1s
1136/8086 [===>..........................] - ETA: 1s
1360/8086 [====>.........................] - ETA: 1s
1584/8086 [====>.........................] - ETA: 1s
1792/8086 [=====>........................] - ETA: 1s
2016/8086 [======>.......................] - ETA: 1s
2240/8086 [=======>......................] - ETA: 1s
2464/8086 [========>.....................] - ETA: 1s
2688/8086 [========>.....................] - ETA: 1s
2912/8086 [=========>....................] - ETA: 1s
3136/8086 [==========>...................] - ETA: 1s
3360/8086 [===========>..................] - ETA: 1s
3584/8086 [============>.................] - ETA: 1s
3808/8086 [=============>................] - ETA: 1s
4032/8086 [=============>................] - ETA: 0s
4240/8086 [==============>...............] - ETA: 0s
4464/8086 [===============>..............] - ETA: 0s
4688/8086 [================>.............] - ETA: 0s
4896/8086 [=================>............] - ETA: 0s
5120/8086 [=================>............] - ETA: 0s
5344/8086 [==================>...........] - ETA: 0s
5568/8086 [===================>..........] - ETA: 0s
5792/8086 [====================>.........] - ETA: 0s
6016/8086 [=====================>........] - ETA: 0s
6240/8086 [======================>.......] - ETA: 0s
6464/8086 [======================>.......] - ETA: 0s
6688/8086 [=======================>......] - ETA: 0s
6912/8086 [========================>.....] - ETA: 0s
7136/8086 [=========================>....] - ETA: 0s
7360/8086 [==========================>...] - ETA: 0s
7584/8086 [===========================>..] - ETA: 0s
7808/8086 [===========================>..] - ETA: 0s
8032/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 234us/step

test score: [0.6699082206491489, 0.7200098936580769]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=128, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 27s - loss: 0.9035 - acc: 0.5920 - val_loss: 0.8326 - val_acc: 0.6532
Epoch 2/10
 - 27s - loss: 0.8071 - acc: 0.6567 - val_loss: 0.7874 - val_acc: 0.6750
Epoch 3/10
 - 27s - loss: 0.7430 - acc: 0.6921 - val_loss: 0.7487 - val_acc: 0.6773
Epoch 4/10
 - 26s - loss: 0.7010 - acc: 0.7104 - val_loss: 0.7090 - val_acc: 0.7032
Epoch 5/10
 - 26s - loss: 0.6688 - acc: 0.7275 - val_loss: 0.6777 - val_acc: 0.7291
Epoch 6/10
 - 26s - loss: 0.6454 - acc: 0.7369 - val_loss: 0.6735 - val_acc: 0.7342
Epoch 7/10
 - 26s - loss: 0.6320 - acc: 0.7433 - val_loss: 0.6598 - val_acc: 0.7348
Epoch 8/10
 - 27s - loss: 0.6207 - acc: 0.7466 - val_loss: 0.6540 - val_acc: 0.7381
Epoch 9/10
 - 25s - loss: 0.6089 - acc: 0.7529 - val_loss: 0.6512 - val_acc: 0.7341
Epoch 10/10
 - 24s - loss: 0.6014 - acc: 0.7554 - val_loss: 0.6721 - val_acc: 0.7265

  16/8086 [..............................] - ETA: 2s
 192/8086 [..............................] - ETA: 2s
 384/8086 [>.............................] - ETA: 2s
 576/8086 [=>............................] - ETA: 2s
 768/8086 [=>............................] - ETA: 2s
 960/8086 [==>...........................] - ETA: 1s
1152/8086 [===>..........................] - ETA: 1s
1344/8086 [===>..........................] - ETA: 1s
1536/8086 [====>.........................] - ETA: 1s
1728/8086 [=====>........................] - ETA: 1s
1920/8086 [======>.......................] - ETA: 1s
2112/8086 [======>.......................] - ETA: 1s
2304/8086 [=======>......................] - ETA: 1s
2496/8086 [========>.....................] - ETA: 1s
2688/8086 [========>.....................] - ETA: 1s
2880/8086 [=========>....................] - ETA: 1s
3072/8086 [==========>...................] - ETA: 1s
3264/8086 [===========>..................] - ETA: 1s
3456/8086 [===========>..................] - ETA: 1s
3648/8086 [============>.................] - ETA: 1s
3840/8086 [=============>................] - ETA: 1s
4032/8086 [=============>................] - ETA: 1s
4224/8086 [==============>...............] - ETA: 1s
4416/8086 [===============>..............] - ETA: 0s
4608/8086 [================>.............] - ETA: 0s
4800/8086 [================>.............] - ETA: 0s
4992/8086 [=================>............] - ETA: 0s
5184/8086 [==================>...........] - ETA: 0s
5376/8086 [==================>...........] - ETA: 0s
5568/8086 [===================>..........] - ETA: 0s
5760/8086 [====================>.........] - ETA: 0s
5952/8086 [=====================>........] - ETA: 0s
6144/8086 [=====================>........] - ETA: 0s
6320/8086 [======================>.......] - ETA: 0s
6512/8086 [=======================>......] - ETA: 0s
6704/8086 [=======================>......] - ETA: 0s
6896/8086 [========================>.....] - ETA: 0s
7088/8086 [=========================>....] - ETA: 0s
7280/8086 [==========================>...] - ETA: 0s
7456/8086 [==========================>...] - ETA: 0s
7648/8086 [===========================>..] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8032/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 273us/step

test score: [0.6879120179252285, 0.7072718278506059]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=256, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 51s - loss: 0.8953 - acc: 0.5985 - val_loss: 0.8424 - val_acc: 0.6469
Epoch 2/10
 - 54s - loss: 0.7736 - acc: 0.6727 - val_loss: 0.7411 - val_acc: 0.6998
Epoch 3/10
 - 56s - loss: 0.7068 - acc: 0.7092 - val_loss: 0.7302 - val_acc: 0.7106
Epoch 4/10
 - 51s - loss: 0.6651 - acc: 0.7294 - val_loss: 0.6724 - val_acc: 0.7335
Epoch 5/10
 - 51s - loss: 0.6404 - acc: 0.7399 - val_loss: 0.6623 - val_acc: 0.7348
Epoch 6/10
 - 51s - loss: 0.6223 - acc: 0.7474 - val_loss: 0.6533 - val_acc: 0.7347
Epoch 7/10
 - 51s - loss: 0.6070 - acc: 0.7521 - val_loss: 0.6468 - val_acc: 0.7358
Epoch 8/10
 - 51s - loss: 0.5949 - acc: 0.7575 - val_loss: 0.6600 - val_acc: 0.7324
Epoch 9/10
 - 51s - loss: 0.5812 - acc: 0.7645 - val_loss: 0.6388 - val_acc: 0.7409
Epoch 10/10
 - 51s - loss: 0.5658 - acc: 0.7713 - val_loss: 0.6452 - val_acc: 0.7359

  16/8086 [..............................] - ETA: 3s
 128/8086 [..............................] - ETA: 3s
 240/8086 [..............................] - ETA: 3s
 352/8086 [>.............................] - ETA: 3s
 464/8086 [>.............................] - ETA: 3s
 592/8086 [=>............................] - ETA: 3s
 704/8086 [=>............................] - ETA: 3s
 832/8086 [==>...........................] - ETA: 3s
 960/8086 [==>...........................] - ETA: 3s
1088/8086 [===>..........................] - ETA: 3s
1216/8086 [===>..........................] - ETA: 3s
1344/8086 [===>..........................] - ETA: 2s
1472/8086 [====>.........................] - ETA: 2s
1600/8086 [====>.........................] - ETA: 2s
1728/8086 [=====>........................] - ETA: 2s
1840/8086 [=====>........................] - ETA: 2s
1952/8086 [======>.......................] - ETA: 2s
2064/8086 [======>.......................] - ETA: 2s
2176/8086 [=======>......................] - ETA: 2s
2304/8086 [=======>......................] - ETA: 2s
2416/8086 [=======>......................] - ETA: 2s
2528/8086 [========>.....................] - ETA: 2s
2640/8086 [========>.....................] - ETA: 2s
2752/8086 [=========>....................] - ETA: 2s
2880/8086 [=========>....................] - ETA: 2s
2992/8086 [==========>...................] - ETA: 2s
3104/8086 [==========>...................] - ETA: 2s
3216/8086 [==========>...................] - ETA: 2s
3344/8086 [===========>..................] - ETA: 2s
3472/8086 [===========>..................] - ETA: 2s
3600/8086 [============>.................] - ETA: 1s
3728/8086 [============>.................] - ETA: 1s
3856/8086 [=============>................] - ETA: 1s
3984/8086 [=============>................] - ETA: 1s
4112/8086 [==============>...............] - ETA: 1s
4240/8086 [==============>...............] - ETA: 1s
4368/8086 [===============>..............] - ETA: 1s
4496/8086 [===============>..............] - ETA: 1s
4624/8086 [================>.............] - ETA: 1s
4752/8086 [================>.............] - ETA: 1s
4880/8086 [=================>............] - ETA: 1s
5008/8086 [=================>............] - ETA: 1s
5136/8086 [==================>...........] - ETA: 1s
5264/8086 [==================>...........] - ETA: 1s
5392/8086 [===================>..........] - ETA: 1s
5520/8086 [===================>..........] - ETA: 1s
5648/8086 [===================>..........] - ETA: 1s
5776/8086 [====================>.........] - ETA: 1s
5904/8086 [====================>.........] - ETA: 0s
6032/8086 [=====================>........] - ETA: 0s
6160/8086 [=====================>........] - ETA: 0s
6288/8086 [======================>.......] - ETA: 0s
6416/8086 [======================>.......] - ETA: 0s
6544/8086 [=======================>......] - ETA: 0s
6656/8086 [=======================>......] - ETA: 0s
6784/8086 [========================>.....] - ETA: 0s
6912/8086 [========================>.....] - ETA: 0s
7040/8086 [=========================>....] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7296/8086 [==========================>...] - ETA: 0s
7424/8086 [==========================>...] - ETA: 0s
7552/8086 [===========================>..] - ETA: 0s
7680/8086 [===========================>..] - ETA: 0s
7808/8086 [===========================>..] - ETA: 0s
7936/8086 [============================>.] - ETA: 0s
8064/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 3s 432us/step

test score: [0.6839151510486442, 0.7114766262676231]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=256, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 55s - loss: 0.8931 - acc: 0.5981 - val_loss: 0.8220 - val_acc: 0.6651
Epoch 2/10
 - 54s - loss: 0.7753 - acc: 0.6795 - val_loss: 0.7462 - val_acc: 0.6967
Epoch 3/10
 - 54s - loss: 0.7076 - acc: 0.7113 - val_loss: 0.7168 - val_acc: 0.7149
Epoch 4/10
 - 54s - loss: 0.6641 - acc: 0.7280 - val_loss: 0.7100 - val_acc: 0.7185
Epoch 5/10
 - 53s - loss: 0.6404 - acc: 0.7404 - val_loss: 0.6568 - val_acc: 0.7333
Epoch 6/10
 - 53s - loss: 0.6209 - acc: 0.7469 - val_loss: 0.6662 - val_acc: 0.7271
Epoch 7/10
 - 53s - loss: 0.6063 - acc: 0.7566 - val_loss: 0.6453 - val_acc: 0.7393
Epoch 8/10
 - 52s - loss: 0.5947 - acc: 0.7582 - val_loss: 0.6553 - val_acc: 0.7338
Epoch 9/10
 - 52s - loss: 0.5801 - acc: 0.7653 - val_loss: 0.6512 - val_acc: 0.7330
Epoch 10/10
 - 53s - loss: 0.5623 - acc: 0.7735 - val_loss: 0.6502 - val_acc: 0.7341

  16/8086 [..............................] - ETA: 3s
 144/8086 [..............................] - ETA: 3s
 272/8086 [>.............................] - ETA: 3s
 400/8086 [>.............................] - ETA: 3s
 528/8086 [>.............................] - ETA: 3s
 656/8086 [=>............................] - ETA: 3s
 784/8086 [=>............................] - ETA: 3s
 928/8086 [==>...........................] - ETA: 2s
1056/8086 [==>...........................] - ETA: 2s
1184/8086 [===>..........................] - ETA: 2s
1312/8086 [===>..........................] - ETA: 2s
1440/8086 [====>.........................] - ETA: 2s
1568/8086 [====>.........................] - ETA: 2s
1696/8086 [=====>........................] - ETA: 2s
1824/8086 [=====>........................] - ETA: 2s
1952/8086 [======>.......................] - ETA: 2s
2080/8086 [======>.......................] - ETA: 2s
2208/8086 [=======>......................] - ETA: 2s
2336/8086 [=======>......................] - ETA: 2s
2464/8086 [========>.....................] - ETA: 2s
2592/8086 [========>.....................] - ETA: 2s
2720/8086 [=========>....................] - ETA: 2s
2848/8086 [=========>....................] - ETA: 2s
2976/8086 [==========>...................] - ETA: 2s
3104/8086 [==========>...................] - ETA: 2s
3232/8086 [==========>...................] - ETA: 2s
3360/8086 [===========>..................] - ETA: 1s
3488/8086 [===========>..................] - ETA: 1s
3616/8086 [============>.................] - ETA: 1s
3744/8086 [============>.................] - ETA: 1s
3872/8086 [=============>................] - ETA: 1s
4000/8086 [=============>................] - ETA: 1s
4128/8086 [==============>...............] - ETA: 1s
4256/8086 [==============>...............] - ETA: 1s
4384/8086 [===============>..............] - ETA: 1s
4496/8086 [===============>..............] - ETA: 1s
4624/8086 [================>.............] - ETA: 1s
4752/8086 [================>.............] - ETA: 1s
4880/8086 [=================>............] - ETA: 1s
5008/8086 [=================>............] - ETA: 1s
5136/8086 [==================>...........] - ETA: 1s
5264/8086 [==================>...........] - ETA: 1s
5392/8086 [===================>..........] - ETA: 1s
5520/8086 [===================>..........] - ETA: 1s
5648/8086 [===================>..........] - ETA: 1s
5776/8086 [====================>.........] - ETA: 0s
5904/8086 [====================>.........] - ETA: 0s
6032/8086 [=====================>........] - ETA: 0s
6160/8086 [=====================>........] - ETA: 0s
6288/8086 [======================>.......] - ETA: 0s
6416/8086 [======================>.......] - ETA: 0s
6544/8086 [=======================>......] - ETA: 0s
6672/8086 [=======================>......] - ETA: 0s
6800/8086 [========================>.....] - ETA: 0s
6928/8086 [========================>.....] - ETA: 0s
7056/8086 [=========================>....] - ETA: 0s
7184/8086 [=========================>....] - ETA: 0s
7312/8086 [==========================>...] - ETA: 0s
7440/8086 [==========================>...] - ETA: 0s
7568/8086 [===========================>..] - ETA: 0s
7696/8086 [===========================>..] - ETA: 0s
7824/8086 [============================>.] - ETA: 0s
7952/8086 [============================>.] - ETA: 0s
8080/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 3s 417us/step

test score: [0.6778952305988601, 0.7135790254761316]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=256, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 55s - loss: 0.9095 - acc: 0.5857 - val_loss: 0.8506 - val_acc: 0.6331
Epoch 2/10
 - 54s - loss: 0.7744 - acc: 0.6755 - val_loss: 0.7441 - val_acc: 0.6925
Epoch 3/10
 - 54s - loss: 0.7037 - acc: 0.7095 - val_loss: 0.7165 - val_acc: 0.7004
Epoch 4/10
 - 59s - loss: 0.6646 - acc: 0.7287 - val_loss: 0.6862 - val_acc: 0.7263
Epoch 5/10
 - 60s - loss: 0.6414 - acc: 0.7399 - val_loss: 0.6678 - val_acc: 0.7331
Epoch 6/10
 - 61s - loss: 0.6244 - acc: 0.7468 - val_loss: 0.6616 - val_acc: 0.7316
Epoch 7/10
 - 59s - loss: 0.6093 - acc: 0.7512 - val_loss: 0.6464 - val_acc: 0.7341
Epoch 8/10
 - 58s - loss: 0.5983 - acc: 0.7582 - val_loss: 0.6391 - val_acc: 0.7409
Epoch 9/10
 - 58s - loss: 0.5831 - acc: 0.7619 - val_loss: 0.6397 - val_acc: 0.7423
Epoch 10/10
 - 58s - loss: 0.5681 - acc: 0.7674 - val_loss: 0.6441 - val_acc: 0.7382

  16/8086 [..............................] - ETA: 3s
 128/8086 [..............................] - ETA: 3s
 240/8086 [..............................] - ETA: 3s
 352/8086 [>.............................] - ETA: 3s
 464/8086 [>.............................] - ETA: 3s
 576/8086 [=>............................] - ETA: 3s
 688/8086 [=>............................] - ETA: 3s
 800/8086 [=>............................] - ETA: 3s
 912/8086 [==>...........................] - ETA: 3s
1024/8086 [==>...........................] - ETA: 3s
1136/8086 [===>..........................] - ETA: 3s
1248/8086 [===>..........................] - ETA: 3s
1376/8086 [====>.........................] - ETA: 3s
1488/8086 [====>.........................] - ETA: 3s
1600/8086 [====>.........................] - ETA: 2s
1712/8086 [=====>........................] - ETA: 2s
1824/8086 [=====>........................] - ETA: 2s
1936/8086 [======>.......................] - ETA: 2s
2048/8086 [======>.......................] - ETA: 2s
2160/8086 [=======>......................] - ETA: 2s
2272/8086 [=======>......................] - ETA: 2s
2384/8086 [=======>......................] - ETA: 2s
2496/8086 [========>.....................] - ETA: 2s
2624/8086 [========>.....................] - ETA: 2s
2736/8086 [=========>....................] - ETA: 2s
2848/8086 [=========>....................] - ETA: 2s
2960/8086 [=========>....................] - ETA: 2s
3072/8086 [==========>...................] - ETA: 2s
3184/8086 [==========>...................] - ETA: 2s
3296/8086 [===========>..................] - ETA: 2s
3408/8086 [===========>..................] - ETA: 2s
3520/8086 [============>.................] - ETA: 2s
3632/8086 [============>.................] - ETA: 2s
3744/8086 [============>.................] - ETA: 1s
3856/8086 [=============>................] - ETA: 1s
3968/8086 [=============>................] - ETA: 1s
4080/8086 [==============>...............] - ETA: 1s
4192/8086 [==============>...............] - ETA: 1s
4304/8086 [==============>...............] - ETA: 1s
4416/8086 [===============>..............] - ETA: 1s
4528/8086 [===============>..............] - ETA: 1s
4640/8086 [================>.............] - ETA: 1s
4768/8086 [================>.............] - ETA: 1s
4880/8086 [=================>............] - ETA: 1s
4992/8086 [=================>............] - ETA: 1s
5104/8086 [=================>............] - ETA: 1s
5216/8086 [==================>...........] - ETA: 1s
5328/8086 [==================>...........] - ETA: 1s
5440/8086 [===================>..........] - ETA: 1s
5552/8086 [===================>..........] - ETA: 1s
5664/8086 [====================>.........] - ETA: 1s
5776/8086 [====================>.........] - ETA: 1s
5888/8086 [====================>.........] - ETA: 1s
6000/8086 [=====================>........] - ETA: 0s
6112/8086 [=====================>........] - ETA: 0s
6224/8086 [======================>.......] - ETA: 0s
6336/8086 [======================>.......] - ETA: 0s
6448/8086 [======================>.......] - ETA: 0s
6560/8086 [=======================>......] - ETA: 0s
6672/8086 [=======================>......] - ETA: 0s
6784/8086 [========================>.....] - ETA: 0s
6896/8086 [========================>.....] - ETA: 0s
7024/8086 [=========================>....] - ETA: 0s
7136/8086 [=========================>....] - ETA: 0s
7248/8086 [=========================>....] - ETA: 0s
7360/8086 [==========================>...] - ETA: 0s
7472/8086 [==========================>...] - ETA: 0s
7584/8086 [===========================>..] - ETA: 0s
7696/8086 [===========================>..] - ETA: 0s
7808/8086 [===========================>..] - ETA: 0s
7920/8086 [============================>.] - ETA: 0s
8032/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 4s 458us/step

test score: [0.678681237698191, 0.719515211491369]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=256, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 68s - loss: 0.9041 - acc: 0.5887 - val_loss: 0.8384 - val_acc: 0.6430
Epoch 2/10
 - 61s - loss: 0.7928 - acc: 0.6686 - val_loss: 0.7580 - val_acc: 0.6948
Epoch 3/10
 - 62s - loss: 0.7202 - acc: 0.7050 - val_loss: 0.7139 - val_acc: 0.7025
Epoch 4/10
 - 62s - loss: 0.6753 - acc: 0.7224 - val_loss: 0.6792 - val_acc: 0.7219
Epoch 5/10
 - 62s - loss: 0.6482 - acc: 0.7343 - val_loss: 0.6877 - val_acc: 0.7158
Epoch 6/10
 - 63s - loss: 0.6327 - acc: 0.7437 - val_loss: 0.6600 - val_acc: 0.7345
Epoch 7/10
 - 62s - loss: 0.6169 - acc: 0.7519 - val_loss: 0.6495 - val_acc: 0.7378
Epoch 8/10
 - 62s - loss: 0.6057 - acc: 0.7551 - val_loss: 0.6648 - val_acc: 0.7387
Epoch 9/10
 - 62s - loss: 0.5903 - acc: 0.7619 - val_loss: 0.6513 - val_acc: 0.7403
Epoch 10/10
 - 65s - loss: 0.5782 - acc: 0.7682 - val_loss: 0.6395 - val_acc: 0.7409

  16/8086 [..............................] - ETA: 5s
 128/8086 [..............................] - ETA: 4s
 240/8086 [..............................] - ETA: 3s
 352/8086 [>.............................] - ETA: 3s
 464/8086 [>.............................] - ETA: 3s
 576/8086 [=>............................] - ETA: 3s
 688/8086 [=>............................] - ETA: 3s
 800/8086 [=>............................] - ETA: 3s
 912/8086 [==>...........................] - ETA: 3s
1008/8086 [==>...........................] - ETA: 3s
1120/8086 [===>..........................] - ETA: 3s
1216/8086 [===>..........................] - ETA: 3s
1312/8086 [===>..........................] - ETA: 3s
1408/8086 [====>.........................] - ETA: 3s
1504/8086 [====>.........................] - ETA: 3s
1600/8086 [====>.........................] - ETA: 3s
1696/8086 [=====>........................] - ETA: 3s
1808/8086 [=====>........................] - ETA: 3s
1920/8086 [======>.......................] - ETA: 3s
2032/8086 [======>.......................] - ETA: 3s
2144/8086 [======>.......................] - ETA: 3s
2256/8086 [=======>......................] - ETA: 2s
2368/8086 [=======>......................] - ETA: 2s
2480/8086 [========>.....................] - ETA: 2s
2592/8086 [========>.....................] - ETA: 2s
2704/8086 [=========>....................] - ETA: 2s
2816/8086 [=========>....................] - ETA: 2s
2928/8086 [=========>....................] - ETA: 2s
3040/8086 [==========>...................] - ETA: 2s
3152/8086 [==========>...................] - ETA: 2s
3248/8086 [===========>..................] - ETA: 2s
3344/8086 [===========>..................] - ETA: 2s
3440/8086 [===========>..................] - ETA: 2s
3552/8086 [============>.................] - ETA: 2s
3664/8086 [============>.................] - ETA: 2s
3776/8086 [=============>................] - ETA: 2s
3888/8086 [=============>................] - ETA: 2s
3984/8086 [=============>................] - ETA: 2s
4096/8086 [==============>...............] - ETA: 2s
4208/8086 [==============>...............] - ETA: 1s
4320/8086 [===============>..............] - ETA: 1s
4432/8086 [===============>..............] - ETA: 1s
4544/8086 [===============>..............] - ETA: 1s
4656/8086 [================>.............] - ETA: 1s
4768/8086 [================>.............] - ETA: 1s
4880/8086 [=================>............] - ETA: 1s
4992/8086 [=================>............] - ETA: 1s
5104/8086 [=================>............] - ETA: 1s
5216/8086 [==================>...........] - ETA: 1s
5328/8086 [==================>...........] - ETA: 1s
5440/8086 [===================>..........] - ETA: 1s
5552/8086 [===================>..........] - ETA: 1s
5664/8086 [====================>.........] - ETA: 1s
5776/8086 [====================>.........] - ETA: 1s
5888/8086 [====================>.........] - ETA: 1s
6000/8086 [=====================>........] - ETA: 1s
6112/8086 [=====================>........] - ETA: 0s
6224/8086 [======================>.......] - ETA: 0s
6336/8086 [======================>.......] - ETA: 0s
6448/8086 [======================>.......] - ETA: 0s
6560/8086 [=======================>......] - ETA: 0s
6672/8086 [=======================>......] - ETA: 0s
6784/8086 [========================>.....] - ETA: 0s
6896/8086 [========================>.....] - ETA: 0s
7008/8086 [=========================>....] - ETA: 0s
7120/8086 [=========================>....] - ETA: 0s
7216/8086 [=========================>....] - ETA: 0s
7328/8086 [==========================>...] - ETA: 0s
7424/8086 [==========================>...] - ETA: 0s
7536/8086 [==========================>...] - ETA: 0s
7648/8086 [===========================>..] - ETA: 0s
7760/8086 [===========================>..] - ETA: 0s
7872/8086 [============================>.] - ETA: 0s
7968/8086 [============================>.] - ETA: 0s
8080/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 4s 505us/step

test score: [0.6728579876131272, 0.7229779866583241]


Training new model, loss:categorical_crossentropy, optimizer=adam, lstm_len=256, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 39s - loss: 0.9233 - acc: 0.5787 - val_loss: 0.8511 - val_acc: 0.6336
Epoch 2/10
 - 40s - loss: 0.8134 - acc: 0.6566 - val_loss: 0.7873 - val_acc: 0.6827
Epoch 3/10
 - 39s - loss: 0.7316 - acc: 0.7013 - val_loss: 0.7266 - val_acc: 0.7118
Epoch 4/10
 - 31s - loss: 0.6848 - acc: 0.7209 - val_loss: 0.6940 - val_acc: 0.7180
Epoch 5/10
 - 30s - loss: 0.6515 - acc: 0.7352 - val_loss: 0.6645 - val_acc: 0.7301
Epoch 6/10
 - 30s - loss: 0.6353 - acc: 0.7411 - val_loss: 0.6597 - val_acc: 0.7301
Epoch 7/10
 - 30s - loss: 0.6194 - acc: 0.7476 - val_loss: 0.6580 - val_acc: 0.7319
Epoch 8/10
 - 29s - loss: 0.6071 - acc: 0.7526 - val_loss: 0.6543 - val_acc: 0.7378
Epoch 9/10
 - 30s - loss: 0.5936 - acc: 0.7611 - val_loss: 0.6534 - val_acc: 0.7324
Epoch 10/10
 - 29s - loss: 0.5780 - acc: 0.7653 - val_loss: 0.6667 - val_acc: 0.7313

  16/8086 [..............................] - ETA: 1s
 304/8086 [>.............................] - ETA: 1s
 592/8086 [=>............................] - ETA: 1s
 896/8086 [==>...........................] - ETA: 1s
1184/8086 [===>..........................] - ETA: 1s
1472/8086 [====>.........................] - ETA: 1s
1776/8086 [=====>........................] - ETA: 1s
2064/8086 [======>.......................] - ETA: 1s
2352/8086 [=======>......................] - ETA: 0s
2640/8086 [========>.....................] - ETA: 0s
2928/8086 [=========>....................] - ETA: 0s
3232/8086 [==========>...................] - ETA: 0s
3520/8086 [============>.................] - ETA: 0s
3808/8086 [=============>................] - ETA: 0s
4096/8086 [==============>...............] - ETA: 0s
4400/8086 [===============>..............] - ETA: 0s
4688/8086 [================>.............] - ETA: 0s
4976/8086 [=================>............] - ETA: 0s
5264/8086 [==================>...........] - ETA: 0s
5568/8086 [===================>..........] - ETA: 0s
5856/8086 [====================>.........] - ETA: 0s
6144/8086 [=====================>........] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
6736/8086 [=======================>......] - ETA: 0s
7040/8086 [=========================>....] - ETA: 0s
7328/8086 [==========================>...] - ETA: 0s
7616/8086 [===========================>..] - ETA: 0s
7904/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 174us/step

test score: [0.7005892231002645, 0.7123423200741046]

Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=32, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 12s - loss: 0.8567 - acc: 0.6255 - val_loss: 0.7791 - val_acc: 0.6807
Epoch 2/10
 - 9s - loss: 0.7417 - acc: 0.6966 - val_loss: 0.7392 - val_acc: 0.6923
Epoch 3/10
 - 10s - loss: 0.6970 - acc: 0.7118 - val_loss: 0.7112 - val_acc: 0.7050
Epoch 4/10
 - 9s - loss: 0.6658 - acc: 0.7249 - val_loss: 0.6874 - val_acc: 0.7262
Epoch 5/10
 - 10s - loss: 0.6465 - acc: 0.7359 - val_loss: 0.6595 - val_acc: 0.7297
Epoch 6/10
 - 10s - loss: 0.6311 - acc: 0.7434 - val_loss: 0.6668 - val_acc: 0.7370
Epoch 7/10
 - 10s - loss: 0.6173 - acc: 0.7497 - val_loss: 0.6463 - val_acc: 0.7406
Epoch 8/10
 - 12s - loss: 0.6070 - acc: 0.7536 - val_loss: 0.6547 - val_acc: 0.7338
Epoch 9/10
 - 10s - loss: 0.6011 - acc: 0.7571 - val_loss: 0.6381 - val_acc: 0.7406
Epoch 10/10
 - 11s - loss: 0.5908 - acc: 0.7619 - val_loss: 0.6371 - val_acc: 0.7421

  16/8086 [..............................] - ETA: 0s
 624/8086 [=>............................] - ETA: 0s
1200/8086 [===>..........................] - ETA: 0s
1824/8086 [=====>........................] - ETA: 0s
2432/8086 [========>.....................] - ETA: 0s
2992/8086 [==========>...................] - ETA: 0s
3584/8086 [============>.................] - ETA: 0s
4160/8086 [==============>...............] - ETA: 0s
4784/8086 [================>.............] - ETA: 0s
5392/8086 [===================>..........] - ETA: 0s
6016/8086 [=====================>........] - ETA: 0s
6704/8086 [=======================>......] - ETA: 0s
7408/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 82us/step

test score: [0.6570086512358035, 0.7261934207419255]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=32, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 11s - loss: 0.8529 - acc: 0.6257 - val_loss: 0.7687 - val_acc: 0.6848
Epoch 2/10
 - 32s - loss: 0.7338 - acc: 0.6991 - val_loss: 0.7268 - val_acc: 0.6956
Epoch 3/10
 - 29s - loss: 0.6888 - acc: 0.7216 - val_loss: 0.6915 - val_acc: 0.7135
Epoch 4/10
 - 20s - loss: 0.6660 - acc: 0.7305 - val_loss: 0.6982 - val_acc: 0.7189
Epoch 5/10
 - 18s - loss: 0.6518 - acc: 0.7386 - val_loss: 0.6669 - val_acc: 0.7280
Epoch 6/10
 - 28s - loss: 0.6363 - acc: 0.7429 - val_loss: 0.6633 - val_acc: 0.7314
Epoch 7/10
 - 29s - loss: 0.6255 - acc: 0.7471 - val_loss: 0.6628 - val_acc: 0.7318
Epoch 8/10
 - 30s - loss: 0.6152 - acc: 0.7506 - val_loss: 0.6442 - val_acc: 0.7441
Epoch 9/10
 - 28s - loss: 0.6058 - acc: 0.7539 - val_loss: 0.6426 - val_acc: 0.7401
Epoch 10/10
 - 30s - loss: 0.5960 - acc: 0.7581 - val_loss: 0.6449 - val_acc: 0.7393

  16/8086 [..............................] - ETA: 2s
 272/8086 [>.............................] - ETA: 1s
 576/8086 [=>............................] - ETA: 1s
 656/8086 [=>............................] - ETA: 1s
 848/8086 [==>...........................] - ETA: 1s
1152/8086 [===>..........................] - ETA: 1s
1440/8086 [====>.........................] - ETA: 1s
1760/8086 [=====>........................] - ETA: 1s
2048/8086 [======>.......................] - ETA: 1s
2224/8086 [=======>......................] - ETA: 1s
2400/8086 [=======>......................] - ETA: 1s
2656/8086 [========>.....................] - ETA: 1s
2912/8086 [=========>....................] - ETA: 1s
3232/8086 [==========>...................] - ETA: 1s
3488/8086 [===========>..................] - ETA: 0s
3744/8086 [============>.................] - ETA: 0s
4032/8086 [=============>................] - ETA: 0s
4320/8086 [===============>..............] - ETA: 0s
4496/8086 [===============>..............] - ETA: 0s
4752/8086 [================>.............] - ETA: 0s
5008/8086 [=================>............] - ETA: 0s
5264/8086 [==================>...........] - ETA: 0s
5584/8086 [===================>..........] - ETA: 0s
5792/8086 [====================>.........] - ETA: 0s
6016/8086 [=====================>........] - ETA: 0s
6240/8086 [======================>.......] - ETA: 0s
6464/8086 [======================>.......] - ETA: 0s
6768/8086 [========================>.....] - ETA: 0s
6976/8086 [========================>.....] - ETA: 0s
7024/8086 [=========================>....] - ETA: 0s
7296/8086 [==========================>...] - ETA: 0s
7648/8086 [===========================>..] - ETA: 0s
7936/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 215us/step

test score: [0.6772086373980671, 0.7201335641850112]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=32, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 40s - loss: 0.8793 - acc: 0.6098 - val_loss: 0.8202 - val_acc: 0.6606
Epoch 2/10
 - 31s - loss: 0.7802 - acc: 0.6799 - val_loss: 0.7515 - val_acc: 0.7028
Epoch 3/10
 - 28s - loss: 0.7172 - acc: 0.7083 - val_loss: 0.7379 - val_acc: 0.6984
Epoch 4/10
 - 32s - loss: 0.6792 - acc: 0.7234 - val_loss: 0.7041 - val_acc: 0.7233
Epoch 5/10
 - 30s - loss: 0.6626 - acc: 0.7324 - val_loss: 0.6828 - val_acc: 0.7277
Epoch 6/10
 - 28s - loss: 0.6510 - acc: 0.7370 - val_loss: 0.6980 - val_acc: 0.7208
Epoch 7/10
 - 28s - loss: 0.6381 - acc: 0.7417 - val_loss: 0.6707 - val_acc: 0.7288
Epoch 8/10
 - 26s - loss: 0.6286 - acc: 0.7469 - val_loss: 0.6675 - val_acc: 0.7291
Epoch 9/10
 - 27s - loss: 0.6158 - acc: 0.7573 - val_loss: 0.6599 - val_acc: 0.7335
Epoch 10/10
 - 27s - loss: 0.6095 - acc: 0.7572 - val_loss: 0.6559 - val_acc: 0.7367

  16/8086 [..............................] - ETA: 0s
 384/8086 [>.............................] - ETA: 1s
 656/8086 [=>............................] - ETA: 1s
 976/8086 [==>...........................] - ETA: 1s
1232/8086 [===>..........................] - ETA: 1s
1488/8086 [====>.........................] - ETA: 1s
1808/8086 [=====>........................] - ETA: 1s
2112/8086 [======>.......................] - ETA: 1s
2432/8086 [========>.....................] - ETA: 0s
2784/8086 [=========>....................] - ETA: 0s
3136/8086 [==========>...................] - ETA: 0s
3440/8086 [===========>..................] - ETA: 0s
3808/8086 [=============>................] - ETA: 0s
4160/8086 [==============>...............] - ETA: 0s
4432/8086 [===============>..............] - ETA: 0s
4800/8086 [================>.............] - ETA: 0s
5120/8086 [=================>............] - ETA: 0s
5472/8086 [===================>..........] - ETA: 0s
5728/8086 [====================>.........] - ETA: 0s
5984/8086 [=====================>........] - ETA: 0s
6320/8086 [======================>.......] - ETA: 0s
6608/8086 [=======================>......] - ETA: 0s
6912/8086 [========================>.....] - ETA: 0s
7264/8086 [=========================>....] - ETA: 0s
7488/8086 [==========================>...] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 165us/step

test score: [0.6730244829458321, 0.7160524363170424]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=32, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 30s - loss: 0.8724 - acc: 0.6122 - val_loss: 0.7822 - val_acc: 0.6704
Epoch 2/10
 - 26s - loss: 0.7587 - acc: 0.6853 - val_loss: 0.7418 - val_acc: 0.6987
Epoch 3/10
 - 28s - loss: 0.7129 - acc: 0.7089 - val_loss: 0.7228 - val_acc: 0.7049
Epoch 4/10
 - 31s - loss: 0.6800 - acc: 0.7246 - val_loss: 0.6874 - val_acc: 0.7191
Epoch 5/10
 - 28s - loss: 0.6585 - acc: 0.7343 - val_loss: 0.6739 - val_acc: 0.7243
Epoch 6/10
 - 30s - loss: 0.6429 - acc: 0.7394 - val_loss: 0.6555 - val_acc: 0.7356
Epoch 7/10
 - 29s - loss: 0.6331 - acc: 0.7461 - val_loss: 0.6593 - val_acc: 0.7376
Epoch 8/10
 - 28s - loss: 0.6159 - acc: 0.7531 - val_loss: 0.6605 - val_acc: 0.7395
Epoch 9/10
 - 30s - loss: 0.6105 - acc: 0.7577 - val_loss: 0.6426 - val_acc: 0.7369
Epoch 10/10
 - 28s - loss: 0.6027 - acc: 0.7587 - val_loss: 0.6410 - val_acc: 0.7406

  16/8086 [..............................] - ETA: 2s
 384/8086 [>.............................] - ETA: 1s
 752/8086 [=>............................] - ETA: 1s
1104/8086 [===>..........................] - ETA: 1s
1472/8086 [====>.........................] - ETA: 0s
1840/8086 [=====>........................] - ETA: 0s
2160/8086 [=======>......................] - ETA: 0s
2352/8086 [=======>......................] - ETA: 0s
2592/8086 [========>.....................] - ETA: 0s
2864/8086 [=========>....................] - ETA: 0s
3072/8086 [==========>...................] - ETA: 0s
3360/8086 [===========>..................] - ETA: 0s
3632/8086 [============>.................] - ETA: 0s
3888/8086 [=============>................] - ETA: 0s
4128/8086 [==============>...............] - ETA: 0s
4336/8086 [===============>..............] - ETA: 0s
4544/8086 [===============>..............] - ETA: 0s
4688/8086 [================>.............] - ETA: 0s
4944/8086 [=================>............] - ETA: 0s
5168/8086 [==================>...........] - ETA: 0s
5408/8086 [===================>..........] - ETA: 0s
5648/8086 [===================>..........] - ETA: 0s
5888/8086 [====================>.........] - ETA: 0s
6160/8086 [=====================>........] - ETA: 0s
6448/8086 [======================>.......] - ETA: 0s
6768/8086 [========================>.....] - ETA: 0s
7040/8086 [=========================>....] - ETA: 0s
7360/8086 [==========================>...] - ETA: 0s
7696/8086 [===========================>..] - ETA: 0s
8000/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 190us/step

test score: [0.6651047178934094, 0.7156814246846401]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=32, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 29s - loss: 0.8684 - acc: 0.6160 - val_loss: 0.7772 - val_acc: 0.6843
Epoch 2/10
 - 27s - loss: 0.7538 - acc: 0.6918 - val_loss: 0.7374 - val_acc: 0.6953
Epoch 3/10
 - 29s - loss: 0.7121 - acc: 0.7160 - val_loss: 0.7038 - val_acc: 0.7081
Epoch 4/10
 - 29s - loss: 0.6801 - acc: 0.7302 - val_loss: 0.6921 - val_acc: 0.7165
Epoch 5/10
 - 30s - loss: 0.6647 - acc: 0.7345 - val_loss: 0.6825 - val_acc: 0.7168
Epoch 6/10
 - 30s - loss: 0.6518 - acc: 0.7389 - val_loss: 0.6779 - val_acc: 0.7270
Epoch 7/10
 - 28s - loss: 0.6375 - acc: 0.7418 - val_loss: 0.6571 - val_acc: 0.7325
Epoch 8/10
 - 28s - loss: 0.6287 - acc: 0.7503 - val_loss: 0.6518 - val_acc: 0.7361
Epoch 9/10
 - 28s - loss: 0.6170 - acc: 0.7542 - val_loss: 0.6467 - val_acc: 0.7389
Epoch 10/10
 - 25s - loss: 0.6103 - acc: 0.7553 - val_loss: 0.6505 - val_acc: 0.7406

  16/8086 [..............................] - ETA: 3s
 208/8086 [..............................] - ETA: 2s
 432/8086 [>.............................] - ETA: 1s
 544/8086 [=>............................] - ETA: 2s
 784/8086 [=>............................] - ETA: 2s
1008/8086 [==>...........................] - ETA: 1s
1232/8086 [===>..........................] - ETA: 1s
1536/8086 [====>.........................] - ETA: 1s
1904/8086 [======>.......................] - ETA: 1s
2176/8086 [=======>......................] - ETA: 1s
2400/8086 [=======>......................] - ETA: 1s
2608/8086 [========>.....................] - ETA: 1s
2848/8086 [=========>....................] - ETA: 1s
3104/8086 [==========>...................] - ETA: 1s
3376/8086 [===========>..................] - ETA: 1s
3744/8086 [============>.................] - ETA: 0s
4016/8086 [=============>................] - ETA: 0s
4304/8086 [==============>...............] - ETA: 0s
4608/8086 [================>.............] - ETA: 0s
4928/8086 [=================>............] - ETA: 0s
5232/8086 [==================>...........] - ETA: 0s
5472/8086 [===================>..........] - ETA: 0s
5664/8086 [====================>.........] - ETA: 0s
5888/8086 [====================>.........] - ETA: 0s
6128/8086 [=====================>........] - ETA: 0s
6320/8086 [======================>.......] - ETA: 0s
6576/8086 [=======================>......] - ETA: 0s
6848/8086 [========================>.....] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7392/8086 [==========================>...] - ETA: 0s
7680/8086 [===========================>..] - ETA: 0s
8000/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 203us/step

test score: [0.66918435097183, 0.7174128122681177]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=64, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 35s - loss: 0.8539 - acc: 0.6299 - val_loss: 0.7994 - val_acc: 0.6557
Epoch 2/10
 - 29s - loss: 0.7310 - acc: 0.7053 - val_loss: 0.7245 - val_acc: 0.7093
Epoch 3/10
 - 32s - loss: 0.6750 - acc: 0.7259 - val_loss: 0.6918 - val_acc: 0.7180
Epoch 4/10
 - 30s - loss: 0.6502 - acc: 0.7349 - val_loss: 0.6739 - val_acc: 0.7223
Epoch 5/10
 - 31s - loss: 0.6292 - acc: 0.7419 - val_loss: 0.6565 - val_acc: 0.7327
Epoch 6/10
 - 33s - loss: 0.6156 - acc: 0.7499 - val_loss: 0.6525 - val_acc: 0.7378
Epoch 7/10
 - 31s - loss: 0.6055 - acc: 0.7539 - val_loss: 0.6486 - val_acc: 0.7373
Epoch 8/10
 - 32s - loss: 0.5930 - acc: 0.7570 - val_loss: 0.6364 - val_acc: 0.7427
Epoch 9/10
 - 32s - loss: 0.5824 - acc: 0.7622 - val_loss: 0.6413 - val_acc: 0.7407
Epoch 10/10
 - 29s - loss: 0.5726 - acc: 0.7681 - val_loss: 0.6271 - val_acc: 0.7475

  16/8086 [..............................] - ETA: 2s
 224/8086 [..............................] - ETA: 1s
 480/8086 [>.............................] - ETA: 1s
 768/8086 [=>............................] - ETA: 1s
 928/8086 [==>...........................] - ETA: 1s
1104/8086 [===>..........................] - ETA: 1s
1216/8086 [===>..........................] - ETA: 1s
1456/8086 [====>.........................] - ETA: 1s
1744/8086 [=====>........................] - ETA: 1s
2000/8086 [======>.......................] - ETA: 1s
2272/8086 [=======>......................] - ETA: 1s
2560/8086 [========>.....................] - ETA: 1s
2832/8086 [=========>....................] - ETA: 1s
3072/8086 [==========>...................] - ETA: 1s
3376/8086 [===========>..................] - ETA: 1s
3648/8086 [============>.................] - ETA: 0s
3936/8086 [=============>................] - ETA: 0s
4208/8086 [==============>...............] - ETA: 0s
4480/8086 [===============>..............] - ETA: 0s
4688/8086 [================>.............] - ETA: 0s
4928/8086 [=================>............] - ETA: 0s
5168/8086 [==================>...........] - ETA: 0s
5472/8086 [===================>..........] - ETA: 0s
5712/8086 [====================>.........] - ETA: 0s
5968/8086 [=====================>........] - ETA: 0s
6288/8086 [======================>.......] - ETA: 0s
6576/8086 [=======================>......] - ETA: 0s
6864/8086 [========================>.....] - ETA: 0s
7120/8086 [=========================>....] - ETA: 0s
7424/8086 [==========================>...] - ETA: 0s
7744/8086 [===========================>..] - ETA: 0s
8016/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 200us/step

test score: [0.6784292565266625, 0.7207519169081387]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=64, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 34s - loss: 0.8651 - acc: 0.6173 - val_loss: 0.7814 - val_acc: 0.6804
Epoch 2/10
 - 32s - loss: 0.7316 - acc: 0.6963 - val_loss: 0.7179 - val_acc: 0.6945
Epoch 3/10
 - 30s - loss: 0.6800 - acc: 0.7231 - val_loss: 0.6880 - val_acc: 0.7199
Epoch 4/10
 - 32s - loss: 0.6467 - acc: 0.7378 - val_loss: 0.6621 - val_acc: 0.7370
Epoch 5/10
 - 30s - loss: 0.6256 - acc: 0.7429 - val_loss: 0.6814 - val_acc: 0.7222
Epoch 6/10
 - 32s - loss: 0.6116 - acc: 0.7507 - val_loss: 0.6610 - val_acc: 0.7348
Epoch 7/10
 - 30s - loss: 0.6018 - acc: 0.7575 - val_loss: 0.6454 - val_acc: 0.7386
Epoch 8/10
 - 32s - loss: 0.5883 - acc: 0.7623 - val_loss: 0.6473 - val_acc: 0.7304
Epoch 9/10
 - 29s - loss: 0.5774 - acc: 0.7676 - val_loss: 0.6563 - val_acc: 0.7369
Epoch 10/10
 - 31s - loss: 0.5642 - acc: 0.7749 - val_loss: 0.6457 - val_acc: 0.7392

  16/8086 [..............................] - ETA: 2s
 208/8086 [..............................] - ETA: 2s
 400/8086 [>.............................] - ETA: 2s
 640/8086 [=>............................] - ETA: 1s
 800/8086 [=>............................] - ETA: 2s
1024/8086 [==>...........................] - ETA: 1s
1232/8086 [===>..........................] - ETA: 1s
1360/8086 [====>.........................] - ETA: 1s
1568/8086 [====>.........................] - ETA: 1s
1760/8086 [=====>........................] - ETA: 1s
1936/8086 [======>.......................] - ETA: 1s
2256/8086 [=======>......................] - ETA: 1s
2576/8086 [========>.....................] - ETA: 1s
2816/8086 [=========>....................] - ETA: 1s
2960/8086 [=========>....................] - ETA: 1s
3088/8086 [==========>...................] - ETA: 1s
3264/8086 [===========>..................] - ETA: 1s
3488/8086 [===========>..................] - ETA: 1s
3680/8086 [============>.................] - ETA: 1s
3856/8086 [=============>................] - ETA: 1s
4096/8086 [==============>...............] - ETA: 1s
4272/8086 [==============>...............] - ETA: 0s
4496/8086 [===============>..............] - ETA: 0s
4688/8086 [================>.............] - ETA: 0s
4896/8086 [=================>............] - ETA: 0s
5088/8086 [=================>............] - ETA: 0s
5312/8086 [==================>...........] - ETA: 0s
5472/8086 [===================>..........] - ETA: 0s
5616/8086 [===================>..........] - ETA: 0s
5776/8086 [====================>.........] - ETA: 0s
6000/8086 [=====================>........] - ETA: 0s
6208/8086 [======================>.......] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
6560/8086 [=======================>......] - ETA: 0s
6768/8086 [========================>.....] - ETA: 0s
6912/8086 [========================>.....] - ETA: 0s
7040/8086 [=========================>....] - ETA: 0s
7328/8086 [==========================>...] - ETA: 0s
7488/8086 [==========================>...] - ETA: 0s
7744/8086 [===========================>..] - ETA: 0s
7984/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 270us/step

test score: [0.6776185533197215, 0.7180311649912453]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=64, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 35s - loss: 0.8637 - acc: 0.6201 - val_loss: 0.7845 - val_acc: 0.6776
Epoch 2/10
 - 33s - loss: 0.7333 - acc: 0.6971 - val_loss: 0.7209 - val_acc: 0.6979
Epoch 3/10
 - 33s - loss: 0.6861 - acc: 0.7174 - val_loss: 0.6854 - val_acc: 0.7186
Epoch 4/10
 - 30s - loss: 0.6564 - acc: 0.7288 - val_loss: 0.6632 - val_acc: 0.7276
Epoch 5/10
 - 30s - loss: 0.6339 - acc: 0.7408 - val_loss: 0.6721 - val_acc: 0.7288
Epoch 6/10
 - 32s - loss: 0.6180 - acc: 0.7513 - val_loss: 0.6663 - val_acc: 0.7284
Epoch 7/10
 - 31s - loss: 0.6051 - acc: 0.7519 - val_loss: 0.6442 - val_acc: 0.7365
Epoch 8/10
 - 32s - loss: 0.5929 - acc: 0.7580 - val_loss: 0.6383 - val_acc: 0.7392
Epoch 9/10
 - 31s - loss: 0.5798 - acc: 0.7653 - val_loss: 0.6386 - val_acc: 0.7421
Epoch 10/10
 - 29s - loss: 0.5708 - acc: 0.7672 - val_loss: 0.6343 - val_acc: 0.7409

  16/8086 [..............................] - ETA: 1s
 304/8086 [>.............................] - ETA: 1s
 512/8086 [>.............................] - ETA: 1s
 736/8086 [=>............................] - ETA: 1s
 928/8086 [==>...........................] - ETA: 1s
1072/8086 [==>...........................] - ETA: 1s
1264/8086 [===>..........................] - ETA: 1s
1488/8086 [====>.........................] - ETA: 1s
1696/8086 [=====>........................] - ETA: 1s
1824/8086 [=====>........................] - ETA: 1s
2096/8086 [======>.......................] - ETA: 1s
2352/8086 [=======>......................] - ETA: 1s
2640/8086 [========>.....................] - ETA: 1s
2928/8086 [=========>....................] - ETA: 1s
3152/8086 [==========>...................] - ETA: 1s
3440/8086 [===========>..................] - ETA: 1s
3584/8086 [============>.................] - ETA: 1s
3808/8086 [=============>................] - ETA: 0s
4000/8086 [=============>................] - ETA: 0s
4128/8086 [==============>...............] - ETA: 0s
4576/8086 [===============>..............] - ETA: 0s
4912/8086 [=================>............] - ETA: 0s
5184/8086 [==================>...........] - ETA: 0s
5344/8086 [==================>...........] - ETA: 0s
5552/8086 [===================>..........] - ETA: 0s
5776/8086 [====================>.........] - ETA: 0s
5968/8086 [=====================>........] - ETA: 0s
6160/8086 [=====================>........] - ETA: 0s
6336/8086 [======================>.......] - ETA: 0s
6544/8086 [=======================>......] - ETA: 0s
6704/8086 [=======================>......] - ETA: 0s
6800/8086 [========================>.....] - ETA: 0s
6976/8086 [========================>.....] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7376/8086 [==========================>...] - ETA: 0s
7600/8086 [===========================>..] - ETA: 0s
7792/8086 [===========================>..] - ETA: 0s
7984/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 247us/step

test score: [0.6644941939890104, 0.7198862231016572]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=64, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 39s - loss: 0.8577 - acc: 0.6228 - val_loss: 0.7659 - val_acc: 0.6809
Epoch 2/10
 - 35s - loss: 0.7236 - acc: 0.6977 - val_loss: 0.7309 - val_acc: 0.6945
Epoch 3/10
 - 36s - loss: 0.6811 - acc: 0.7193 - val_loss: 0.7031 - val_acc: 0.7070
Epoch 4/10
 - 32s - loss: 0.6572 - acc: 0.7311 - val_loss: 0.6923 - val_acc: 0.7185
Epoch 5/10
 - 33s - loss: 0.6398 - acc: 0.7392 - val_loss: 0.6601 - val_acc: 0.7322
Epoch 6/10
 - 27s - loss: 0.6252 - acc: 0.7462 - val_loss: 0.6478 - val_acc: 0.7358
Epoch 7/10
 - 11s - loss: 0.6098 - acc: 0.7528 - val_loss: 0.6398 - val_acc: 0.7427
Epoch 8/10
 - 12s - loss: 0.5989 - acc: 0.7568 - val_loss: 0.6481 - val_acc: 0.7381
Epoch 9/10
 - 12s - loss: 0.5885 - acc: 0.7650 - val_loss: 0.6479 - val_acc: 0.7426
Epoch 10/10
 - 11s - loss: 0.5758 - acc: 0.7699 - val_loss: 0.6483 - val_acc: 0.7389

  16/8086 [..............................] - ETA: 0s
 576/8086 [=>............................] - ETA: 0s
1152/8086 [===>..........................] - ETA: 0s
1760/8086 [=====>........................] - ETA: 0s
2384/8086 [=======>......................] - ETA: 0s
2928/8086 [=========>....................] - ETA: 0s
3552/8086 [============>.................] - ETA: 0s
4160/8086 [==============>...............] - ETA: 0s
4736/8086 [================>.............] - ETA: 0s
5312/8086 [==================>...........] - ETA: 0s
5904/8086 [====================>.........] - ETA: 0s
6528/8086 [=======================>......] - ETA: 0s
7104/8086 [=========================>....] - ETA: 0s
7728/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 86us/step

test score: [0.6711466617581869, 0.716547118476379]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=64, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 12s - loss: 0.8633 - acc: 0.6159 - val_loss: 0.7905 - val_acc: 0.6654
Epoch 2/10
 - 11s - loss: 0.7392 - acc: 0.6948 - val_loss: 0.7167 - val_acc: 0.7093
Epoch 3/10
 - 11s - loss: 0.6872 - acc: 0.7200 - val_loss: 0.6959 - val_acc: 0.7126
Epoch 4/10
 - 11s - loss: 0.6574 - acc: 0.7330 - val_loss: 0.7066 - val_acc: 0.7254
Epoch 5/10
 - 11s - loss: 0.6390 - acc: 0.7434 - val_loss: 0.6674 - val_acc: 0.7324
Epoch 6/10
 - 11s - loss: 0.6255 - acc: 0.7495 - val_loss: 0.6636 - val_acc: 0.7370
Epoch 7/10
 - 11s - loss: 0.6117 - acc: 0.7544 - val_loss: 0.6519 - val_acc: 0.7362
Epoch 8/10
 - 11s - loss: 0.5988 - acc: 0.7611 - val_loss: 0.6405 - val_acc: 0.7395
Epoch 9/10
 - 11s - loss: 0.5870 - acc: 0.7673 - val_loss: 0.6378 - val_acc: 0.7437
Epoch 10/10
 - 11s - loss: 0.5761 - acc: 0.7690 - val_loss: 0.6393 - val_acc: 0.7375

  16/8086 [..............................] - ETA: 0s
 640/8086 [=>............................] - ETA: 0s
1232/8086 [===>..........................] - ETA: 0s
1824/8086 [=====>........................] - ETA: 0s
2432/8086 [========>.....................] - ETA: 0s
3040/8086 [==========>...................] - ETA: 0s
3664/8086 [============>.................] - ETA: 0s
4272/8086 [==============>...............] - ETA: 0s
4864/8086 [=================>............] - ETA: 0s
5440/8086 [===================>..........] - ETA: 0s
6048/8086 [=====================>........] - ETA: 0s
6560/8086 [=======================>......] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7712/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 87us/step

test score: [0.663753443938684, 0.7208755874498156]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=128, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 17s - loss: 0.8504 - acc: 0.6250 - val_loss: 0.7743 - val_acc: 0.6626
Epoch 2/10
 - 15s - loss: 0.7100 - acc: 0.7090 - val_loss: 0.7056 - val_acc: 0.7024
Epoch 3/10
 - 15s - loss: 0.6562 - acc: 0.7324 - val_loss: 0.6644 - val_acc: 0.7333
Epoch 4/10
 - 15s - loss: 0.6290 - acc: 0.7455 - val_loss: 0.6536 - val_acc: 0.7390
Epoch 5/10
 - 15s - loss: 0.6068 - acc: 0.7534 - val_loss: 0.6468 - val_acc: 0.7353
Epoch 6/10
 - 15s - loss: 0.5914 - acc: 0.7615 - val_loss: 0.6512 - val_acc: 0.7348
Epoch 7/10
 - 15s - loss: 0.5749 - acc: 0.7653 - val_loss: 0.6513 - val_acc: 0.7375
Epoch 8/10
 - 16s - loss: 0.5608 - acc: 0.7736 - val_loss: 0.6402 - val_acc: 0.7384
Epoch 9/10
 - 15s - loss: 0.5425 - acc: 0.7800 - val_loss: 0.6537 - val_acc: 0.7420
Epoch 10/10
 - 15s - loss: 0.5236 - acc: 0.7842 - val_loss: 0.6629 - val_acc: 0.7330

  16/8086 [..............................] - ETA: 0s
 416/8086 [>.............................] - ETA: 0s
 752/8086 [=>............................] - ETA: 1s
1104/8086 [===>..........................] - ETA: 0s
1440/8086 [====>.........................] - ETA: 0s
1760/8086 [=====>........................] - ETA: 0s
2144/8086 [======>.......................] - ETA: 0s
2560/8086 [========>.....................] - ETA: 0s
2976/8086 [==========>...................] - ETA: 0s
3392/8086 [===========>..................] - ETA: 0s
3760/8086 [============>.................] - ETA: 0s
4192/8086 [==============>...............] - ETA: 0s
4608/8086 [================>.............] - ETA: 0s
5008/8086 [=================>............] - ETA: 0s
5376/8086 [==================>...........] - ETA: 0s
5760/8086 [====================>.........] - ETA: 0s
6176/8086 [=====================>........] - ETA: 0s
6576/8086 [=======================>......] - ETA: 0s
6976/8086 [========================>.....] - ETA: 0s
7360/8086 [==========================>...] - ETA: 0s
7728/8086 [===========================>..] - ETA: 0s
8064/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 134us/step

test score: [0.6968935907749501, 0.714939401449321]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=128, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 17s - loss: 0.8538 - acc: 0.6236 - val_loss: 0.7837 - val_acc: 0.6846
Epoch 2/10
 - 16s - loss: 0.7137 - acc: 0.7044 - val_loss: 0.7019 - val_acc: 0.7113
Epoch 3/10
 - 16s - loss: 0.6625 - acc: 0.7294 - val_loss: 0.6827 - val_acc: 0.7216
Epoch 4/10
 - 15s - loss: 0.6325 - acc: 0.7422 - val_loss: 0.6766 - val_acc: 0.7166
Epoch 5/10
 - 15s - loss: 0.6127 - acc: 0.7490 - val_loss: 0.6582 - val_acc: 0.7361
Epoch 6/10
 - 15s - loss: 0.5961 - acc: 0.7576 - val_loss: 0.6478 - val_acc: 0.7386
Epoch 7/10
 - 16s - loss: 0.5805 - acc: 0.7645 - val_loss: 0.6486 - val_acc: 0.7362
Epoch 8/10
 - 15s - loss: 0.5594 - acc: 0.7740 - val_loss: 0.6418 - val_acc: 0.7359
Epoch 9/10
 - 15s - loss: 0.5423 - acc: 0.7822 - val_loss: 0.6571 - val_acc: 0.7288
Epoch 10/10
 - 16s - loss: 0.5220 - acc: 0.7926 - val_loss: 0.6458 - val_acc: 0.7401

  16/8086 [..............................] - ETA: 0s
 368/8086 [>.............................] - ETA: 1s
 704/8086 [=>............................] - ETA: 1s
1040/8086 [==>...........................] - ETA: 1s
1392/8086 [====>.........................] - ETA: 1s
1744/8086 [=====>........................] - ETA: 0s
2064/8086 [======>.......................] - ETA: 0s
2464/8086 [========>.....................] - ETA: 0s
2880/8086 [=========>....................] - ETA: 0s
3296/8086 [===========>..................] - ETA: 0s
3664/8086 [============>.................] - ETA: 0s
4032/8086 [=============>................] - ETA: 0s
4448/8086 [===============>..............] - ETA: 0s
4848/8086 [================>.............] - ETA: 0s
5248/8086 [==================>...........] - ETA: 0s
5664/8086 [====================>.........] - ETA: 0s
6064/8086 [=====================>........] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
6832/8086 [========================>.....] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7488/8086 [==========================>...] - ETA: 0s
7856/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 138us/step

test score: [0.6710976152661234, 0.7208755874498156]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=128, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 18s - loss: 0.8652 - acc: 0.6155 - val_loss: 0.7792 - val_acc: 0.6665
Epoch 2/10
 - 16s - loss: 0.7291 - acc: 0.6948 - val_loss: 0.7047 - val_acc: 0.7121
Epoch 3/10
 - 16s - loss: 0.6773 - acc: 0.7200 - val_loss: 0.6995 - val_acc: 0.7191
Epoch 4/10
 - 16s - loss: 0.6481 - acc: 0.7333 - val_loss: 0.6697 - val_acc: 0.7318
Epoch 5/10
 - 16s - loss: 0.6258 - acc: 0.7449 - val_loss: 0.6494 - val_acc: 0.7355
Epoch 6/10
 - 16s - loss: 0.6060 - acc: 0.7538 - val_loss: 0.6535 - val_acc: 0.7335
Epoch 7/10
 - 16s - loss: 0.5910 - acc: 0.7592 - val_loss: 0.6462 - val_acc: 0.7353
Epoch 8/10
 - 16s - loss: 0.5721 - acc: 0.7668 - val_loss: 0.6486 - val_acc: 0.7373
Epoch 9/10
 - 16s - loss: 0.5569 - acc: 0.7754 - val_loss: 0.6412 - val_acc: 0.7413
Epoch 10/10
 - 16s - loss: 0.5362 - acc: 0.7836 - val_loss: 0.6580 - val_acc: 0.7263

  16/8086 [..............................] - ETA: 0s
 384/8086 [>.............................] - ETA: 1s
 752/8086 [=>............................] - ETA: 1s
1120/8086 [===>..........................] - ETA: 0s
1488/8086 [====>.........................] - ETA: 0s
1856/8086 [=====>........................] - ETA: 0s
2240/8086 [=======>......................] - ETA: 0s
2560/8086 [========>.....................] - ETA: 0s
2912/8086 [=========>....................] - ETA: 0s
3296/8086 [===========>..................] - ETA: 0s
3696/8086 [============>.................] - ETA: 0s
4096/8086 [==============>...............] - ETA: 0s
4496/8086 [===============>..............] - ETA: 0s
4880/8086 [=================>............] - ETA: 0s
5248/8086 [==================>...........] - ETA: 0s
5616/8086 [===================>..........] - ETA: 0s
5984/8086 [=====================>........] - ETA: 0s
6336/8086 [======================>.......] - ETA: 0s
6640/8086 [=======================>......] - ETA: 0s
6896/8086 [========================>.....] - ETA: 0s
7232/8086 [=========================>....] - ETA: 0s
7616/8086 [===========================>..] - ETA: 0s
7984/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 142us/step

test score: [0.6751054838472977, 0.7158050952410597]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=128, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 18s - loss: 0.8685 - acc: 0.6177 - val_loss: 0.7859 - val_acc: 0.6821
Epoch 2/10
 - 16s - loss: 0.7377 - acc: 0.6943 - val_loss: 0.7174 - val_acc: 0.7035
Epoch 3/10
 - 16s - loss: 0.6756 - acc: 0.7205 - val_loss: 0.6759 - val_acc: 0.7236
Epoch 4/10
 - 16s - loss: 0.6407 - acc: 0.7349 - val_loss: 0.6773 - val_acc: 0.7254
Epoch 5/10
 - 16s - loss: 0.6160 - acc: 0.7492 - val_loss: 0.6770 - val_acc: 0.7178
Epoch 6/10
 - 16s - loss: 0.5984 - acc: 0.7561 - val_loss: 0.6569 - val_acc: 0.7362
Epoch 7/10
 - 16s - loss: 0.5839 - acc: 0.7650 - val_loss: 0.6356 - val_acc: 0.7429
Epoch 8/10
 - 16s - loss: 0.5657 - acc: 0.7704 - val_loss: 0.6434 - val_acc: 0.7420
Epoch 9/10
 - 16s - loss: 0.5471 - acc: 0.7790 - val_loss: 0.6412 - val_acc: 0.7443
Epoch 10/10
 - 17s - loss: 0.5276 - acc: 0.7880 - val_loss: 0.6581 - val_acc: 0.7347

  16/8086 [..............................] - ETA: 1s
 352/8086 [>.............................] - ETA: 1s
 720/8086 [=>............................] - ETA: 1s
1008/8086 [==>...........................] - ETA: 1s
1328/8086 [===>..........................] - ETA: 1s
1712/8086 [=====>........................] - ETA: 0s
2080/8086 [======>.......................] - ETA: 0s
2464/8086 [========>.....................] - ETA: 0s
2832/8086 [=========>....................] - ETA: 0s
3200/8086 [==========>...................] - ETA: 0s
3568/8086 [============>.................] - ETA: 0s
3808/8086 [=============>................] - ETA: 0s
4048/8086 [==============>...............] - ETA: 0s
4384/8086 [===============>..............] - ETA: 0s
4752/8086 [================>.............] - ETA: 0s
5088/8086 [=================>............] - ETA: 0s
5408/8086 [===================>..........] - ETA: 0s
5792/8086 [====================>.........] - ETA: 0s
6160/8086 [=====================>........] - ETA: 0s
6528/8086 [=======================>......] - ETA: 0s
6896/8086 [========================>.....] - ETA: 0s
7248/8086 [=========================>....] - ETA: 0s
7632/8086 [===========================>..] - ETA: 0s
8016/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 148us/step

test score: [0.6828372846757189, 0.7148157308929013]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=128, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 18s - loss: 0.8681 - acc: 0.6132 - val_loss: 0.7652 - val_acc: 0.6677
Epoch 2/10
 - 16s - loss: 0.7247 - acc: 0.7003 - val_loss: 0.7199 - val_acc: 0.7022
Epoch 3/10
 - 16s - loss: 0.6728 - acc: 0.7254 - val_loss: 0.6787 - val_acc: 0.7208
Epoch 4/10
 - 16s - loss: 0.6493 - acc: 0.7326 - val_loss: 0.6644 - val_acc: 0.7254
Epoch 5/10
 - 16s - loss: 0.6291 - acc: 0.7443 - val_loss: 0.6573 - val_acc: 0.7299
Epoch 6/10
 - 16s - loss: 0.6122 - acc: 0.7496 - val_loss: 0.6596 - val_acc: 0.7331
Epoch 7/10
 - 16s - loss: 0.5993 - acc: 0.7570 - val_loss: 0.6448 - val_acc: 0.7376
Epoch 8/10
 - 16s - loss: 0.5827 - acc: 0.7645 - val_loss: 0.6456 - val_acc: 0.7413
Epoch 9/10
 - 16s - loss: 0.5633 - acc: 0.7736 - val_loss: 0.6537 - val_acc: 0.7333
Epoch 10/10
 - 16s - loss: 0.5426 - acc: 0.7835 - val_loss: 0.6448 - val_acc: 0.7446

  16/8086 [..............................] - ETA: 0s
 352/8086 [>.............................] - ETA: 1s
 704/8086 [=>............................] - ETA: 1s
1104/8086 [===>..........................] - ETA: 1s
1408/8086 [====>.........................] - ETA: 0s
1760/8086 [=====>........................] - ETA: 0s
2128/8086 [======>.......................] - ETA: 0s
2512/8086 [========>.....................] - ETA: 0s
2880/8086 [=========>....................] - ETA: 0s
3264/8086 [===========>..................] - ETA: 0s
3648/8086 [============>.................] - ETA: 0s
4032/8086 [=============>................] - ETA: 0s
4416/8086 [===============>..............] - ETA: 0s
4800/8086 [================>.............] - ETA: 0s
5168/8086 [==================>...........] - ETA: 0s
5504/8086 [===================>..........] - ETA: 0s
5856/8086 [====================>.........] - ETA: 0s
6208/8086 [======================>.......] - ETA: 0s
6544/8086 [=======================>......] - ETA: 0s
6896/8086 [========================>.....] - ETA: 0s
7248/8086 [=========================>....] - ETA: 0s
7616/8086 [===========================>..] - ETA: 0s
7984/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 143us/step

test score: [0.6785154036961951, 0.7186495176996301]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=256, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 39s - loss: 0.8770 - acc: 0.6100 - val_loss: 0.7721 - val_acc: 0.6803
Epoch 2/10
 - 38s - loss: 0.7189 - acc: 0.7036 - val_loss: 0.7336 - val_acc: 0.6832
Epoch 3/10
 - 37s - loss: 0.6658 - acc: 0.7253 - val_loss: 0.6940 - val_acc: 0.7110
Epoch 4/10
 - 39s - loss: 0.6350 - acc: 0.7381 - val_loss: 0.6624 - val_acc: 0.7270
Epoch 5/10
 - 53s - loss: 0.6108 - acc: 0.7499 - val_loss: 0.6524 - val_acc: 0.7382
Epoch 6/10
 - 46s - loss: 0.5838 - acc: 0.7622 - val_loss: 0.6500 - val_acc: 0.7365
Epoch 7/10
 - 45s - loss: 0.5590 - acc: 0.7736 - val_loss: 0.6562 - val_acc: 0.7282
Epoch 8/10
 - 43s - loss: 0.5306 - acc: 0.7870 - val_loss: 0.6612 - val_acc: 0.7308
Epoch 9/10
 - 41s - loss: 0.4988 - acc: 0.8023 - val_loss: 0.6752 - val_acc: 0.7267
Epoch 10/10
 - 42s - loss: 0.4622 - acc: 0.8157 - val_loss: 0.7068 - val_acc: 0.7245

  16/8086 [..............................] - ETA: 2s
 192/8086 [..............................] - ETA: 2s
 352/8086 [>.............................] - ETA: 2s
 528/8086 [>.............................] - ETA: 2s
 704/8086 [=>............................] - ETA: 2s
 880/8086 [==>...........................] - ETA: 2s
1056/8086 [==>...........................] - ETA: 2s
1216/8086 [===>..........................] - ETA: 2s
1376/8086 [====>.........................] - ETA: 2s
1536/8086 [====>.........................] - ETA: 2s
1696/8086 [=====>........................] - ETA: 1s
1872/8086 [=====>........................] - ETA: 1s
2048/8086 [======>.......................] - ETA: 1s
2224/8086 [=======>......................] - ETA: 1s
2400/8086 [=======>......................] - ETA: 1s
2576/8086 [========>.....................] - ETA: 1s
2752/8086 [=========>....................] - ETA: 1s
2896/8086 [=========>....................] - ETA: 1s
3072/8086 [==========>...................] - ETA: 1s
3248/8086 [===========>..................] - ETA: 1s
3408/8086 [===========>..................] - ETA: 1s
3584/8086 [============>.................] - ETA: 1s
3760/8086 [============>.................] - ETA: 1s
3936/8086 [=============>................] - ETA: 1s
4096/8086 [==============>...............] - ETA: 1s
4272/8086 [==============>...............] - ETA: 1s
4448/8086 [===============>..............] - ETA: 1s
4608/8086 [================>.............] - ETA: 1s
4768/8086 [================>.............] - ETA: 1s
4944/8086 [=================>............] - ETA: 0s
5120/8086 [=================>............] - ETA: 0s
5280/8086 [==================>...........] - ETA: 0s
5456/8086 [===================>..........] - ETA: 0s
5632/8086 [===================>..........] - ETA: 0s
5808/8086 [====================>.........] - ETA: 0s
5952/8086 [=====================>........] - ETA: 0s
6112/8086 [=====================>........] - ETA: 0s
6288/8086 [======================>.......] - ETA: 0s
6464/8086 [======================>.......] - ETA: 0s
6640/8086 [=======================>......] - ETA: 0s
6800/8086 [========================>.....] - ETA: 0s
6976/8086 [========================>.....] - ETA: 0s
7152/8086 [=========================>....] - ETA: 0s
7328/8086 [==========================>...] - ETA: 0s
7504/8086 [==========================>...] - ETA: 0s
7664/8086 [===========================>..] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8000/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 308us/step

test score: [0.7379351764467722, 0.7078901805589909]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=256, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 39s - loss: 0.8622 - acc: 0.6211 - val_loss: 0.7488 - val_acc: 0.6962
Epoch 2/10
 - 37s - loss: 0.7072 - acc: 0.7110 - val_loss: 0.6938 - val_acc: 0.7146
Epoch 3/10
 - 34s - loss: 0.6604 - acc: 0.7273 - val_loss: 0.6739 - val_acc: 0.7206
Epoch 4/10
 - 34s - loss: 0.6313 - acc: 0.7415 - val_loss: 0.6865 - val_acc: 0.7209
Epoch 5/10
 - 36s - loss: 0.6107 - acc: 0.7532 - val_loss: 0.6564 - val_acc: 0.7308
Epoch 6/10
 - 31s - loss: 0.5913 - acc: 0.7579 - val_loss: 0.6459 - val_acc: 0.7364
Epoch 7/10
 - 31s - loss: 0.5691 - acc: 0.7703 - val_loss: 0.6419 - val_acc: 0.7387
Epoch 8/10
 - 31s - loss: 0.5424 - acc: 0.7835 - val_loss: 0.6321 - val_acc: 0.7460
Epoch 9/10
 - 31s - loss: 0.5169 - acc: 0.7923 - val_loss: 0.6548 - val_acc: 0.7336
Epoch 10/10
 - 31s - loss: 0.4832 - acc: 0.8072 - val_loss: 0.6985 - val_acc: 0.7279

  16/8086 [..............................] - ETA: 1s
 304/8086 [>.............................] - ETA: 1s
 576/8086 [=>............................] - ETA: 1s
 848/8086 [==>...........................] - ETA: 1s
1120/8086 [===>..........................] - ETA: 1s
1392/8086 [====>.........................] - ETA: 1s
1664/8086 [=====>........................] - ETA: 1s
1936/8086 [======>.......................] - ETA: 1s
2208/8086 [=======>......................] - ETA: 1s
2496/8086 [========>.....................] - ETA: 1s
2768/8086 [=========>....................] - ETA: 0s
3040/8086 [==========>...................] - ETA: 0s
3312/8086 [===========>..................] - ETA: 0s
3584/8086 [============>.................] - ETA: 0s
3872/8086 [=============>................] - ETA: 0s
4160/8086 [==============>...............] - ETA: 0s
4448/8086 [===============>..............] - ETA: 0s
4736/8086 [================>.............] - ETA: 0s
5024/8086 [=================>............] - ETA: 0s
5312/8086 [==================>...........] - ETA: 0s
5584/8086 [===================>..........] - ETA: 0s
5872/8086 [====================>.........] - ETA: 0s
6160/8086 [=====================>........] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
6720/8086 [=======================>......] - ETA: 0s
7008/8086 [=========================>....] - ETA: 0s
7296/8086 [==========================>...] - ETA: 0s
7568/8086 [===========================>..] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 184us/step

test score: [0.7215882583498041, 0.7179074944348256]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=256, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 32s - loss: 0.8834 - acc: 0.6005 - val_loss: 0.7948 - val_acc: 0.6498
Epoch 2/10
 - 31s - loss: 0.7221 - acc: 0.6952 - val_loss: 0.7152 - val_acc: 0.7058
Epoch 3/10
 - 31s - loss: 0.6642 - acc: 0.7248 - val_loss: 0.6769 - val_acc: 0.7254
Epoch 4/10
 - 31s - loss: 0.6316 - acc: 0.7429 - val_loss: 0.6574 - val_acc: 0.7345
Epoch 5/10
 - 31s - loss: 0.6102 - acc: 0.7523 - val_loss: 0.6508 - val_acc: 0.7361
Epoch 6/10
 - 31s - loss: 0.5898 - acc: 0.7617 - val_loss: 0.6438 - val_acc: 0.7369
Epoch 7/10
 - 31s - loss: 0.5696 - acc: 0.7724 - val_loss: 0.6383 - val_acc: 0.7387
Epoch 8/10
 - 31s - loss: 0.5459 - acc: 0.7831 - val_loss: 0.6603 - val_acc: 0.7396
Epoch 9/10
 - 31s - loss: 0.5214 - acc: 0.7920 - val_loss: 0.6513 - val_acc: 0.7293
Epoch 10/10
 - 31s - loss: 0.4913 - acc: 0.8038 - val_loss: 0.6649 - val_acc: 0.7270

  16/8086 [..............................] - ETA: 1s
 288/8086 [>.............................] - ETA: 1s
 560/8086 [=>............................] - ETA: 1s
 832/8086 [==>...........................] - ETA: 1s
1104/8086 [===>..........................] - ETA: 1s
1376/8086 [====>.........................] - ETA: 1s
1648/8086 [=====>........................] - ETA: 1s
1920/8086 [======>.......................] - ETA: 1s
2192/8086 [=======>......................] - ETA: 1s
2464/8086 [========>.....................] - ETA: 1s
2736/8086 [=========>....................] - ETA: 1s
3008/8086 [==========>...................] - ETA: 0s
3280/8086 [===========>..................] - ETA: 0s
3552/8086 [============>.................] - ETA: 0s
3824/8086 [=============>................] - ETA: 0s
4096/8086 [==============>...............] - ETA: 0s
4368/8086 [===============>..............] - ETA: 0s
4640/8086 [================>.............] - ETA: 0s
4912/8086 [=================>............] - ETA: 0s
5184/8086 [==================>...........] - ETA: 0s
5456/8086 [===================>..........] - ETA: 0s
5728/8086 [====================>.........] - ETA: 0s
6000/8086 [=====================>........] - ETA: 0s
6272/8086 [======================>.......] - ETA: 0s
6544/8086 [=======================>......] - ETA: 0s
6816/8086 [========================>.....] - ETA: 0s
7088/8086 [=========================>....] - ETA: 0s
7360/8086 [==========================>...] - ETA: 0s
7632/8086 [===========================>..] - ETA: 0s
7904/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 189us/step

test score: [0.6940858023438806, 0.713455354941826]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=256, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 30s - loss: 0.8562 - acc: 0.6211 - val_loss: 0.7521 - val_acc: 0.6906
Epoch 2/10
 - 28s - loss: 0.7141 - acc: 0.7044 - val_loss: 0.7228 - val_acc: 0.7036
Epoch 3/10
 - 29s - loss: 0.6674 - acc: 0.7239 - val_loss: 0.6945 - val_acc: 0.7233
Epoch 4/10
 - 28s - loss: 0.6361 - acc: 0.7392 - val_loss: 0.6688 - val_acc: 0.7314
Epoch 5/10
 - 29s - loss: 0.6122 - acc: 0.7509 - val_loss: 0.6596 - val_acc: 0.7359
Epoch 6/10
 - 28s - loss: 0.5934 - acc: 0.7592 - val_loss: 0.6496 - val_acc: 0.7378
Epoch 7/10
 - 28s - loss: 0.5712 - acc: 0.7687 - val_loss: 0.6504 - val_acc: 0.7325
Epoch 8/10
 - 28s - loss: 0.5496 - acc: 0.7784 - val_loss: 0.6501 - val_acc: 0.7336
Epoch 9/10
 - 28s - loss: 0.5233 - acc: 0.7885 - val_loss: 0.6617 - val_acc: 0.7284
Epoch 10/10
 - 28s - loss: 0.4909 - acc: 0.8025 - val_loss: 0.6699 - val_acc: 0.7262

  16/8086 [..............................] - ETA: 1s
 272/8086 [>.............................] - ETA: 1s
 544/8086 [=>............................] - ETA: 1s
 800/8086 [=>............................] - ETA: 1s
1040/8086 [==>...........................] - ETA: 1s
1296/8086 [===>..........................] - ETA: 1s
1536/8086 [====>.........................] - ETA: 1s
1792/8086 [=====>........................] - ETA: 1s
2032/8086 [======>.......................] - ETA: 1s
2304/8086 [=======>......................] - ETA: 1s
2576/8086 [========>.....................] - ETA: 1s
2848/8086 [=========>....................] - ETA: 1s
3120/8086 [==========>...................] - ETA: 0s
3392/8086 [===========>..................] - ETA: 0s
3648/8086 [============>.................] - ETA: 0s
3920/8086 [=============>................] - ETA: 0s
4176/8086 [==============>...............] - ETA: 0s
4448/8086 [===============>..............] - ETA: 0s
4704/8086 [================>.............] - ETA: 0s
4976/8086 [=================>............] - ETA: 0s
5232/8086 [==================>...........] - ETA: 0s
5504/8086 [===================>..........] - ETA: 0s
5776/8086 [====================>.........] - ETA: 0s
6048/8086 [=====================>........] - ETA: 0s
6320/8086 [======================>.......] - ETA: 0s
6592/8086 [=======================>......] - ETA: 0s
6864/8086 [========================>.....] - ETA: 0s
7136/8086 [=========================>....] - ETA: 0s
7408/8086 [==========================>...] - ETA: 0s
7680/8086 [===========================>..] - ETA: 0s
7952/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 196us/step

test score: [0.6943037942136658, 0.716176106851348]


Training new model, loss:categorical_crossentropy, optimizer=nadam, lstm_len=256, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 30s - loss: 0.8820 - acc: 0.6073 - val_loss: 0.7877 - val_acc: 0.6616
Epoch 2/10
 - 29s - loss: 0.7279 - acc: 0.6973 - val_loss: 0.7268 - val_acc: 0.6965
Epoch 3/10
 - 29s - loss: 0.6693 - acc: 0.7274 - val_loss: 0.6862 - val_acc: 0.7212
Epoch 4/10
 - 29s - loss: 0.6391 - acc: 0.7385 - val_loss: 0.6694 - val_acc: 0.7307
Epoch 5/10
 - 29s - loss: 0.6151 - acc: 0.7487 - val_loss: 0.6549 - val_acc: 0.7321
Epoch 6/10
 - 29s - loss: 0.5960 - acc: 0.7595 - val_loss: 0.6390 - val_acc: 0.7382
Epoch 7/10
 - 29s - loss: 0.5755 - acc: 0.7670 - val_loss: 0.6414 - val_acc: 0.7387
Epoch 8/10
 - 29s - loss: 0.5534 - acc: 0.7767 - val_loss: 0.6369 - val_acc: 0.7382
Epoch 9/10
 - 29s - loss: 0.5278 - acc: 0.7879 - val_loss: 0.6395 - val_acc: 0.7432
Epoch 10/10
 - 28s - loss: 0.4925 - acc: 0.7987 - val_loss: 0.7233 - val_acc: 0.7375

  16/8086 [..............................] - ETA: 1s
 272/8086 [>.............................] - ETA: 1s
 544/8086 [=>............................] - ETA: 1s
 800/8086 [=>............................] - ETA: 1s
1056/8086 [==>...........................] - ETA: 1s
1312/8086 [===>..........................] - ETA: 1s
1552/8086 [====>.........................] - ETA: 1s
1808/8086 [=====>........................] - ETA: 1s
2064/8086 [======>.......................] - ETA: 1s
2320/8086 [=======>......................] - ETA: 1s
2576/8086 [========>.....................] - ETA: 1s
2832/8086 [=========>....................] - ETA: 1s
3088/8086 [==========>...................] - ETA: 1s
3344/8086 [===========>..................] - ETA: 0s
3600/8086 [============>.................] - ETA: 0s
3856/8086 [=============>................] - ETA: 0s
4112/8086 [==============>...............] - ETA: 0s
4368/8086 [===============>..............] - ETA: 0s
4624/8086 [================>.............] - ETA: 0s
4880/8086 [=================>............] - ETA: 0s
5136/8086 [==================>...........] - ETA: 0s
5392/8086 [===================>..........] - ETA: 0s
5664/8086 [====================>.........] - ETA: 0s
5920/8086 [====================>.........] - ETA: 0s
6176/8086 [=====================>........] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
6688/8086 [=======================>......] - ETA: 0s
6944/8086 [========================>.....] - ETA: 0s
7200/8086 [=========================>....] - ETA: 0s
7456/8086 [==========================>...] - ETA: 0s
7712/8086 [===========================>..] - ETA: 0s
7968/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 201us/step

test score: [0.7646781098472579, 0.7179074944495683]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=32, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 9s - loss: 1.0431 - acc: 0.4973 - val_loss: 1.0324 - val_acc: 0.5128
Epoch 2/10
 - 9s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0327 - val_acc: 0.5128
Epoch 3/10
 - 8s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128
Epoch 4/10
 - 9s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0311 - val_acc: 0.5128
Epoch 5/10
 - 8s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0307 - val_acc: 0.5128
Epoch 6/10
 - 8s - loss: 1.0400 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 7/10
 - 8s - loss: 1.0399 - acc: 0.4978 - val_loss: 1.0313 - val_acc: 0.5128
Epoch 8/10
 - 8s - loss: 1.0395 - acc: 0.4978 - val_loss: 1.0316 - val_acc: 0.5128
Epoch 9/10
 - 8s - loss: 1.0384 - acc: 0.4978 - val_loss: 1.0286 - val_acc: 0.5128
Epoch 10/10
 - 8s - loss: 1.0351 - acc: 0.4978 - val_loss: 1.0246 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 784/8086 [=>............................] - ETA: 0s
1536/8086 [====>.........................] - ETA: 0s
2320/8086 [=======>......................] - ETA: 0s
2960/8086 [=========>....................] - ETA: 0s
3696/8086 [============>.................] - ETA: 0s
4464/8086 [===============>..............] - ETA: 0s
5248/8086 [==================>...........] - ETA: 0s
6000/8086 [=====================>........] - ETA: 0s
6736/8086 [=======================>......] - ETA: 0s
7488/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 68us/step

test score: [1.0269243180117866, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=32, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 9s - loss: 1.0440 - acc: 0.4973 - val_loss: 1.0324 - val_acc: 0.5128
Epoch 2/10
 - 8s - loss: 1.0415 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 3/10
 - 8s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128
Epoch 4/10
 - 8s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0350 - val_acc: 0.5128
Epoch 5/10
 - 8s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0327 - val_acc: 0.5128
Epoch 6/10
 - 9s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 7/10
 - 9s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0322 - val_acc: 0.5128
Epoch 8/10
 - 8s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 9/10
 - 8s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 10/10
 - 8s - loss: 1.0401 - acc: 0.4978 - val_loss: 1.0320 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 768/8086 [=>............................] - ETA: 0s
1552/8086 [====>.........................] - ETA: 0s
2224/8086 [=======>......................] - ETA: 0s
3008/8086 [==========>...................] - ETA: 0s
3760/8086 [============>.................] - ETA: 0s
4544/8086 [===============>..............] - ETA: 0s
5312/8086 [==================>...........] - ETA: 0s
6096/8086 [=====================>........] - ETA: 0s
6864/8086 [========================>.....] - ETA: 0s
7648/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 66us/step

test score: [1.034424910974656, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=32, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 9s - loss: 1.0433 - acc: 0.4971 - val_loss: 1.0309 - val_acc: 0.5128
Epoch 2/10
 - 9s - loss: 1.0415 - acc: 0.4978 - val_loss: 1.0328 - val_acc: 0.5128
Epoch 3/10
 - 9s - loss: 1.0411 - acc: 0.4978 - val_loss: 1.0318 - val_acc: 0.5128
Epoch 4/10
 - 8s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0316 - val_acc: 0.5128
Epoch 5/10
 - 9s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0326 - val_acc: 0.5128
Epoch 6/10
 - 8s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 7/10
 - 9s - loss: 1.0400 - acc: 0.4978 - val_loss: 1.0307 - val_acc: 0.5128
Epoch 8/10
 - 9s - loss: 1.0395 - acc: 0.4978 - val_loss: 1.0316 - val_acc: 0.5128
Epoch 9/10
 - 9s - loss: 1.0391 - acc: 0.4978 - val_loss: 1.0299 - val_acc: 0.5128
Epoch 10/10
 - 9s - loss: 1.0373 - acc: 0.4978 - val_loss: 1.0269 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 784/8086 [=>............................] - ETA: 0s
1552/8086 [====>.........................] - ETA: 0s
2336/8086 [=======>......................] - ETA: 0s
3088/8086 [==========>...................] - ETA: 0s
3664/8086 [============>.................] - ETA: 0s
4432/8086 [===============>..............] - ETA: 0s
5200/8086 [==================>...........] - ETA: 0s
5968/8086 [=====================>........] - ETA: 0s
6736/8086 [=======================>......] - ETA: 0s
7488/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 68us/step

test score: [1.029693247819047, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=32, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 10s - loss: 1.0442 - acc: 0.4975 - val_loss: 1.0344 - val_acc: 0.5128
Epoch 2/10
 - 8s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0336 - val_acc: 0.5128
Epoch 3/10
 - 9s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0319 - val_acc: 0.5128
Epoch 4/10
 - 9s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0324 - val_acc: 0.5128
Epoch 5/10
 - 9s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 6/10
 - 9s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 7/10
 - 8s - loss: 1.0400 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128
Epoch 8/10
 - 8s - loss: 1.0401 - acc: 0.4978 - val_loss: 1.0306 - val_acc: 0.5128
Epoch 9/10
 - 8s - loss: 1.0398 - acc: 0.4978 - val_loss: 1.0324 - val_acc: 0.5128
Epoch 10/10
 - 14s - loss: 1.0392 - acc: 0.4978 - val_loss: 1.0324 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 1s
 528/8086 [>.............................] - ETA: 0s
 960/8086 [==>...........................] - ETA: 0s
1552/8086 [====>.........................] - ETA: 0s
2096/8086 [======>.......................] - ETA: 0s
2704/8086 [=========>....................] - ETA: 0s
3344/8086 [===========>..................] - ETA: 0s
3952/8086 [=============>................] - ETA: 0s
4576/8086 [===============>..............] - ETA: 0s
5200/8086 [==================>...........] - ETA: 0s
5792/8086 [====================>.........] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
7072/8086 [=========================>....] - ETA: 0s
7680/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 85us/step

test score: [1.0337243503548856, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=32, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 11s - loss: 1.0438 - acc: 0.4972 - val_loss: 1.0333 - val_acc: 0.5128
Epoch 2/10
 - 9s - loss: 1.0418 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128
Epoch 3/10
 - 10s - loss: 1.0412 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 4/10
 - 10s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0342 - val_acc: 0.5128
Epoch 5/10
 - 10s - loss: 1.0409 - acc: 0.4978 - val_loss: 1.0315 - val_acc: 0.5128
Epoch 6/10
 - 10s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 7/10
 - 9s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0311 - val_acc: 0.5128
Epoch 8/10
 - 10s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0325 - val_acc: 0.5128
Epoch 9/10
 - 9s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 10/10
 - 9s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0328 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 736/8086 [=>............................] - ETA: 0s
1312/8086 [===>..........................] - ETA: 0s
2048/8086 [======>.......................] - ETA: 0s
2816/8086 [=========>....................] - ETA: 0s
3568/8086 [============>.................] - ETA: 0s
4320/8086 [===============>..............] - ETA: 0s
5024/8086 [=================>............] - ETA: 0s
5760/8086 [====================>.........] - ETA: 0s
6512/8086 [=======================>......] - ETA: 0s
7264/8086 [=========================>....] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 72us/step

test score: [1.0346703902789929, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=64, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 13s - loss: 1.0428 - acc: 0.4970 - val_loss: 1.0315 - val_acc: 0.5128
Epoch 2/10
 - 11s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0358 - val_acc: 0.5128
Epoch 3/10
 - 11s - loss: 1.0412 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128
Epoch 4/10
 - 11s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 5/10
 - 10s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0318 - val_acc: 0.5128
Epoch 6/10
 - 11s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0318 - val_acc: 0.5128
Epoch 7/10
 - 11s - loss: 1.0401 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128
Epoch 8/10
 - 12s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0306 - val_acc: 0.5128
Epoch 9/10
 - 10s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0314 - val_acc: 0.5128
Epoch 10/10
 - 10s - loss: 1.0399 - acc: 0.4978 - val_loss: 1.0314 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 672/8086 [=>............................] - ETA: 0s
1328/8086 [===>..........................] - ETA: 0s
1952/8086 [======>.......................] - ETA: 0s
2592/8086 [========>.....................] - ETA: 0s
3232/8086 [==========>...................] - ETA: 0s
3824/8086 [=============>................] - ETA: 0s
4464/8086 [===============>..............] - ETA: 0s
5104/8086 [=================>............] - ETA: 0s
5696/8086 [====================>.........] - ETA: 0s
6288/8086 [======================>.......] - ETA: 0s
6928/8086 [========================>.....] - ETA: 0s
7520/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 81us/step

test score: [1.0339491267384735, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=64, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 12s - loss: 1.0436 - acc: 0.4969 - val_loss: 1.0325 - val_acc: 0.5128
Epoch 2/10
 - 11s - loss: 1.0409 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 3/10
 - 11s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0315 - val_acc: 0.5128
Epoch 4/10
 - 10s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0334 - val_acc: 0.5128
Epoch 5/10
 - 10s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0319 - val_acc: 0.5128
Epoch 6/10
 - 10s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0324 - val_acc: 0.5128
Epoch 7/10
 - 10s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 8/10
 - 10s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0311 - val_acc: 0.5128
Epoch 9/10
 - 10s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 10/10
 - 10s - loss: 1.0401 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 528/8086 [>.............................] - ETA: 0s
1072/8086 [==>...........................] - ETA: 0s
1648/8086 [=====>........................] - ETA: 0s
2272/8086 [=======>......................] - ETA: 0s
2896/8086 [=========>....................] - ETA: 0s
3520/8086 [============>.................] - ETA: 0s
4160/8086 [==============>...............] - ETA: 0s
4832/8086 [================>.............] - ETA: 0s
5488/8086 [===================>..........] - ETA: 0s
6144/8086 [=====================>........] - ETA: 0s
6784/8086 [========================>.....] - ETA: 0s
7440/8086 [==========================>...] - ETA: 0s
8086/8086 [==============================] - 1s 82us/step

test score: [1.0348695428425092, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=64, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 12s - loss: 1.0434 - acc: 0.4976 - val_loss: 1.0310 - val_acc: 0.5128
Epoch 2/10
 - 10s - loss: 1.0413 - acc: 0.4978 - val_loss: 1.0333 - val_acc: 0.5128
Epoch 3/10
 - 11s - loss: 1.0409 - acc: 0.4978 - val_loss: 1.0315 - val_acc: 0.5128
Epoch 4/10
 - 10s - loss: 1.0412 - acc: 0.4978 - val_loss: 1.0333 - val_acc: 0.5128
Epoch 5/10
 - 10s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0313 - val_acc: 0.5128
Epoch 6/10
 - 10s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0313 - val_acc: 0.5128
Epoch 7/10
 - 10s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0306 - val_acc: 0.5128
Epoch 8/10
 - 10s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0326 - val_acc: 0.5128
Epoch 9/10
 - 10s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0324 - val_acc: 0.5128
Epoch 10/10
 - 10s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0319 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 672/8086 [=>............................] - ETA: 0s
1056/8086 [==>...........................] - ETA: 0s
1712/8086 [=====>........................] - ETA: 0s
2368/8086 [=======>......................] - ETA: 0s
3008/8086 [==========>...................] - ETA: 0s
3664/8086 [============>.................] - ETA: 0s
4304/8086 [==============>...............] - ETA: 0s
4960/8086 [=================>............] - ETA: 0s
5616/8086 [===================>..........] - ETA: 0s
6256/8086 [======================>.......] - ETA: 0s
6912/8086 [========================>.....] - ETA: 0s
7568/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 81us/step

test score: [1.034488395892126, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=64, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 13s - loss: 1.0436 - acc: 0.4976 - val_loss: 1.0318 - val_acc: 0.5128
Epoch 2/10
 - 13s - loss: 1.0414 - acc: 0.4978 - val_loss: 1.0320 - val_acc: 0.5128
Epoch 3/10
 - 12s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0325 - val_acc: 0.5128
Epoch 4/10
 - 12s - loss: 1.0411 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128
Epoch 5/10
 - 11s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 6/10
 - 12s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0327 - val_acc: 0.5128
Epoch 7/10
 - 12s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0320 - val_acc: 0.5128
Epoch 8/10
 - 11s - loss: 1.0400 - acc: 0.4978 - val_loss: 1.0305 - val_acc: 0.5128
Epoch 9/10
 - 11s - loss: 1.0400 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 10/10
 - 11s - loss: 1.0400 - acc: 0.4978 - val_loss: 1.0322 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 496/8086 [>.............................] - ETA: 0s
1104/8086 [===>..........................] - ETA: 0s
1728/8086 [=====>........................] - ETA: 0s
2208/8086 [=======>......................] - ETA: 0s
2512/8086 [========>.....................] - ETA: 0s
2992/8086 [==========>...................] - ETA: 0s
3440/8086 [===========>..................] - ETA: 0s
3936/8086 [=============>................] - ETA: 0s
4448/8086 [===============>..............] - ETA: 0s
5008/8086 [=================>............] - ETA: 0s
5568/8086 [===================>..........] - ETA: 0s
6160/8086 [=====================>........] - ETA: 0s
6720/8086 [=======================>......] - ETA: 0s
7248/8086 [=========================>....] - ETA: 0s
7824/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 98us/step

test score: [1.034256036627154, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=64, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 15s - loss: 1.0437 - acc: 0.4970 - val_loss: 1.0316 - val_acc: 0.5128
Epoch 2/10
 - 13s - loss: 1.0416 - acc: 0.4978 - val_loss: 1.0332 - val_acc: 0.5128
Epoch 3/10
 - 12s - loss: 1.0411 - acc: 0.4978 - val_loss: 1.0309 - val_acc: 0.5128
Epoch 4/10
 - 11s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 5/10
 - 12s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0326 - val_acc: 0.5128
Epoch 6/10
 - 13s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 7/10
 - 12s - loss: 1.0396 - acc: 0.4978 - val_loss: 1.0305 - val_acc: 0.5128
Epoch 8/10
 - 12s - loss: 1.0390 - acc: 0.4978 - val_loss: 1.0299 - val_acc: 0.5128
Epoch 9/10
 - 12s - loss: 1.0377 - acc: 0.4978 - val_loss: 1.0308 - val_acc: 0.5128
Epoch 10/10
 - 12s - loss: 1.0315 - acc: 0.4978 - val_loss: 1.0093 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 400/8086 [>.............................] - ETA: 1s
 800/8086 [=>............................] - ETA: 0s
1232/8086 [===>..........................] - ETA: 0s
1824/8086 [=====>........................] - ETA: 0s
2384/8086 [=======>......................] - ETA: 0s
2976/8086 [==========>...................] - ETA: 0s
3536/8086 [============>.................] - ETA: 0s
4080/8086 [==============>...............] - ETA: 0s
4544/8086 [===============>..............] - ETA: 0s
5072/8086 [=================>............] - ETA: 0s
5632/8086 [===================>..........] - ETA: 0s
6128/8086 [=====================>........] - ETA: 0s
6720/8086 [=======================>......] - ETA: 0s
7280/8086 [==========================>...] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 98us/step

test score: [1.019019094944708, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 19s - loss: 1.0430 - acc: 0.4973 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 2/10
 - 18s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0319 - val_acc: 0.5128
Epoch 3/10
 - 18s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0314 - val_acc: 0.5128
Epoch 4/10
 - 15s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0333 - val_acc: 0.5128
Epoch 5/10
 - 14s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0338 - val_acc: 0.5128
Epoch 6/10
 - 14s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0319 - val_acc: 0.5128
Epoch 7/10
 - 14s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0326 - val_acc: 0.5128
Epoch 8/10
 - 14s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 9/10
 - 14s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 10/10
 - 14s - loss: 1.0400 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 1s
 400/8086 [>.............................] - ETA: 1s
 784/8086 [=>............................] - ETA: 0s
1168/8086 [===>..........................] - ETA: 0s
1552/8086 [====>.........................] - ETA: 0s
1936/8086 [======>.......................] - ETA: 0s
2320/8086 [=======>......................] - ETA: 0s
2704/8086 [=========>....................] - ETA: 0s
3104/8086 [==========>...................] - ETA: 0s
3472/8086 [===========>..................] - ETA: 0s
3856/8086 [=============>................] - ETA: 0s
4240/8086 [==============>...............] - ETA: 0s
4624/8086 [================>.............] - ETA: 0s
4992/8086 [=================>............] - ETA: 0s
5280/8086 [==================>...........] - ETA: 0s
5664/8086 [====================>.........] - ETA: 0s
6048/8086 [=====================>........] - ETA: 0s
6432/8086 [======================>.......] - ETA: 0s
6800/8086 [========================>.....] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7552/8086 [===========================>..] - ETA: 0s
7936/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 135us/step

test score: [1.0343294346535998, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 15s - loss: 1.0425 - acc: 0.4975 - val_loss: 1.0315 - val_acc: 0.5128
Epoch 2/10
 - 14s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0332 - val_acc: 0.5128
Epoch 3/10
 - 14s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0318 - val_acc: 0.5128
Epoch 4/10
 - 14s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0337 - val_acc: 0.5128
Epoch 5/10
 - 14s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 6/10
 - 14s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0313 - val_acc: 0.5128
Epoch 7/10
 - 14s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0328 - val_acc: 0.5128
Epoch 8/10
 - 14s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0351 - val_acc: 0.5128
Epoch 9/10
 - 14s - loss: 1.0399 - acc: 0.4978 - val_loss: 1.0312 - val_acc: 0.5128
Epoch 10/10
 - 14s - loss: 1.0399 - acc: 0.4978 - val_loss: 1.0304 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 1s
 320/8086 [>.............................] - ETA: 1s
 704/8086 [=>............................] - ETA: 1s
1104/8086 [===>..........................] - ETA: 0s
1504/8086 [====>.........................] - ETA: 0s
1904/8086 [======>.......................] - ETA: 0s
2304/8086 [=======>......................] - ETA: 0s
2720/8086 [=========>....................] - ETA: 0s
3120/8086 [==========>...................] - ETA: 0s
3520/8086 [============>.................] - ETA: 0s
3920/8086 [=============>................] - ETA: 0s
4336/8086 [===============>..............] - ETA: 0s
4736/8086 [================>.............] - ETA: 0s
5136/8086 [==================>...........] - ETA: 0s
5536/8086 [===================>..........] - ETA: 0s
5952/8086 [=====================>........] - ETA: 0s
6352/8086 [======================>.......] - ETA: 0s
6768/8086 [========================>.....] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7552/8086 [===========================>..] - ETA: 0s
7936/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 130us/step

test score: [1.034002370448682, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 16s - loss: 1.0431 - acc: 0.4972 - val_loss: 1.0340 - val_acc: 0.5128
Epoch 2/10
 - 14s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0337 - val_acc: 0.5128
Epoch 3/10
 - 14s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0322 - val_acc: 0.5128
Epoch 4/10
 - 14s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 5/10
 - 14s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0318 - val_acc: 0.5128
Epoch 6/10
 - 14s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0322 - val_acc: 0.5128
Epoch 7/10
 - 14s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0302 - val_acc: 0.5128
Epoch 8/10
 - 14s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0320 - val_acc: 0.5128
Epoch 9/10
 - 14s - loss: 1.0401 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 10/10
 - 14s - loss: 1.0398 - acc: 0.4978 - val_loss: 1.0302 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 0s
 400/8086 [>.............................] - ETA: 1s
 784/8086 [=>............................] - ETA: 0s
1168/8086 [===>..........................] - ETA: 0s
1424/8086 [====>.........................] - ETA: 0s
1824/8086 [=====>........................] - ETA: 0s
2208/8086 [=======>......................] - ETA: 0s
2592/8086 [========>.....................] - ETA: 0s
2976/8086 [==========>...................] - ETA: 0s
3360/8086 [===========>..................] - ETA: 0s
3744/8086 [============>.................] - ETA: 0s
4128/8086 [==============>...............] - ETA: 0s
4512/8086 [===============>..............] - ETA: 0s
4912/8086 [=================>............] - ETA: 0s
5296/8086 [==================>...........] - ETA: 0s
5680/8086 [====================>.........] - ETA: 0s
6064/8086 [=====================>........] - ETA: 0s
6448/8086 [======================>.......] - ETA: 0s
6832/8086 [========================>.....] - ETA: 0s
7216/8086 [=========================>....] - ETA: 0s
7600/8086 [===========================>..] - ETA: 0s
7984/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 135us/step

test score: [1.0339687150980676, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 16s - loss: 1.0428 - acc: 0.4972 - val_loss: 1.0319 - val_acc: 0.5128
Epoch 2/10
 - 14s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0315 - val_acc: 0.5128
Epoch 3/10
 - 14s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 4/10
 - 15s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 5/10
 - 14s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0325 - val_acc: 0.5128
Epoch 6/10
 - 14s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 7/10
 - 14s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0315 - val_acc: 0.5128
Epoch 8/10
 - 14s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0331 - val_acc: 0.5128
Epoch 9/10
 - 15s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0332 - val_acc: 0.5128
Epoch 10/10
 - 14s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0315 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 1s
 368/8086 [>.............................] - ETA: 1s
 656/8086 [=>............................] - ETA: 1s
 992/8086 [==>...........................] - ETA: 1s
1344/8086 [===>..........................] - ETA: 1s
1712/8086 [=====>........................] - ETA: 0s
2080/8086 [======>.......................] - ETA: 0s
2416/8086 [=======>......................] - ETA: 0s
2768/8086 [=========>....................] - ETA: 0s
3136/8086 [==========>...................] - ETA: 0s
3504/8086 [============>.................] - ETA: 0s
3872/8086 [=============>................] - ETA: 0s
4240/8086 [==============>...............] - ETA: 0s
4608/8086 [================>.............] - ETA: 0s
4960/8086 [=================>............] - ETA: 0s
5328/8086 [==================>...........] - ETA: 0s
5696/8086 [====================>.........] - ETA: 0s
6064/8086 [=====================>........] - ETA: 0s
6416/8086 [======================>.......] - ETA: 0s
6736/8086 [=======================>......] - ETA: 0s
7088/8086 [=========================>....] - ETA: 0s
7456/8086 [==========================>...] - ETA: 0s
7728/8086 [===========================>..] - ETA: 0s
8086/8086 [==============================] - 1s 146us/step

test score: [1.0345504071020768, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=128, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 17s - loss: 1.0432 - acc: 0.4977 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 2/10
 - 15s - loss: 1.0412 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 3/10
 - 15s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0325 - val_acc: 0.5128
Epoch 4/10
 - 15s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0320 - val_acc: 0.5128
Epoch 5/10
 - 15s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0325 - val_acc: 0.5128
Epoch 6/10
 - 15s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0309 - val_acc: 0.5128
Epoch 7/10
 - 15s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0320 - val_acc: 0.5128
Epoch 8/10
 - 14s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0311 - val_acc: 0.5128
Epoch 9/10
 - 14s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0311 - val_acc: 0.5128
Epoch 10/10
 - 14s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0316 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 1s
 368/8086 [>.............................] - ETA: 1s
 720/8086 [=>............................] - ETA: 1s
1072/8086 [==>...........................] - ETA: 1s
1376/8086 [====>.........................] - ETA: 1s
1728/8086 [=====>........................] - ETA: 0s
2080/8086 [======>.......................] - ETA: 0s
2416/8086 [=======>......................] - ETA: 0s
2784/8086 [=========>....................] - ETA: 0s
3136/8086 [==========>...................] - ETA: 0s
3504/8086 [============>.................] - ETA: 0s
3856/8086 [=============>................] - ETA: 0s
4224/8086 [==============>...............] - ETA: 0s
4576/8086 [===============>..............] - ETA: 0s
4928/8086 [=================>............] - ETA: 0s
5280/8086 [==================>...........] - ETA: 0s
5600/8086 [===================>..........] - ETA: 0s
5936/8086 [=====================>........] - ETA: 0s
6288/8086 [======================>.......] - ETA: 0s
6624/8086 [=======================>......] - ETA: 0s
6976/8086 [========================>.....] - ETA: 0s
7328/8086 [==========================>...] - ETA: 0s
7680/8086 [===========================>..] - ETA: 0s
8032/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 1s 147us/step

test score: [1.034435377508999, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.4
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 31s - loss: 1.0428 - acc: 0.4975 - val_loss: 1.0334 - val_acc: 0.5128
Epoch 2/10
 - 29s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0324 - val_acc: 0.5128
Epoch 3/10
 - 29s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0322 - val_acc: 0.5128
Epoch 4/10
 - 29s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0332 - val_acc: 0.5128
Epoch 5/10
 - 29s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0326 - val_acc: 0.5128
Epoch 6/10
 - 29s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 7/10
 - 29s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0335 - val_acc: 0.5128
Epoch 8/10
 - 29s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0360 - val_acc: 0.5128
Epoch 9/10
 - 29s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0330 - val_acc: 0.5128
Epoch 10/10
 - 29s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0315 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 2s
 224/8086 [..............................] - ETA: 1s
 432/8086 [>.............................] - ETA: 1s
 640/8086 [=>............................] - ETA: 1s
 848/8086 [==>...........................] - ETA: 1s
1056/8086 [==>...........................] - ETA: 1s
1264/8086 [===>..........................] - ETA: 1s
1472/8086 [====>.........................] - ETA: 1s
1680/8086 [=====>........................] - ETA: 1s
1872/8086 [=====>........................] - ETA: 1s
2080/8086 [======>.......................] - ETA: 1s
2272/8086 [=======>......................] - ETA: 1s
2464/8086 [========>.....................] - ETA: 1s
2672/8086 [========>.....................] - ETA: 1s
2848/8086 [=========>....................] - ETA: 1s
3056/8086 [==========>...................] - ETA: 1s
3264/8086 [===========>..................] - ETA: 1s
3472/8086 [===========>..................] - ETA: 1s
3680/8086 [============>.................] - ETA: 1s
3888/8086 [=============>................] - ETA: 1s
4096/8086 [==============>...............] - ETA: 1s
4304/8086 [==============>...............] - ETA: 0s
4512/8086 [===============>..............] - ETA: 0s
4720/8086 [================>.............] - ETA: 0s
4928/8086 [=================>............] - ETA: 0s
5136/8086 [==================>...........] - ETA: 0s
5344/8086 [==================>...........] - ETA: 0s
5552/8086 [===================>..........] - ETA: 0s
5760/8086 [====================>.........] - ETA: 0s
5968/8086 [=====================>........] - ETA: 0s
6176/8086 [=====================>........] - ETA: 0s
6384/8086 [======================>.......] - ETA: 0s
6592/8086 [=======================>......] - ETA: 0s
6800/8086 [========================>.....] - ETA: 0s
7008/8086 [=========================>....] - ETA: 0s
7216/8086 [=========================>....] - ETA: 0s
7424/8086 [==========================>...] - ETA: 0s
7632/8086 [===========================>..] - ETA: 0s
7840/8086 [============================>.] - ETA: 0s
8048/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 253us/step

test score: [1.034574517306201, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.45
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 31s - loss: 1.0425 - acc: 0.4974 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 2/10
 - 29s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0313 - val_acc: 0.5128
Epoch 3/10
 - 29s - loss: 1.0409 - acc: 0.4978 - val_loss: 1.0312 - val_acc: 0.5128
Epoch 4/10
 - 29s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0343 - val_acc: 0.5128
Epoch 5/10
 - 29s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0310 - val_acc: 0.5128
Epoch 6/10
 - 29s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0324 - val_acc: 0.5128
Epoch 7/10
 - 29s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0341 - val_acc: 0.5128
Epoch 8/10
 - 29s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0322 - val_acc: 0.5128
Epoch 9/10
 - 29s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0336 - val_acc: 0.5128
Epoch 10/10
 - 29s - loss: 1.0399 - acc: 0.4978 - val_loss: 1.0314 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 2s
 224/8086 [..............................] - ETA: 1s
 432/8086 [>.............................] - ETA: 1s
 640/8086 [=>............................] - ETA: 1s
 848/8086 [==>...........................] - ETA: 1s
1056/8086 [==>...........................] - ETA: 1s
1232/8086 [===>..........................] - ETA: 1s
1424/8086 [====>.........................] - ETA: 1s
1616/8086 [====>.........................] - ETA: 1s
1808/8086 [=====>........................] - ETA: 1s
2016/8086 [======>.......................] - ETA: 1s
2224/8086 [=======>......................] - ETA: 1s
2432/8086 [========>.....................] - ETA: 1s
2640/8086 [========>.....................] - ETA: 1s
2816/8086 [=========>....................] - ETA: 1s
3024/8086 [==========>...................] - ETA: 1s
3232/8086 [==========>...................] - ETA: 1s
3440/8086 [===========>..................] - ETA: 1s
3648/8086 [============>.................] - ETA: 1s
3856/8086 [=============>................] - ETA: 1s
4064/8086 [==============>...............] - ETA: 1s
4272/8086 [==============>...............] - ETA: 0s
4480/8086 [===============>..............] - ETA: 0s
4688/8086 [================>.............] - ETA: 0s
4896/8086 [=================>............] - ETA: 0s
5104/8086 [=================>............] - ETA: 0s
5312/8086 [==================>...........] - ETA: 0s
5520/8086 [===================>..........] - ETA: 0s
5712/8086 [====================>.........] - ETA: 0s
5904/8086 [====================>.........] - ETA: 0s
6112/8086 [=====================>........] - ETA: 0s
6320/8086 [======================>.......] - ETA: 0s
6528/8086 [=======================>......] - ETA: 0s
6720/8086 [=======================>......] - ETA: 0s
6928/8086 [========================>.....] - ETA: 0s
7136/8086 [=========================>....] - ETA: 0s
7344/8086 [==========================>...] - ETA: 0s
7552/8086 [===========================>..] - ETA: 0s
7760/8086 [===========================>..] - ETA: 0s
7968/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 255us/step

test score: [1.0344331258598778, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.5
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 33s - loss: 1.0433 - acc: 0.4976 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 2/10
 - 31s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0325 - val_acc: 0.5128
Epoch 3/10
 - 31s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 4/10
 - 31s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0338 - val_acc: 0.5128
Epoch 5/10
 - 31s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 6/10
 - 31s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0347 - val_acc: 0.5128
Epoch 7/10
 - 31s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0313 - val_acc: 0.5128
Epoch 8/10
 - 31s - loss: 1.0403 - acc: 0.4978 - val_loss: 1.0326 - val_acc: 0.5128
Epoch 9/10
 - 32s - loss: 1.0402 - acc: 0.4978 - val_loss: 1.0318 - val_acc: 0.5128
Epoch 10/10
 - 31s - loss: 1.0398 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 2s
 208/8086 [..............................] - ETA: 2s
 400/8086 [>.............................] - ETA: 2s
 576/8086 [=>............................] - ETA: 2s
 768/8086 [=>............................] - ETA: 2s
 960/8086 [==>...........................] - ETA: 1s
1136/8086 [===>..........................] - ETA: 1s
1328/8086 [===>..........................] - ETA: 1s
1520/8086 [====>.........................] - ETA: 1s
1712/8086 [=====>........................] - ETA: 1s
1904/8086 [======>.......................] - ETA: 1s
2096/8086 [======>.......................] - ETA: 1s
2288/8086 [=======>......................] - ETA: 1s
2480/8086 [========>.....................] - ETA: 1s
2672/8086 [========>.....................] - ETA: 1s
2864/8086 [=========>....................] - ETA: 1s
3056/8086 [==========>...................] - ETA: 1s
3248/8086 [===========>..................] - ETA: 1s
3440/8086 [===========>..................] - ETA: 1s
3632/8086 [============>.................] - ETA: 1s
3824/8086 [=============>................] - ETA: 1s
4016/8086 [=============>................] - ETA: 1s
4192/8086 [==============>...............] - ETA: 1s
4368/8086 [===============>..............] - ETA: 1s
4560/8086 [===============>..............] - ETA: 0s
4736/8086 [================>.............] - ETA: 0s
4928/8086 [=================>............] - ETA: 0s
5120/8086 [=================>............] - ETA: 0s
5296/8086 [==================>...........] - ETA: 0s
5488/8086 [===================>..........] - ETA: 0s
5680/8086 [====================>.........] - ETA: 0s
5840/8086 [====================>.........] - ETA: 0s
6032/8086 [=====================>........] - ETA: 0s
6224/8086 [======================>.......] - ETA: 0s
6416/8086 [======================>.......] - ETA: 0s
6592/8086 [=======================>......] - ETA: 0s
6784/8086 [========================>.....] - ETA: 0s
6976/8086 [========================>.....] - ETA: 0s
7168/8086 [=========================>....] - ETA: 0s
7360/8086 [==========================>...] - ETA: 0s
7552/8086 [===========================>..] - ETA: 0s
7728/8086 [===========================>..] - ETA: 0s
7904/8086 [============================>.] - ETA: 0s
8080/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 281us/step

test score: [1.0342548691840152, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.55
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 33s - loss: 1.0427 - acc: 0.4977 - val_loss: 1.0336 - val_acc: 0.5128
Epoch 2/10
 - 31s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0319 - val_acc: 0.5128
Epoch 3/10
 - 31s - loss: 1.0410 - acc: 0.4978 - val_loss: 1.0329 - val_acc: 0.5128
Epoch 4/10
 - 31s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0315 - val_acc: 0.5128
Epoch 5/10
 - 31s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0336 - val_acc: 0.5128
Epoch 6/10
 - 31s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 7/10
 - 31s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 8/10
 - 31s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0319 - val_acc: 0.5128
Epoch 9/10
 - 31s - loss: 1.0405 - acc: 0.4978 - val_loss: 1.0316 - val_acc: 0.5128
Epoch 10/10
 - 31s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0328 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 2s
 208/8086 [..............................] - ETA: 2s
 400/8086 [>.............................] - ETA: 2s
 592/8086 [=>............................] - ETA: 2s
 752/8086 [=>............................] - ETA: 2s
 944/8086 [==>...........................] - ETA: 2s
1136/8086 [===>..........................] - ETA: 1s
1328/8086 [===>..........................] - ETA: 1s
1520/8086 [====>.........................] - ETA: 1s
1712/8086 [=====>........................] - ETA: 1s
1904/8086 [======>.......................] - ETA: 1s
2096/8086 [======>.......................] - ETA: 1s
2288/8086 [=======>......................] - ETA: 1s
2464/8086 [========>.....................] - ETA: 1s
2624/8086 [========>.....................] - ETA: 1s
2784/8086 [=========>....................] - ETA: 1s
2960/8086 [=========>....................] - ETA: 1s
3152/8086 [==========>...................] - ETA: 1s
3344/8086 [===========>..................] - ETA: 1s
3536/8086 [============>.................] - ETA: 1s
3728/8086 [============>.................] - ETA: 1s
3920/8086 [=============>................] - ETA: 1s
4112/8086 [==============>...............] - ETA: 1s
4304/8086 [==============>...............] - ETA: 1s
4496/8086 [===============>..............] - ETA: 1s
4688/8086 [================>.............] - ETA: 0s
4880/8086 [=================>............] - ETA: 0s
5072/8086 [=================>............] - ETA: 0s
5264/8086 [==================>...........] - ETA: 0s
5440/8086 [===================>..........] - ETA: 0s
5632/8086 [===================>..........] - ETA: 0s
5824/8086 [====================>.........] - ETA: 0s
6016/8086 [=====================>........] - ETA: 0s
6208/8086 [======================>.......] - ETA: 0s
6384/8086 [======================>.......] - ETA: 0s
6576/8086 [=======================>......] - ETA: 0s
6768/8086 [========================>.....] - ETA: 0s
6960/8086 [========================>.....] - ETA: 0s
7152/8086 [=========================>....] - ETA: 0s
7344/8086 [==========================>...] - ETA: 0s
7536/8086 [==========================>...] - ETA: 0s
7728/8086 [===========================>..] - ETA: 0s
7920/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 279us/step

test score: [1.0346758485636498, 0.5053178332921098]


Training new model, loss:categorical_crossentropy, optimizer=sgd, lstm_len=256, dropoff=0.6
Train on 25872 samples, validate on 6468 samples
Epoch 1/10
 - 33s - loss: 1.0431 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 2/10
 - 31s - loss: 1.0411 - acc: 0.4978 - val_loss: 1.0317 - val_acc: 0.5128
Epoch 3/10
 - 32s - loss: 1.0409 - acc: 0.4978 - val_loss: 1.0321 - val_acc: 0.5128
Epoch 4/10
 - 31s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 5/10
 - 31s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0323 - val_acc: 0.5128
Epoch 6/10
 - 31s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0326 - val_acc: 0.5128
Epoch 7/10
 - 31s - loss: 1.0408 - acc: 0.4978 - val_loss: 1.0335 - val_acc: 0.5128
Epoch 8/10
 - 31s - loss: 1.0407 - acc: 0.4978 - val_loss: 1.0351 - val_acc: 0.5128
Epoch 9/10
 - 31s - loss: 1.0406 - acc: 0.4978 - val_loss: 1.0333 - val_acc: 0.5128
Epoch 10/10
 - 31s - loss: 1.0404 - acc: 0.4978 - val_loss: 1.0319 - val_acc: 0.5128

  16/8086 [..............................] - ETA: 2s
 208/8086 [..............................] - ETA: 2s
 400/8086 [>.............................] - ETA: 2s
 592/8086 [=>............................] - ETA: 2s
 784/8086 [=>............................] - ETA: 2s
 976/8086 [==>...........................] - ETA: 1s
1152/8086 [===>..........................] - ETA: 1s
1344/8086 [===>..........................] - ETA: 1s
1520/8086 [====>.........................] - ETA: 1s
1712/8086 [=====>........................] - ETA: 1s
1888/8086 [======>.......................] - ETA: 1s
2080/8086 [======>.......................] - ETA: 1s
2272/8086 [=======>......................] - ETA: 1s
2464/8086 [========>.....................] - ETA: 1s
2656/8086 [========>.....................] - ETA: 1s
2848/8086 [=========>....................] - ETA: 1s
3040/8086 [==========>...................] - ETA: 1s
3232/8086 [==========>...................] - ETA: 1s
3424/8086 [===========>..................] - ETA: 1s
3616/8086 [============>.................] - ETA: 1s
3792/8086 [=============>................] - ETA: 1s
3984/8086 [=============>................] - ETA: 1s
4176/8086 [==============>...............] - ETA: 1s
4368/8086 [===============>..............] - ETA: 1s
4560/8086 [===============>..............] - ETA: 0s
4736/8086 [================>.............] - ETA: 0s
4928/8086 [=================>............] - ETA: 0s
5120/8086 [=================>............] - ETA: 0s
5312/8086 [==================>...........] - ETA: 0s
5504/8086 [===================>..........] - ETA: 0s
5696/8086 [====================>.........] - ETA: 0s
5888/8086 [====================>.........] - ETA: 0s
6080/8086 [=====================>........] - ETA: 0s
6272/8086 [======================>.......] - ETA: 0s
6464/8086 [======================>.......] - ETA: 0s
6656/8086 [=======================>......] - ETA: 0s
6848/8086 [========================>.....] - ETA: 0s
7040/8086 [=========================>....] - ETA: 0s
7232/8086 [=========================>....] - ETA: 0s
7424/8086 [==========================>...] - ETA: 0s
7616/8086 [===========================>..] - ETA: 0s
7808/8086 [===========================>..] - ETA: 0s
8000/8086 [============================>.] - ETA: 0s
8086/8086 [==============================] - 2s 276us/step

test score: [1.0348778424686411, 0.5053178332921098]


