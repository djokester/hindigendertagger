Training new model, loss:binary_crossentropy, optimizer=nadam, lstm_len=256, dropoff=0.5
Epoch 1/20
 - 3s - loss: 0.6799 - acc: 0.5716
Epoch 2/20
 - 1s - loss: 0.6045 - acc: 0.6718
Epoch 3/20
 - 1s - loss: 0.5850 - acc: 0.6982
Epoch 4/20
 - 1s - loss: 0.5602 - acc: 0.7047
Epoch 5/20
 - 1s - loss: 0.5503 - acc: 0.7151
Epoch 6/20
 - 1s - loss: 0.5372 - acc: 0.7218
Epoch 7/20
 - 1s - loss: 0.5321 - acc: 0.7246
Epoch 8/20
 - 1s - loss: 0.5248 - acc: 0.7271
Epoch 9/20
 - 1s - loss: 0.5116 - acc: 0.7364
Epoch 10/20
 - 1s - loss: 0.5137 - acc: 0.7335
Epoch 11/20
 - 1s - loss: 0.5040 - acc: 0.7422
Epoch 12/20
 - 1s - loss: 0.4913 - acc: 0.7566
Epoch 13/20
 - 1s - loss: 0.4781 - acc: 0.7624
Epoch 14/20
 - 1s - loss: 0.4683 - acc: 0.7693
Epoch 15/20
 - 1s - loss: 0.4556 - acc: 0.7777
Epoch 16/20
 - 1s - loss: 0.4534 - acc: 0.7767
Epoch 17/20
 - 1s - loss: 0.4395 - acc: 0.7859
Epoch 18/20
 - 1s - loss: 0.4329 - acc: 0.7951
Epoch 19/20
 - 1s - loss: 0.4216 - acc: 0.7964
Epoch 20/20
 - 1s - loss: 0.4105 - acc: 0.8040

  32/3329 [..............................] - ETA: 10s
 192/3329 [>.............................] - ETA: 2s 
 320/3329 [=>............................] - ETA: 2s
 448/3329 [===>..........................] - ETA: 1s
 608/3329 [====>.........................] - ETA: 1s
 736/3329 [=====>........................] - ETA: 1s
 896/3329 [=======>......................] - ETA: 1s
1024/3329 [========>.....................] - ETA: 1s
1152/3329 [=========>....................] - ETA: 1s
1312/3329 [==========>...................] - ETA: 0s
1440/3329 [===========>..................] - ETA: 0s
1568/3329 [=============>................] - ETA: 0s
1728/3329 [==============>...............] - ETA: 0s
1888/3329 [================>.............] - ETA: 0s
2048/3329 [=================>............] - ETA: 0s
2208/3329 [==================>...........] - ETA: 0s
2368/3329 [====================>.........] - ETA: 0s
2496/3329 [=====================>........] - ETA: 0s
2688/3329 [=======================>......] - ETA: 0s
2848/3329 [========================>.....] - ETA: 0s
3008/3329 [==========================>...] - ETA: 0s
3136/3329 [===========================>..] - ETA: 0s
3296/3329 [============================>.] - ETA: 0s
3329/3329 [==============================] - 1s 408us/step
[0.43723766276986076, 0.7846200060078101]
recall: array([0.78725962, 0.78198198])
precision: array([0.78302451, 0.78623188])
true count:array([1664, 1665])
predict count:array([1673, 1656])

Training new model, loss:binary_crossentropy, optimizer=nadam, lstm_len=256, dropoff=0.4
Epoch 1/20
 - 1s - loss: 0.6902 - acc: 0.5377
Epoch 2/20
 - 1s - loss: 0.6180 - acc: 0.6659
Epoch 3/20
 - 1s - loss: 0.5825 - acc: 0.6943
Epoch 4/20
 - 1s - loss: 0.5616 - acc: 0.7100
Epoch 5/20
 - 1s - loss: 0.5469 - acc: 0.7213
Epoch 6/20
 - 1s - loss: 0.5415 - acc: 0.7206
Epoch 7/20
 - 1s - loss: 0.5296 - acc: 0.7305
Epoch 8/20
 - 1s - loss: 0.5208 - acc: 0.7319
Epoch 9/20
 - 1s - loss: 0.5286 - acc: 0.7338
Epoch 10/20
 - 1s - loss: 0.5123 - acc: 0.7400
Epoch 11/20
 - 1s - loss: 0.5004 - acc: 0.7453
Epoch 12/20
 - 1s - loss: 0.4774 - acc: 0.7633
Epoch 13/20
 - 1s - loss: 0.4801 - acc: 0.7611
Epoch 14/20
 - 1s - loss: 0.4651 - acc: 0.7705
Epoch 15/20
 - 1s - loss: 0.4490 - acc: 0.7809
Epoch 16/20
 - 1s - loss: 0.4481 - acc: 0.7838
Epoch 17/20
 - 1s - loss: 0.4345 - acc: 0.7901
Epoch 18/20
 - 1s - loss: 0.4259 - acc: 0.7965
Epoch 19/20
 - 1s - loss: 0.4174 - acc: 0.7997
Epoch 20/20
 - 1s - loss: 0.4111 - acc: 0.8045

  32/3329 [..............................] - ETA: 12s
 192/3329 [>.............................] - ETA: 2s 
 320/3329 [=>............................] - ETA: 2s
 480/3329 [===>..........................] - ETA: 1s
 608/3329 [====>.........................] - ETA: 1s
 768/3329 [=====>........................] - ETA: 1s
 928/3329 [=======>......................] - ETA: 1s
1088/3329 [========>.....................] - ETA: 1s
1248/3329 [==========>...................] - ETA: 0s
1408/3329 [===========>..................] - ETA: 0s
1536/3329 [============>.................] - ETA: 0s
1696/3329 [==============>...............] - ETA: 0s
1824/3329 [===============>..............] - ETA: 0s
1984/3329 [================>.............] - ETA: 0s
2144/3329 [==================>...........] - ETA: 0s
2272/3329 [===================>..........] - ETA: 0s
2432/3329 [====================>.........] - ETA: 0s
2560/3329 [======================>.......] - ETA: 0s
2688/3329 [=======================>......] - ETA: 0s
2848/3329 [========================>.....] - ETA: 0s
2976/3329 [=========================>....] - ETA: 0s
3104/3329 [==========================>...] - ETA: 0s
3232/3329 [============================>.] - ETA: 0s
3329/3329 [==============================] - 1s 427us/step
[0.4263162280067233, 0.7948332832682488]
recall: array([0.78365385, 0.80600601])
precision: array([0.80147511, 0.78848414])
true count:array([1664, 1665])
predict count:array([1627, 1702])

Training new model, loss:binary_crossentropy, optimizer=nadam, lstm_len=256, dropoff=0.45
Epoch 1/20
 - 1s - loss: 0.6794 - acc: 0.5672
Epoch 2/20
 - 1s - loss: 0.6142 - acc: 0.6619
Epoch 3/20
 - 1s - loss: 0.5766 - acc: 0.7018
Epoch 4/20
 - 1s - loss: 0.5620 - acc: 0.7082
Epoch 5/20
 - 1s - loss: 0.5529 - acc: 0.7112
Epoch 6/20
 - 1s - loss: 0.5441 - acc: 0.7163
Epoch 7/20
 - 1s - loss: 0.5358 - acc: 0.7233
Epoch 8/20
 - 1s - loss: 0.5291 - acc: 0.7292
Epoch 9/20
 - 1s - loss: 0.5143 - acc: 0.7405
Epoch 10/20
 - 1s - loss: 0.5048 - acc: 0.7435
Epoch 11/20
 - 1s - loss: 0.4946 - acc: 0.7524
Epoch 12/20
 - 1s - loss: 0.4801 - acc: 0.7635
Epoch 13/20
 - 1s - loss: 0.4665 - acc: 0.7721
Epoch 14/20
 - 1s - loss: 0.4604 - acc: 0.7726
Epoch 15/20
 - 1s - loss: 0.4507 - acc: 0.7787
Epoch 16/20
 - 1s - loss: 0.4430 - acc: 0.7850
Epoch 17/20
 - 1s - loss: 0.4351 - acc: 0.7866
Epoch 18/20
 - 1s - loss: 0.4278 - acc: 0.7930
Epoch 19/20
 - 1s - loss: 0.4146 - acc: 0.8028
Epoch 20/20
 - 1s - loss: 0.4139 - acc: 0.8009

  32/3329 [..............................] - ETA: 14s
 192/3329 [>.............................] - ETA: 3s 
 352/3329 [==>...........................] - ETA: 2s
 512/3329 [===>..........................] - ETA: 1s
 672/3329 [=====>........................] - ETA: 1s
 832/3329 [======>.......................] - ETA: 1s
 992/3329 [=======>......................] - ETA: 1s
1184/3329 [=========>....................] - ETA: 0s
1312/3329 [==========>...................] - ETA: 0s
1440/3329 [===========>..................] - ETA: 0s
1568/3329 [=============>................] - ETA: 0s
1696/3329 [==============>...............] - ETA: 0s
1856/3329 [===============>..............] - ETA: 0s
1984/3329 [================>.............] - ETA: 0s
2144/3329 [==================>...........] - ETA: 0s
2272/3329 [===================>..........] - ETA: 0s
2432/3329 [====================>.........] - ETA: 0s
2592/3329 [======================>.......] - ETA: 0s
2720/3329 [=======================>......] - ETA: 0s
2880/3329 [========================>.....] - ETA: 0s
3040/3329 [==========================>...] - ETA: 0s
3168/3329 [===========================>..] - ETA: 0s
3296/3329 [============================>.] - ETA: 0s
3329/3329 [==============================] - 1s 421us/step
[0.42899805925099405, 0.7978371883448483]
recall: array([0.74338942, 0.85225225])
precision: array([0.83412003, 0.76868906])
true count:array([1664, 1665])
predict count:array([1483, 1846])

Training new model, loss:binary_crossentropy, optimizer=nadam, lstm_len=128, dropoff=0.5
Epoch 1/20
 - 1s - loss: 0.6603 - acc: 0.5853
Epoch 2/20
 - 1s - loss: 0.6001 - acc: 0.6739
Epoch 3/20
 - 1s - loss: 0.5758 - acc: 0.6980
Epoch 4/20
 - 1s - loss: 0.5510 - acc: 0.7139
Epoch 5/20
 - 1s - loss: 0.5394 - acc: 0.7226
Epoch 6/20
 - 1s - loss: 0.5428 - acc: 0.7209
Epoch 7/20
 - 1s - loss: 0.5223 - acc: 0.7278
Epoch 8/20
 - 1s - loss: 0.5212 - acc: 0.7293
Epoch 9/20
 - 1s - loss: 0.5091 - acc: 0.7441
Epoch 10/20
 - 1s - loss: 0.5034 - acc: 0.7462
Epoch 11/20
 - 1s - loss: 0.4883 - acc: 0.7547
Epoch 12/20
 - 1s - loss: 0.4842 - acc: 0.7574
Epoch 13/20
 - 1s - loss: 0.4761 - acc: 0.7633
Epoch 14/20
 - 1s - loss: 0.4601 - acc: 0.7743
Epoch 15/20
 - 1s - loss: 0.4505 - acc: 0.7808
Epoch 16/20
 - 1s - loss: 0.4461 - acc: 0.7800
Epoch 17/20
 - 1s - loss: 0.4399 - acc: 0.7879
Epoch 18/20
 - 1s - loss: 0.4320 - acc: 0.7888
Epoch 19/20
 - 1s - loss: 0.4247 - acc: 0.7951
Epoch 20/20
 - 1s - loss: 0.4187 - acc: 0.7982

  32/3329 [..............................] - ETA: 17s
 192/3329 [>.............................] - ETA: 3s 
 352/3329 [==>...........................] - ETA: 2s
 512/3329 [===>..........................] - ETA: 1s
 672/3329 [=====>........................] - ETA: 1s
 832/3329 [======>.......................] - ETA: 1s
 992/3329 [=======>......................] - ETA: 1s
1152/3329 [=========>....................] - ETA: 1s
1312/3329 [==========>...................] - ETA: 0s
1472/3329 [============>.................] - ETA: 0s
1632/3329 [=============>................] - ETA: 0s
1760/3329 [==============>...............] - ETA: 0s
1920/3329 [================>.............] - ETA: 0s
2080/3329 [=================>............] - ETA: 0s
2240/3329 [===================>..........] - ETA: 0s
2400/3329 [====================>.........] - ETA: 0s
2560/3329 [======================>.......] - ETA: 0s
2720/3329 [=======================>......] - ETA: 0s
2880/3329 [========================>.....] - ETA: 0s
3008/3329 [==========================>...] - ETA: 0s
3168/3329 [===========================>..] - ETA: 0s
3328/3329 [============================>.] - ETA: 0s
3329/3329 [==============================] - 1s 430us/step
[0.43611412456208215, 0.7810153199158907]
recall: array([0.79507212, 0.76696697])
precision: array([0.77323203, 0.78924598])
true count:array([1664, 1665])
predict count:array([1711, 1618])

Training new model, loss:binary_crossentropy, optimizer=nadam, lstm_len=128, dropoff=0.4
Epoch 1/20
 - 2s - loss: 0.6720 - acc: 0.5673
Epoch 2/20
 - 1s - loss: 0.6022 - acc: 0.6770
Epoch 3/20
 - 1s - loss: 0.5791 - acc: 0.6968
Epoch 4/20
 - 1s - loss: 0.5535 - acc: 0.7160
Epoch 5/20
 - 1s - loss: 0.5472 - acc: 0.7164
Epoch 6/20
 - 1s - loss: 0.5361 - acc: 0.7224
Epoch 7/20
 - 1s - loss: 0.5223 - acc: 0.7332
Epoch 8/20
 - 1s - loss: 0.5207 - acc: 0.7353
Epoch 9/20
 - 1s - loss: 0.5051 - acc: 0.7433
Epoch 10/20
 - 1s - loss: 0.4992 - acc: 0.7478
Epoch 11/20
 - 1s - loss: 0.4890 - acc: 0.7498
Epoch 12/20
 - 1s - loss: 0.4776 - acc: 0.7639
Epoch 13/20
 - 1s - loss: 0.4774 - acc: 0.7612
Epoch 14/20
 - 1s - loss: 0.4569 - acc: 0.7770
Epoch 15/20
 - 1s - loss: 0.4506 - acc: 0.7786
Epoch 16/20
 - 1s - loss: 0.4430 - acc: 0.7826
Epoch 17/20
 - 1s - loss: 0.4375 - acc: 0.7849
Epoch 18/20
 - 1s - loss: 0.4298 - acc: 0.7868
Epoch 19/20
 - 1s - loss: 0.4203 - acc: 0.7961
Epoch 20/20
 - 1s - loss: 0.4253 - acc: 0.7939

  32/3329 [..............................] - ETA: 20s
 192/3329 [>.............................] - ETA: 4s 
 320/3329 [=>............................] - ETA: 2s
 448/3329 [===>..........................] - ETA: 2s
 576/3329 [====>.........................] - ETA: 2s
 704/3329 [=====>........................] - ETA: 1s
 832/3329 [======>.......................] - ETA: 1s
 992/3329 [=======>......................] - ETA: 1s
1120/3329 [=========>....................] - ETA: 1s
1248/3329 [==========>...................] - ETA: 1s
1376/3329 [===========>..................] - ETA: 1s
1504/3329 [============>.................] - ETA: 0s
1632/3329 [=============>................] - ETA: 0s
1760/3329 [==============>...............] - ETA: 0s
1888/3329 [================>.............] - ETA: 0s
2016/3329 [=================>............] - ETA: 0s
2176/3329 [==================>...........] - ETA: 0s
2304/3329 [===================>..........] - ETA: 0s
2464/3329 [=====================>........] - ETA: 0s
2624/3329 [======================>.......] - ETA: 0s
2784/3329 [========================>.....] - ETA: 0s
2944/3329 [=========================>....] - ETA: 0s
3072/3329 [==========================>...] - ETA: 0s
3232/3329 [============================>.] - ETA: 0s
3329/3329 [==============================] - 2s 460us/step
[0.4385691120589449, 0.78552117753079]
recall: array([0.77223558, 0.7987988 ])
precision: array([0.79320988, 0.77823288])
true count:array([1664, 1665])
predict count:array([1620, 1709])

